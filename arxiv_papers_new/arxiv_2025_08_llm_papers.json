{
  "metadata": {
    "last_updated": "2025-08-12T12:02:11.693682+00:00",
    "total_papers": 837,
    "source": "arXiv",
    "keywords": [
      "LLM",
      "large language model"
    ]
  },
  "papers": [
    {
      "title": "A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks",
      "abstract": "We introduce a modular prompting framework that supports safer and more\nadaptive use of large language models (LLMs) across dynamic, user-centered\ntasks. Grounded in human learning theory, particularly the Zone of Proximal\nDevelopment (ZPD), our method combines a natural language boundary prompt with\na control schema encoded with fuzzy scaffolding logic and adaptation rules.\nThis architecture enables LLMs to modulate behavior in response to user state\nwithout requiring fine-tuning or external orchestration. In a simulated\nintelligent tutoring setting, the framework improves scaffolding quality,\nadaptivity, and instructional alignment across multiple models, outperforming\nstandard prompting baselines. Evaluation is conducted using rubric-based LLM\ngraders at scale. While initially developed for education, the framework has\nshown promise in other interaction-heavy domains, such as procedural content\ngeneration for games. Designed for safe deployment, it provides a reusable\nmethodology for structuring interpretable, goal-aligned LLM behavior in\nuncertain or evolving contexts.",
      "authors": [
        "Vanessa Figueiredo"
      ],
      "published": "2025-08-08T23:50:48+00:00",
      "updated": "2025-08-08T23:50:48+00:00",
      "arxiv_id": "2508.06754v1",
      "url": "http://arxiv.org/pdf/2508.06754v1",
      "categories": [
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Pushing the Envelope of LLM Inference on AI-PC",
      "abstract": "The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the\nperplexity and end-task performance of their full-precision counterparts using\nthe same model size, is ushering in a new era of LLM inference for\nresource-constrained environments such as edge devices and AI PCs. While these\nquantization advances promise models that are more cost-effective in terms of\nlatency, memory, throughput, and energy consumption, the computational\nefficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)\nused to deploy them remains underexplored. In this work, we take a bottom-up\napproach: we first design and implement 1-bit and 2-bit microkernels optimized\nfor modern CPUs, achieving peak computational efficiency across a variety of\nCPU platforms. We integrate these microkernels into a state-of-the-art LLM\ninference framework, namely PyTorch-TPP, and present end-to-end inference\nresults with 2-bit models that outperform the current SOTA runtime bitnet.cpp\nby up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model\ninference. Our optimized runtime advances the state of LLM inference on AI PCs\nand edge devices, paving the way for efficient deployment of ultra-low-bit LLM\nmodels.",
      "authors": [
        "Evangelos Georganas",
        "Dhiraj Kalamkar",
        "Alexander Heinecke"
      ],
      "published": "2025-08-08T23:33:38+00:00",
      "updated": "2025-08-08T23:33:38+00:00",
      "arxiv_id": "2508.06753v1",
      "url": "http://arxiv.org/pdf/2508.06753v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Mitigating Distribution Shift in Graph-Based Android Malware Classification via Function Metadata and LLM Embeddings",
      "abstract": "Graph-based malware classifiers can achieve over 94% accuracy on standard\nAndroid datasets, yet we find they suffer accuracy drops of up to 45% when\nevaluated on previously unseen malware variants from the same family - a\nscenario where strong generalization would typically be expected. This\nhighlights a key limitation in existing approaches: both the model\narchitectures and their structure-only representations often fail to capture\ndeeper semantic patterns. In this work, we propose a robust semantic enrichment\nframework that enhances function call graphs with contextual features,\nincluding function-level metadata and, when available, code embeddings derived\nfrom large language models. The framework is designed to operate under\nreal-world constraints where feature availability is inconsistent, and supports\nflexible integration of semantic signals. To evaluate generalization under\nrealistic domain and temporal shifts, we introduce two new benchmarks:\nMalNet-Tiny-Common and MalNet-Tiny-Distinct, constructed using malware family\npartitioning to simulate cross-family generalization and evolving threat\nbehavior. Experiments across multiple graph neural network backbones show that\nour method improves classification performance by up to 8% under distribution\nshift and consistently enhances robustness when integrated with\nadaptation-based methods. These results offer a practical path toward building\nresilient malware detection systems in evolving threat environments.",
      "authors": [
        "Ngoc N. Tran",
        "Anwar Said",
        "Waseem Abbas",
        "Tyler Derr",
        "Xenofon D. Koutsoukos"
      ],
      "published": "2025-08-08T22:16:57+00:00",
      "updated": "2025-08-08T22:16:57+00:00",
      "arxiv_id": "2508.06734v1",
      "url": "http://arxiv.org/pdf/2508.06734v1",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets",
      "abstract": "Ensemble datasets are ever more prevalent in various scientific domains. In\nclimate science, ensemble datasets are used to capture variability in\nprojections under plausible future conditions including greenhouse and aerosol\nemissions. Each ensemble model run produces projections that are fundamentally\nsimilar yet meaningfully distinct. Understanding this variability among\nensemble model runs and analyzing its magnitude and patterns is a vital task\nfor climate scientists. In this paper, we present ClimateSOM, a visual analysis\nworkflow that leverages a self-organizing map (SOM) and Large Language Models\n(LLMs) to support interactive exploration and interpretation of climate\nensemble datasets. The workflow abstracts climate ensemble model runs -\nspatiotemporal time series - into a distribution over a 2D space that captures\nthe variability among the ensemble model runs using a SOM. LLMs are integrated\nto assist in sensemaking of this SOM-defined 2D space, the basis for the visual\nanalysis tasks. In all, ClimateSOM enables users to explore the variability\namong ensemble model runs, identify patterns, compare and cluster the ensemble\nmodel runs. To demonstrate the utility of ClimateSOM, we apply the workflow to\nan ensemble dataset of precipitation projections over California and the\nNorthwestern United States. Furthermore, we conduct a short evaluation of our\nLLM integration, and conduct an expert review of the visual workflow and the\ninsights from the case studies with six domain experts to evaluate our approach\nand its utility.",
      "authors": [
        "Yuya Kawakami",
        "Daniel Cayan",
        "Dongyu Liu",
        "Kwan-Liu Ma"
      ],
      "published": "2025-08-08T22:15:43+00:00",
      "updated": "2025-08-08T22:15:43+00:00",
      "arxiv_id": "2508.06732v1",
      "url": "http://arxiv.org/pdf/2508.06732v1",
      "categories": [
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis",
      "abstract": "Oral histories are vital records of lived experience, particularly within\ncommunities affected by systemic injustice and historical erasure. Effective\nand efficient analysis of their oral history archives can promote access and\nunderstanding of the oral histories. However, Large-scale analysis of these\narchives remains limited due to their unstructured format, emotional\ncomplexity, and high annotation costs. This paper presents a scalable framework\nto automate semantic and sentiment annotation for Japanese American\nIncarceration Oral History. Using LLMs, we construct a high-quality dataset,\nevaluate multiple models, and test prompt engineering strategies in\nhistorically sensitive contexts. Our multiphase approach combines expert\nannotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We\nlabeled 558 sentences from 15 narrators for sentiment and semantic\nclassification, then evaluated zero-shot, few-shot, and RAG strategies. For\nsemantic classification, ChatGPT achieved the highest F1 score (88.71%),\nfollowed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama\nslightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models\nshowing comparable results. The best prompt configurations were used to\nannotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our\nfindings show that LLMs can effectively perform semantic and sentiment\nannotation across large oral history collections when guided by well-designed\nprompts. This study provides a reusable annotation pipeline and practical\nguidance for applying LLMs in culturally sensitive archival analysis. By\nbridging archival ethics with scalable NLP techniques, this work lays the\ngroundwork for responsible use of artificial intelligence in digital humanities\nand preservation of collective memory. GitHub:\nhttps://github.com/kc6699c/LLM4OralHistoryAnalysis.",
      "authors": [
        "Komala Subramanyam Cherukuri",
        "Pranav Abishai Moses",
        "Aisa Sakata",
        "Jiangping Chen",
        "Haihua Chen"
      ],
      "published": "2025-08-08T22:06:23+00:00",
      "updated": "2025-08-08T22:06:23+00:00",
      "arxiv_id": "2508.06729v1",
      "url": "http://arxiv.org/pdf/2508.06729v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge",
      "abstract": "Large language models (LLMs) can serve as judges that offer rapid and\nreliable assessments of other LLM outputs. However, models may systematically\nassign overly favorable ratings to their own outputs, a phenomenon known as\nself-bias, which can distort evaluations of true model performance. Previous\nstudies often conflate genuine differences in model quality with bias or\nincorrectly assume that evaluations from LLMs and humans follow the same rating\ndistributions. In this work, we present a statistical framework that explicitly\nformalizes assumptions under which self-bias can be identified and estimated.\nOur method models the difference in the scoring distribution that\nLLM-as-a-judge assigns to its own completions compared to other models, while\naccounting for the underlying quality of the completions provided by an\nindependent, third-party judge (e.g., humans). Our method reliably isolates and\nquantifies self-bias, even when models vary in ability, ensuring that genuine\nperformance differences are not mistaken for self-bias. We conduct an empirical\nanalysis of self-bias on a large dataset (>5000 prompt-completion pairs)\nconsisting of expert human annotations and judgments from nine different LLM\njudges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,\nsystematically assign higher scores to their own outputs. These models also\ndisplay family-bias; systematically assigning higher ratings to outputs\nproduced by other models of the same family. Our findings highlight potential\npitfalls of using LLM judges and offer practical guidance to mitigate biases\nwhen interpreting automated evaluations.",
      "authors": [
        "Evangelia Spiliopoulou",
        "Riccardo Fogliato",
        "Hanna Burnsky",
        "Tamer Soliman",
        "Jie Ma",
        "Graham Horwood",
        "Miguel Ballesteros"
      ],
      "published": "2025-08-08T21:22:12+00:00",
      "updated": "2025-08-08T21:22:12+00:00",
      "arxiv_id": "2508.06709v1",
      "url": "http://arxiv.org/pdf/2508.06709v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Role of Large Language Models and Retrieval-Augmented Generation for Accelerating Crystalline Material Discovery: A Systematic Review",
      "abstract": "Large language models (LLMs) have emerged as powerful tools for\nknowledge-intensive tasks across domains. In materials science, to find novel\nmaterials for various energy efficient devices for various real-world\napplications, requires several time and cost expensive simulations and\nexperiments. In order to tune down the uncharted material search space,\nminimizing the experimental cost, LLMs can play a bigger role to first provide\nan accelerated search of promising known material candidates. Furthermore, the\nintegration of LLMs with domain-specific information via retrieval-augmented\ngeneration (RAG) is poised to revolutionize how researchers predict materials\nstructures, analyze defects, discover novel compounds, and extract knowledge\nfrom literature and databases. In motivation to the potentials of LLMs and RAG\nin accelerating material discovery, this paper presents a broad and systematic\nreview to examine the recent advancements in applying LLMs and RAG to key\nmaterials science problems. We survey state-of-the-art developments in crystal\nstructure prediction, defect analysis, materials discovery, literature mining,\ndatabase integration, and multi-modal retrieval, highlighting how combining\nLLMs with external knowledge sources enables new capabilities. We discuss the\nperformance, limitations, and implications of these approaches, and outline\nfuture directions for leveraging LLMs to accelerate materials research and\ndiscovery for advancement in technologies in the area of electronics, optics,\nbiomedical, and energy storage.",
      "authors": [
        "Agada Joseph Oche",
        "Arpan Biswas"
      ],
      "published": "2025-08-08T20:32:56+00:00",
      "updated": "2025-08-08T20:32:56+00:00",
      "arxiv_id": "2508.06691v1",
      "url": "http://arxiv.org/pdf/2508.06691v1",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci"
    },
    {
      "title": "Do Biased Models Have Biased Thoughts?",
      "abstract": "The impressive performance of language models is undeniable. However, the\npresence of biases based on gender, race, socio-economic status, physical\nappearance, and sexual orientation makes the deployment of language models\nchallenging. This paper studies the effect of chain-of-thought prompting, a\nrecent approach that studies the steps followed by the model before it\nresponds, on fairness. More specifically, we ask the following question:\n\\textit{Do biased models have biased thoughts}? To answer our question, we\nconduct experiments on $5$ popular large language models using fairness metrics\nto quantify $11$ different biases in the model's thoughts and output. Our\nresults show that the bias in the thinking steps is not highly correlated with\nthe output bias (less than $0.6$ correlation with a $p$-value smaller than\n$0.001$ in most cases). In other words, unlike human beings, the tested models\nwith biased decisions do not always possess biased thoughts.",
      "authors": [
        "Swati Rajwal",
        "Shivank Garg",
        "Reem Abdel-Salam",
        "Abdelrahman Zayed"
      ],
      "published": "2025-08-08T19:41:20+00:00",
      "updated": "2025-08-08T19:41:20+00:00",
      "arxiv_id": "2508.06671v1",
      "url": "http://arxiv.org/pdf/2508.06671v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Testing the Limits of Machine Translation from One Book",
      "abstract": "Current state-of-the-art models demonstrate capacity to leverage in-context\nlearning to translate into previously unseen language contexts. Tanzer et al.\n[2024] utilize language materials (e.g. a grammar) to improve translation\nquality for Kalamang using large language models (LLMs). We focus on Kanuri, a\nlanguage that, despite having substantial speaker population, has minimal\ndigital resources. We design two datasets for evaluation: one focused on health\nand humanitarian terms, and another containing generalized terminology,\ninvestigating how domain-specific tasks impact LLM translation quality.\n  By providing different combinations of language resources (grammar,\ndictionary, and parallel sentences), we measure LLM translation effectiveness,\ncomparing results to native speaker translations and human linguist\nperformance. We evaluate using both automatic metrics and native speaker\nassessments of fluency and accuracy.\n  Results demonstrate that parallel sentences remain the most effective data\nsource, outperforming other methods in human evaluations and automatic metrics.\nWhile incorporating grammar improves over zero-shot translation, it fails as an\neffective standalone data source. Human evaluations reveal that LLMs achieve\naccuracy (meaning) more effectively than fluency (grammaticality).\n  These findings suggest LLM translation evaluation benefits from\nmultidimensional assessment beyond simple accuracy metrics, and that grammar\nalone, without parallel sentences, does not provide sufficient context for\neffective domain-specific translation.",
      "authors": [
        "Jonathan Shaw",
        "Dillon Mee",
        "Timothy Khouw",
        "Zackary Leech",
        "Daniel Wilson"
      ],
      "published": "2025-08-08T19:27:44+00:00",
      "updated": "2025-08-08T19:27:44+00:00",
      "arxiv_id": "2508.06665v1",
      "url": "http://arxiv.org/pdf/2508.06665v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Towards Robust Red-Green Watermarking for Autoregressive Image Generators",
      "abstract": "In-generation watermarking for detecting and attributing generated content\nhas recently been explored for latent diffusion models (LDMs), demonstrating\nhigh robustness. However, the use of in-generation watermarks in autoregressive\n(AR) image models has not been explored yet. AR models generate images by\nautoregressively predicting a sequence of visual tokens that are then decoded\ninto pixels using a vector-quantized decoder. Inspired by red-green watermarks\nfor large language models, we examine token-level watermarking schemes that\nbias the next-token prediction based on prior tokens. We find that a direct\ntransfer of these schemes works in principle, but the detectability of the\nwatermarks decreases considerably under common image perturbations. As a\nremedy, we propose two novel watermarking methods that rely on visual token\nclustering to assign similar tokens to the same set. Firstly, we investigate a\ntraining-free approach that relies on a cluster lookup table, and secondly, we\nfinetune VAE encoders to predict token clusters directly from perturbed images.\nOverall, our experiments show that cluster-level watermarks improve robustness\nagainst perturbations and regeneration attacks while preserving image quality.\nCluster classification further boosts watermark detectability, outperforming a\nset of baselines. Moreover, our methods offer fast verification runtime,\ncomparable to lightweight post-hoc watermarking methods.",
      "authors": [
        "Denis Lukovnikov",
        "Andreas MÃ¼ller",
        "Erwin Quiring",
        "Asja Fischer"
      ],
      "published": "2025-08-08T19:14:22+00:00",
      "updated": "2025-08-08T19:14:22+00:00",
      "arxiv_id": "2508.06656v1",
      "url": "http://arxiv.org/pdf/2508.06656v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Measuring Stereotype and Deviation Biases in Large Language Models",
      "abstract": "Large language models (LLMs) are widely applied across diverse domains,\nraising concerns about their limitations and potential risks. In this study, we\ninvestigate two types of bias that LLMs may display: stereotype bias and\ndeviation bias. Stereotype bias refers to when LLMs consistently associate\nspecific traits with a particular demographic group. Deviation bias reflects\nthe disparity between the demographic distributions extracted from\nLLM-generated content and real-world demographic distributions. By asking four\nadvanced LLMs to generate profiles of individuals, we examine the associations\nbetween each demographic group and attributes such as political affiliation,\nreligion, and sexual orientation. Our experimental results show that all\nexamined LLMs exhibit both significant stereotype bias and deviation bias\ntowards multiple groups. Our findings uncover the biases that occur when LLMs\ninfer user attributes and shed light on the potential harms of LLM-generated\noutputs.",
      "authors": [
        "Daniel Wang",
        "Eli Brignac",
        "Minjia Mao",
        "Xiao Fang"
      ],
      "published": "2025-08-08T19:03:57+00:00",
      "updated": "2025-08-08T19:03:57+00:00",
      "arxiv_id": "2508.06649v1",
      "url": "http://arxiv.org/pdf/2508.06649v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Using Imperfect Synthetic Data in Downstream Inference Tasks",
      "abstract": "Predictions and generations from large language models are increasingly being\nexplored as an aid to computational social science and human subject research\nin limited data regimes. While previous technical work has explored the\npotential to use model-predicted labels for unlabeled data in a principled\nmanner, there is increasing interest in using large language models to generate\nentirely new synthetic samples (also termed as synthetic simulations), such as\nin responses to surveys. However, it is not immediately clear by what means\npractitioners can combine such data with real data and yet produce\nstatistically valid conclusions upon them. In this work, we introduce a new\nestimator based on generalized method of moments, providing a\nhyperparameter-free solution with strong theoretical guarantees to address the\nchallenge at hand. Surprisingly, we find that interactions between the moment\nresiduals of synthetic data and those of real data can improve estimates of the\ntarget parameter. We empirically validate the finite-sample performance of our\nestimator across different regression tasks in computational social science\napplications, demonstrating large empirical gains.",
      "authors": [
        "Yewon Byun",
        "Shantanu Gupta",
        "Zachary C. Lipton",
        "Rachel Leah Childers",
        "Bryan Wilder"
      ],
      "published": "2025-08-08T18:32:52+00:00",
      "updated": "2025-08-08T18:32:52+00:00",
      "arxiv_id": "2508.06635v1",
      "url": "http://arxiv.org/pdf/2508.06635v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Generalizing Scaling Laws for Dense and Sparse Large Language Models",
      "abstract": "Over the past few years, the size of language models has grown exponentially,\nas has the computational cost to train these large models. This rapid growth\nhas motivated researchers to develop new techniques aimed at enhancing the\nefficiency of the training process. Despite these advancements, optimally\npredicting the model size or allocating optimal resources remains a challenge.\nSeveral efforts have addressed the challenge by proposing different scaling\nlaws, but almost all of them are architecture-specific (dense or sparse). In\nthis work we revisit existing scaling laws and propose a generalized scaling\nlaw to provide a unified framework that is applicable to both dense and sparse\nlarge language models. We evaluate and compare our proposed scaling law with\nexisting scaling laws to demonstrate its effectiveness.",
      "authors": [
        "Md Arafat Hossain",
        "Xingfu Wu",
        "Valerie Taylor",
        "Ali Jannesari"
      ],
      "published": "2025-08-08T18:07:11+00:00",
      "updated": "2025-08-08T18:07:11+00:00",
      "arxiv_id": "2508.06617v1",
      "url": "http://arxiv.org/pdf/2508.06617v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Generative AI for Intent-Driven Network Management in 6G: A Case Study on Hierarchical Learning Approach",
      "abstract": "With the emergence of 6G, mobile networks are becoming increasingly\nheterogeneous and dynamic, necessitating advanced automation for efficient\nmanagement. Intent-Driven Networks (IDNs) address this by translating\nhigh-level intents into optimization policies. Large Language Models (LLMs) can\nenhance this process by understanding complex human instructions to enable\nadaptive, intelligent automation. Given the rapid advancements in Generative AI\n(GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated\nRadio Access Network (RAN) environments is both timely and critical. This\narticle provides such a survey, along with a case study on a hierarchical\nlearning-enabled IDN architecture that integrates GenAI across three key\nstages: intent processing, intent validation, and intent execution. Unlike most\nexisting approaches that apply GenAI in the form of LLMs for intent processing\nonly, we propose a hierarchical framework that introduces GenAI across all\nthree stages of IDN. To demonstrate the effectiveness of the proposed IDN\nmanagement architecture, we present a case study based on the latest GenAI\narchitecture named Mamba. The case study shows how the proposed GenAI-driven\narchitecture enhances network performance through intelligent automation,\nsurpassing the performance of the conventional IDN architectures.",
      "authors": [
        "Md Arafat Habib",
        "Medhat Elsayed",
        "Yigit Ozcan",
        "Pedro Enrique Iturria-Rivera",
        "Majid Bavand",
        "Melike Erol-Kantarci"
      ],
      "published": "2025-08-08T18:06:52+00:00",
      "updated": "2025-08-08T18:06:52+00:00",
      "arxiv_id": "2508.06616v1",
      "url": "http://arxiv.org/pdf/2508.06616v1",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI"
    },
    {
      "title": "Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs",
      "abstract": "Open-weight AI systems offer unique benefits, including enhanced\ntransparency, open research, and decentralized access. However, they are\nvulnerable to tampering attacks which can efficiently elicit harmful behaviors\nby modifying weights or activations. Currently, there is not yet a robust\nscience of open-weight model risk management. Existing safety fine-tuning\nmethods and other post-training techniques have struggled to make LLMs\nresistant to more than a few dozen steps of adversarial fine-tuning. In this\npaper, we investigate whether filtering text about dual-use topics from\ntraining data can prevent unwanted capabilities and serve as a more\ntamper-resistant safeguard. We introduce a multi-stage pipeline for scalable\ndata filtering and show that it offers a tractable and effective method for\nminimizing biothreat proxy knowledge in LLMs. We pretrain multiple\n6.9B-parameter models from scratch and find that they exhibit substantial\nresistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M\ntokens of biothreat-related text -- outperforming existing post-training\nbaselines by over an order of magnitude -- with no observed degradation to\nunrelated capabilities. However, while filtered models lack internalized\ndangerous knowledge, we find that they can still leverage such information when\nit is provided in context (e.g., via search tool augmentation), demonstrating a\nneed for a defense-in-depth approach. Overall, these findings help to establish\npretraining data curation as a promising layer of defense for open-weight AI\nsystems.",
      "authors": [
        "Kyle O'Brien",
        "Stephen Casper",
        "Quentin Anthony",
        "Tomek Korbak",
        "Robert Kirk",
        "Xander Davies",
        "Ishan Mishra",
        "Geoffrey Irving",
        "Yarin Gal",
        "Stella Biderman"
      ],
      "published": "2025-08-08T17:59:47+00:00",
      "updated": "2025-08-08T17:59:47+00:00",
      "arxiv_id": "2508.06601v1",
      "url": "http://arxiv.org/pdf/2508.06601v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Effective Training Data Synthesis for Improving MLLM Chart Understanding",
      "abstract": "Being able to effectively read scientific plots, or chart understanding, is a\ncentral part toward building effective agents for science. However, existing\nmultimodal large language models (MLLMs), especially open-source ones, are\nstill falling behind with a typical success rate of 30%-50% on challenging\nbenchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are\noften restricted by their inadequate similarity to the real charts, which could\ncompromise model training and performance on complex real-world charts. In this\nstudy, we show that modularizing chart generation and diversifying visual\ndetails improves chart understanding capabilities. In particular, we design a\nfive-step data synthesis pipeline, where we separate data and function creation\nfor single plot generation, condition the generation of later subplots on\nearlier ones for multi-subplot figures, visually diversify the generated\nfigures, filter out low quality data, and finally generate the question-answer\n(QA) pairs with GPT-4o. This approach allows us to streamline the generation of\nfine-tuning datasets and introduce the effective chart dataset (ECD), which\ncontains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring\n250+ chart type combinations with high visual complexity. We show that ECD\nconsistently improves the performance of various MLLMs on a range of real-world\nand synthetic test sets. Code, data and models are available at:\nhttps://github.com/yuweiyang-anu/ECD.",
      "authors": [
        "Yuwei Yang",
        "Zeyu Zhang",
        "Yunzhong Hou",
        "Zhuowan Li",
        "Gaowen Liu",
        "Ali Payani",
        "Yuan-Sen Ting",
        "Liang Zheng"
      ],
      "published": "2025-08-08T17:59:10+00:00",
      "updated": "2025-08-08T17:59:10+00:00",
      "arxiv_id": "2508.06492v1",
      "url": "http://arxiv.org/pdf/2508.06492v1",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent",
      "abstract": "Deep-Research agents, which integrate large language models (LLMs) with\nsearch tools, have shown success in improving the effectiveness of handling\ncomplex queries that require iterative search planning and reasoning over\nsearch results. Evaluations on current benchmarks like BrowseComp relies on\nblack-box live web search APIs, have notable limitations in (1) fairness:\ndynamic and opaque web APIs hinder fair comparisons and reproducibility of deep\nresearch methods; (2) transparency: lack of control over the document corpus\nmakes it difficult to isolate retriever contributions. In other words, the\ncurrent evaluations may compare a complete deep research system at a given\ntime, but they do not foster well-controlled experiments to provide insights\ninto the capability of underlying deep research LLMs. To address these\nchallenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,\nemploying a fixed, carefully curated corpus. Each query in BrowseComp-Plus\nincludes human-verified supporting documents and mined challenging negatives,\nenabling controlled experimentation. The benchmark is shown to be effective in\ndistinguishing the performance of deep research systems. For instance, the\nopen-source model Search-R1, when paired with the BM25 retriever, achieves\n3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with\nthe Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with\nfewer search calls. This benchmark allows comprehensive evaluation and\ndisentangled analysis of deep research agents and retrieval methods, fostering\ninsights into retrieval effectiveness, citation accuracy, and context\nengineering in Deep-Research system.",
      "authors": [
        "Zijian Chen",
        "Xueguang Ma",
        "Shengyao Zhuang",
        "Ping Nie",
        "Kai Zou",
        "Andrew Liu",
        "Joshua Green",
        "Kshama Patel",
        "Ruoxi Meng",
        "Mingyi Su",
        "Sahel Sharifymoghaddam",
        "Yanxi Li",
        "Haoran Hong",
        "Xinyu Shi",
        "Xuye Liu",
        "Nandan Thakur",
        "Crystina Zhang",
        "Luyu Gao",
        "Wenhu Chen",
        "Jimmy Lin"
      ],
      "published": "2025-08-08T17:55:11+00:00",
      "updated": "2025-08-08T17:55:11+00:00",
      "arxiv_id": "2508.06600v1",
      "url": "http://arxiv.org/pdf/2508.06600v1",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Non-programmers Assessing AI-Generated Code: A Case Study of Business Users Analyzing Data",
      "abstract": "Non-technical end-users increasingly rely on AI code generation to perform\ntechnical tasks like data analysis. However, large language models (LLMs)\nremain unreliable, and it is unclear whether end-users can effectively identify\nmodel errors $\\unicode{x2014}$ especially in realistic and domain-specific\nscenarios. We surveyed marketing and sales professionals to assess their\nability to critically evaluate LLM-generated analyses of marketing data.\nParticipants were shown natural language explanations of the AI's code,\nrepeatedly informed the AI often makes mistakes, and explicitly prompted to\nidentify them. Yet, participants frequently failed to detect critical flaws\nthat could compromise decision-making, many of which required no technical\nknowledge to recognize. To investigate why, we reformatted AI responses into\nclearly delineated steps and provided alternative approaches for each decision\nto support critical evaluation. While these changes had a positive effect,\nparticipants often struggled to reason through the AI's steps and alternatives.\nOur findings suggest that business professionals cannot reliably verify\nAI-generated data analyses on their own and explore reasons why to inform\nfuture designs. As non-programmers adopt code-generating AI for technical\ntasks, unreliable AI and insufficient human oversight poses risks of unsafe or\nlow-quality decisions.",
      "authors": [
        "Yuvraj Virk",
        "Dongyu Liu"
      ],
      "published": "2025-08-08T17:49:22+00:00",
      "updated": "2025-08-08T17:49:22+00:00",
      "arxiv_id": "2508.06484v1",
      "url": "http://arxiv.org/pdf/2508.06484v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Post-training for Efficient Communication via Convention Formation",
      "abstract": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods.",
      "authors": [
        "Yilun Hua",
        "Evan Wang",
        "Yoav Artzi"
      ],
      "published": "2025-08-08T17:42:16+00:00",
      "updated": "2025-08-08T17:42:16+00:00",
      "arxiv_id": "2508.06482v1",
      "url": "http://arxiv.org/pdf/2508.06482v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "The Problem of Atypicality in LLM-Powered Psychiatry",
      "abstract": "Large language models (LLMs) are increasingly proposed as scalable solutions\nto the global mental health crisis. But their deployment in psychiatric\ncontexts raises a distinctive ethical concern: the problem of atypicality.\nBecause LLMs generate outputs based on population-level statistical\nregularities, their responses -- while typically appropriate for general users\n-- may be dangerously inappropriate when interpreted by psychiatric patients,\nwho often exhibit atypical cognitive or interpretive patterns. We argue that\nstandard mitigation strategies, such as prompt engineering or fine-tuning, are\ninsufficient to resolve this structural risk. Instead, we propose dynamic\ncontextual certification (DCC): a staged, reversible and context-sensitive\nframework for deploying LLMs in psychiatry, inspired by clinical translation\nand dynamic safety models from artificial intelligence governance. DCC reframes\nchatbot deployment as an ongoing epistemic and ethical process that prioritises\ninterpretive safety over static performance benchmarks. Atypicality, we argue,\ncannot be eliminated -- but it can, and must, be proactively managed.",
      "authors": [
        "Bosco Garcia",
        "Eugene Y. S. Chua",
        "Harman Singh Brah"
      ],
      "published": "2025-08-08T17:36:42+00:00",
      "updated": "2025-08-08T17:36:42+00:00",
      "arxiv_id": "2508.06479v1",
      "url": "http://arxiv.org/pdf/2508.06479v1",
      "categories": [
        "cs.CY"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning",
      "abstract": "Haptic captioning is the task of generating natural language descriptions\nfrom haptic signals, such as vibrations, for use in virtual reality,\naccessibility, and rehabilitation applications. While previous multimodal\nresearch has focused primarily on vision and audio, haptic signals for the\nsense of touch remain underexplored. To address this gap, we formalize the\nhaptic captioning task and propose HapticLLaMA, a multimodal sensory language\nmodel that interprets vibration signals into descriptions in a given sensory,\nemotional, or associative category. We investigate two types of haptic\ntokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that\nconvert haptic signals into sequences of discrete units, enabling their\nintegration with the LLaMA model. HapticLLaMA is trained in two stages: (1)\nsupervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,\nand (2) fine-tuning via reinforcement learning from human feedback (RLHF). We\nassess HapticLLaMA's captioning performance using both automated n-gram metrics\nand human evaluation. HapticLLaMA demonstrates strong capability in\ninterpreting haptic vibration signals, achieving a METEOR score of 59.98 and a\nBLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated\ncaptions received human ratings above 3.5 on a 7-point scale, with RLHF\nyielding a 10% improvement in the overall rating distribution, indicating\nstronger alignment with human haptic perception. These findings highlight the\npotential of large language models to process and adapt to sensory data.",
      "authors": [
        "Guimin Hu",
        "Daniel Hershcovich",
        "Hasti Seifi"
      ],
      "published": "2025-08-08T17:25:37+00:00",
      "updated": "2025-08-08T17:25:37+00:00",
      "arxiv_id": "2508.06475v1",
      "url": "http://arxiv.org/pdf/2508.06475v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
      "abstract": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.",
      "authors": [
        "GLM-4. 5 Team",
        ":",
        "Aohan Zeng",
        "Xin Lv",
        "Qinkai Zheng",
        "Zhenyu Hou",
        "Bin Chen",
        "Chengxing Xie",
        "Cunxiang Wang",
        "Da Yin",
        "Hao Zeng",
        "Jiajie Zhang",
        "Kedong Wang",
        "Lucen Zhong",
        "Mingdao Liu",
        "Rui Lu",
        "Shulin Cao",
        "Xiaohan Zhang",
        "Xuancheng Huang",
        "Yao Wei",
        "Yean Cheng",
        "Yifan An",
        "Yilin Niu",
        "Yuanhao Wen",
        "Yushi Bai",
        "Zhengxiao Du",
        "Zihan Wang",
        "Zilin Zhu",
        "Bohan Zhang",
        "Bosi Wen",
        "Bowen Wu",
        "Bowen Xu",
        "Can Huang",
        "Casey Zhao",
        "Changpeng Cai",
        "Chao Yu",
        "Chen Li",
        "Chendi Ge",
        "Chenghua Huang",
        "Chenhui Zhang",
        "Chenxi Xu",
        "Chenzheng Zhu",
        "Chuang Li",
        "Congfeng Yin",
        "Daoyan Lin",
        "Dayong Yang",
        "Dazhi Jiang",
        "Ding Ai",
        "Erle Zhu",
        "Fei Wang",
        "Gengzheng Pan",
        "Guo Wang",
        "Hailong Sun",
        "Haitao Li",
        "Haiyang Li",
        "Haiyi Hu",
        "Hanyu Zhang",
        "Hao Peng",
        "Hao Tai",
        "Haoke Zhang",
        "Haoran Wang",
        "Haoyu Yang",
        "He Liu",
        "He Zhao",
        "Hongwei Liu",
        "Hongxi Yan",
        "Huan Liu",
        "Huilong Chen",
        "Ji Li",
        "Jiajing Zhao",
        "Jiamin Ren",
        "Jian Jiao",
        "Jiani Zhao",
        "Jianyang Yan",
        "Jiaqi Wang",
        "Jiayi Gui",
        "Jiayue Zhao",
        "Jie Liu",
        "Jijie Li",
        "Jing Li",
        "Jing Lu",
        "Jingsen Wang",
        "Jingwei Yuan",
        "Jingxuan Li",
        "Jingzhao Du",
        "Jinhua Du",
        "Jinxin Liu",
        "Junkai Zhi",
        "Junli Gao",
        "Ke Wang",
        "Lekang Yang",
        "Liang Xu",
        "Lin Fan",
        "Lindong Wu",
        "Lintao Ding",
        "Lu Wang",
        "Man Zhang",
        "Minghao Li",
        "Minghuan Xu",
        "Mingming Zhao",
        "Mingshu Zhai",
        "Pengfan Du",
        "Qian Dong",
        "Shangde Lei",
        "Shangqing Tu",
        "Shangtong Yang",
        "Shaoyou Lu",
        "Shijie Li",
        "Shuang Li",
        "Shuang-Li",
        "Shuxun Yang",
        "Sibo Yi",
        "Tianshu Yu",
        "Wei Tian",
        "Weihan Wang",
        "Wenbo Yu",
        "Weng Lam Tam",
        "Wenjie Liang",
        "Wentao Liu",
        "Xiao Wang",
        "Xiaohan Jia",
        "Xiaotao Gu",
        "Xiaoying Ling",
        "Xin Wang",
        "Xing Fan",
        "Xingru Pan",
        "Xinyuan Zhang",
        "Xinze Zhang",
        "Xiuqing Fu",
        "Xunkai Zhang",
        "Yabo Xu",
        "Yandong Wu",
        "Yida Lu",
        "Yidong Wang",
        "Yilin Zhou",
        "Yiming Pan",
        "Ying Zhang",
        "Yingli Wang",
        "Yingru Li",
        "Yinpei Su",
        "Yipeng Geng",
        "Yitong Zhu",
        "Yongkun Yang",
        "Yuhang Li",
        "Yuhao Wu",
        "Yujiang Li",
        "Yunan Liu",
        "Yunqing Wang",
        "Yuntao Li",
        "Yuxuan Zhang",
        "Zezhen Liu",
        "Zhen Yang",
        "Zhengda Zhou",
        "Zhongpei Qiao",
        "Zhuoer Feng",
        "Zhuorui Liu",
        "Zichen Zhang",
        "Zihan Wang",
        "Zijun Yao",
        "Zikang Wang",
        "Ziqiang Liu",
        "Ziwei Chai",
        "Zixuan Li",
        "Zuodong Zhao",
        "Wenguang Chen",
        "Jidong Zhai",
        "Bin Xu",
        "Minlie Huang",
        "Hongning Wang",
        "Juanzi Li",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "published": "2025-08-08T17:21:06+00:00",
      "updated": "2025-08-08T17:21:06+00:00",
      "arxiv_id": "2508.06471v1",
      "url": "http://arxiv.org/pdf/2508.06471v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection",
      "abstract": "The growing legal and ethical scrutiny of large language models (LLMs)\nnecessitates effective machine unlearning, particularly for sensitive or\nunauthorized data. Existing empirical methods often yield incomplete forgetting\nor unintended degradation of unrelated knowledge due to poor localization. In\nthis work, we propose GRIN: a modular and targeted framework for LLM\nunlearning. GRIN introduces a novel gradient-ratio-based metric to identify\nparameters most responsible for memorizing forget data. We then perform\nselective noise injection into these parameters prior to fine-tuning, which\nimproves unlearning performance while maintaining model utility. Finally, we\npropose new evaluation metrics tailored to the LLM setting and validate our\napproach on standard benchmarks such as TOFU, WMDP, and SafePKU.",
      "authors": [
        "Ameya Anjarlekar",
        "Sandeep Pombra"
      ],
      "published": "2025-08-08T17:15:32+00:00",
      "updated": "2025-08-08T17:15:32+00:00",
      "arxiv_id": "2508.06467v1",
      "url": "http://arxiv.org/pdf/2508.06467v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive fluency and\nreasoning capabilities, but their potential for misuse has raised growing\nconcern. In this paper, we present ScamAgent, an autonomous multi-turn agent\nbuilt on top of LLMs, capable of generating highly realistic scam call scripts\nthat simulate real-world fraud scenarios. Unlike prior work focused on\nsingle-shot prompt misuse, ScamAgent maintains dialogue memory, adapts\ndynamically to simulated user responses, and employs deceptive persuasion\nstrategies across conversational turns. We show that current LLM safety\nguardrails, including refusal mechanisms and content filters, are ineffective\nagainst such agent-based threats. Even models with strong prompt-level\nsafeguards can be bypassed when prompts are decomposed, disguised, or delivered\nincrementally within an agent framework. We further demonstrate the\ntransformation of scam scripts into lifelike voice calls using modern\ntext-to-speech systems, completing a fully automated scam pipeline. Our\nfindings highlight an urgent need for multi-turn safety auditing, agent-level\ncontrol frameworks, and new methods to detect and disrupt conversational\ndeception powered by generative AI.",
      "authors": [
        "Sanket Badhe"
      ],
      "published": "2025-08-08T17:01:41+00:00",
      "updated": "2025-08-08T17:01:41+00:00",
      "arxiv_id": "2508.06457v1",
      "url": "http://arxiv.org/pdf/2508.06457v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Text Embedded Swin-UMamba for DeepLesion Segmentation",
      "abstract": "Segmentation of lesions on CT enables automatic measurement for clinical\nassessment of chronic diseases (e.g., lymphoma). Integrating large language\nmodels (LLMs) into the lesion segmentation workflow offers the potential to\ncombine imaging features with descriptions of lesion characteristics from the\nradiology reports. In this study, we investigate the feasibility of integrating\ntext into the Swin-UMamba architecture for the task of lesion segmentation. The\npublicly available ULS23 DeepLesion dataset was used along with short-form\ndescriptions of the findings from the reports. On the test dataset, a high Dice\nScore of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for\nlesion segmentation. The proposed Text-Swin-UMamba model outperformed prior\napproaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <\n0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by\n1.74% and 0.22%, respectively. The dataset and code can be accessed at\nhttps://github.com/ruida/LLM-Swin-UMamba",
      "authors": [
        "Ruida Cheng",
        "Tejas Sudharshan Mathai",
        "Pritam Mukherjee",
        "Benjamin Hou",
        "Qingqing Zhu",
        "Zhiyong Lu",
        "Matthew McAuliffe",
        "Ronald M. Summers"
      ],
      "published": "2025-08-08T16:54:06+00:00",
      "updated": "2025-08-08T16:54:06+00:00",
      "arxiv_id": "2508.06453v1",
      "url": "http://arxiv.org/pdf/2508.06453v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning",
      "abstract": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
      "authors": [
        "Lingkun Long",
        "Rubing Yang",
        "Yushi Huang",
        "Desheng Hui",
        "Ao Zhou",
        "Jianlei Yang"
      ],
      "published": "2025-08-08T16:42:38+00:00",
      "updated": "2025-08-08T16:42:38+00:00",
      "arxiv_id": "2508.06447v1",
      "url": "http://arxiv.org/pdf/2508.06447v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking",
      "abstract": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media.",
      "authors": [
        "Abolfazl Ansari",
        "Delvin Ce Zhang",
        "Nafis Irtiza Tripto",
        "Dongwon Lee"
      ],
      "published": "2025-08-08T16:38:33+00:00",
      "updated": "2025-08-08T16:38:33+00:00",
      "arxiv_id": "2508.06445v1",
      "url": "http://arxiv.org/pdf/2508.06445v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages",
      "abstract": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.",
      "authors": [
        "Andrea Nasuto",
        "Stefano Maria Iacus",
        "Francisco Rowe",
        "Devika Jain"
      ],
      "published": "2025-08-08T16:23:24+00:00",
      "updated": "2025-08-08T16:23:24+00:00",
      "arxiv_id": "2508.06435v1",
      "url": "http://arxiv.org/pdf/2508.06435v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Memp: Exploring Agent Procedural Memory",
      "abstract": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.",
      "authors": [
        "Runnan Fang",
        "Yuan Liang",
        "Xiaobin Wang",
        "Jialong Wu",
        "Shuofei Qiao",
        "Pengjun Xie",
        "Fei Huang",
        "Huajun Chen",
        "Ningyu Zhang"
      ],
      "published": "2025-08-08T16:20:56+00:00",
      "updated": "2025-08-08T16:20:56+00:00",
      "arxiv_id": "2508.06433v1",
      "url": "http://arxiv.org/pdf/2508.06433v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Quantifying Conversation Drift in MCP via Latent Polytope",
      "abstract": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy.",
      "authors": [
        "Haoran Shi",
        "Hongwei Yao",
        "Shuo Shao",
        "Shaopeng Jiao",
        "Ziqi Peng",
        "Zhan Qin",
        "Cong Wang"
      ],
      "published": "2025-08-08T16:05:27+00:00",
      "updated": "2025-08-08T16:05:27+00:00",
      "arxiv_id": "2508.06418v1",
      "url": "http://arxiv.org/pdf/2508.06418v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "What Builds Effective In-Context Examples for Code Generation?",
      "abstract": "In-Context Learning (ICL) has emerged as a promising solution to enhance the\ncode generation capabilities of Large Language Models (LLMs), which\nincorporates code examples inside the prompt to let LLMs learn from\ndemonstrations. However, despite the substantial effectiveness of the code\nexample-based ICL approach, the specific features (e.g., identifier naming\nstyles, code formatting, solution insight) within the ICL-provided code\nexamples that significantly contribute to the ICL's effectiveness remain\nunclear. This paper systematically investigates the impact of various code\nfeatures on ICL with code examples through controlled ablation studies. Our\nfindings reveal that the appropriate naming of variables and functions is\ncrucial for effective code generation, with their elimination leading to\nperformance decreases of up to 30 percentage points. We further demonstrate\nthat LLMs prioritize semantically meaningful identifier names over formatting\nconventions, with language-specific preferences regarding identifier verbosity.\nAdditionally, our investigation into ICL's potential for enhancing reflection\nand inference capabilities reveals that current LLMs struggle to extract\ngeneralizable problem-solving insights from similar code solutions, despite\nbeing capable of utilizing direct information effectively. These findings are\nexpected to provide valuable insights for optimizing ICL systems in code\ngeneration applications and highlight fundamental challenges in\nreflection-based learning for code generation tasks.",
      "authors": [
        "Dongze Li",
        "Songqiang Chen",
        "Jialun Cao",
        "Shing-Chi Cheung"
      ],
      "published": "2025-08-08T15:58:11+00:00",
      "updated": "2025-08-08T15:58:11+00:00",
      "arxiv_id": "2508.06414v1",
      "url": "http://arxiv.org/pdf/2508.06414v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Sample-efficient LLM Optimization with Reset Replay",
      "abstract": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data.",
      "authors": [
        "Zichuan Liu",
        "Jinyu Wang",
        "Lei Song",
        "Jiang Bian"
      ],
      "published": "2025-08-08T15:56:49+00:00",
      "updated": "2025-08-08T15:56:49+00:00",
      "arxiv_id": "2508.06412v1",
      "url": "http://arxiv.org/pdf/2508.06412v1",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "When AIOps Become \"AI Oops\": Subverting LLM-driven IT Operations via Telemetry Manipulation",
      "abstract": "AI for IT Operations (AIOps) is transforming how organizations manage complex\nsoftware systems by automating anomaly detection, incident diagnosis, and\nremediation. Modern AIOps solutions increasingly rely on autonomous LLM-based\nagents to interpret telemetry data and take corrective actions with minimal\nhuman intervention, promising faster response times and operational cost\nsavings.\n  In this work, we perform the first security analysis of AIOps solutions,\nshowing that, once again, AI-driven automation comes with a profound security\ncost. We demonstrate that adversaries can manipulate system telemetry to\nmislead AIOps agents into taking actions that compromise the integrity of the\ninfrastructure they manage. We introduce techniques to reliably inject\ntelemetry data using error-inducing requests that influence agent behavior\nthrough a form of adversarial reward-hacking; plausible but incorrect system\nerror interpretations that steer the agent's decision-making. Our attack\nmethodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing,\nand LLM-driven adversarial input generation--and operates without any prior\nknowledge of the target system.\n  To counter this threat, we propose AIOpsShield, a defense mechanism that\nsanitizes telemetry data by exploiting its structured nature and the minimal\nrole of user-generated content. Our experiments show that AIOpsShield reliably\nblocks telemetry-based attacks without affecting normal agent performance.\n  Ultimately, this work exposes AIOps as an emerging attack vector for system\ncompromise and underscores the urgent need for security-aware AIOps design.",
      "authors": [
        "Dario Pasquini",
        "Evgenios M. Kornaropoulos",
        "Giuseppe Ateniese",
        "Omer Akgul",
        "Athanasios Theocharis",
        "Petros Efstathopoulos"
      ],
      "published": "2025-08-08T15:25:31+00:00",
      "updated": "2025-08-08T15:25:31+00:00",
      "arxiv_id": "2508.06394v1",
      "url": "http://arxiv.org/pdf/2508.06394v1",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime.",
      "authors": [
        "Lanlan Qiu",
        "Xiao Pu",
        "Yeqi Feng",
        "Tianxing He"
      ],
      "published": "2025-08-08T15:17:24+00:00",
      "updated": "2025-08-08T15:17:24+00:00",
      "arxiv_id": "2508.06388v1",
      "url": "http://arxiv.org/pdf/2508.06388v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation",
      "abstract": "Text-to-SQL bridges the gap between natural language and structured database\nlanguage, thus allowing non-technical users to easily query databases.\nTraditional approaches model text-to-SQL as a direct translation task, where a\ngiven Natural Language Query (NLQ) is mapped to an SQL command. Recent advances\nin large language models (LLMs) have significantly improved translation\naccuracy, however, these methods all require that the target database is\npre-specified. This becomes problematic in scenarios with multiple extensive\ndatabases, where identifying the correct database becomes a crucial yet\noverlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL\nframework to identify the user's intended database before generating SQL\nqueries. Our approach leverages LLMs and prompt engineering to extract implicit\ninformation from natural language queries (NLQs) in the form of a ruleset. We\nthen train a large db\\_id prediction model, which includes a RoBERTa-based\nfinetuned encoder, to predict the correct Database identifier (db\\_id) based on\nboth the NLQ and the LLM-generated rules. Finally, we refine the generated SQL\nby using critic agents to correct errors. Experimental results demonstrate that\nour framework outperforms the current state-of-the-art models in both database\nintent prediction and SQL generation accuracy.",
      "authors": [
        "Anurag Tripathi",
        "Vaibhav Patle",
        "Abhinav Jain",
        "Ayush Pundir",
        "Sairam Menon",
        "Ajeet Kumar Singh",
        "Dorien Herremans"
      ],
      "published": "2025-08-08T15:16:36+00:00",
      "updated": "2025-08-11T04:36:43+00:00",
      "arxiv_id": "2508.06387v2",
      "url": "http://arxiv.org/pdf/2508.06387v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions",
      "abstract": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation.",
      "authors": [
        "Anubhav Jangra",
        "Bahareh Sarrafzadeh",
        "Adrian de Wynter",
        "Silviu Cucerzan",
        "Sujay Kumar Jauhar"
      ],
      "published": "2025-08-08T15:07:31+00:00",
      "updated": "2025-08-08T15:07:31+00:00",
      "arxiv_id": "2508.06374v1",
      "url": "http://arxiv.org/pdf/2508.06374v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models",
      "abstract": "The Speaker Diarization and Recognition (SDR) task aims to predict \"who spoke\nwhen and what\" within an audio clip, which is a crucial task in various\nreal-world multi-speaker scenarios such as meeting transcription and dialogue\nsystems. Existing SDR systems typically adopt a cascaded framework, combining\nmultiple modules such as speaker diarization (SD) and automatic speech\nrecognition (ASR). The cascaded systems suffer from several limitations, such\nas error propagation, difficulty in handling overlapping speech, and lack of\njoint optimization for exploring the synergy between SD and ASR tasks. To\naddress these limitations, we introduce SpeakerLM, a unified multimodal large\nlanguage model for SDR that jointly performs SD and ASR in an end-to-end\nmanner. Moreover, to facilitate diverse real-world scenarios, we incorporate a\nflexible speaker registration mechanism into SpeakerLM, enabling SDR under\ndifferent speaker registration settings. SpeakerLM is progressively developed\nwith a multi-stage training strategy on large-scale real data. Extensive\nexperiments show that SpeakerLM demonstrates strong data scaling capability and\ngeneralizability, outperforming state-of-the-art cascaded baselines on both\nin-domain and out-of-domain public SDR benchmarks. Furthermore, experimental\nresults show that the proposed speaker registration mechanism effectively\nensures robust SDR performance of SpeakerLM across diverse speaker registration\nconditions and varying numbers of registered speakers.",
      "authors": [
        "Han Yin",
        "Yafeng Chen",
        "Chong Deng",
        "Luyao Cheng",
        "Hui Wang",
        "Chao-Hong Tan",
        "Qian Chen",
        "Wen Wang",
        "Xiangang Li"
      ],
      "published": "2025-08-08T15:04:00+00:00",
      "updated": "2025-08-08T15:04:00+00:00",
      "arxiv_id": "2508.06372v1",
      "url": "http://arxiv.org/pdf/2508.06372v1",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD"
    },
    {
      "title": "Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned",
      "abstract": "Legal decision-making process requires the availability of comprehensive and\ndetailed legislative background knowledge and up-to-date information on legal\ncases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a\nvaluable tool to facilitate access to legal information, to be queried and\nexploited for the purpose, and to enable advanced reasoning and machine\nlearning applications. Indeed, legal KGs may act as knowledge intensive\ncomponent to be used by pre-dictive machine learning solutions supporting the\ndecision process of the legal expert. Nevertheless, a few KGs can be found in\nthe legal domain. To fill this gap, we developed a legal KG targeting legal\ncases of violence against women, along with clear adopted methodologies.\nSpecifically, the paper introduces two complementary approaches for automated\nlegal KG construction; a systematic bottom-up approach, customized for the\nlegal domain, and a new solution leveraging Large Language Models. Starting\nfrom legal sentences publicly available from the European Court of Justice, the\nsolutions integrate structured data extraction, ontology development, and\nsemantic enrichment to produce KGs tailored for legal cases involving violence\nagainst women. After analyzing and comparing the results of the two approaches,\nthe developed KGs are validated via suitable competency questions. The obtained\nKG may be impactful for multiple purposes: can improve the accessibility to\nlegal information both to humans and machine, can enable complex queries and\nmay constitute an important knowledge component to be possibly exploited by\nmachine learning tools tailored for predictive justice.",
      "authors": [
        "Claudia dAmato",
        "Giuseppe Rubini",
        "Francesco Didio",
        "Donato Francioso",
        "Fatima Zahra Amara",
        "Nicola Fanizzi"
      ],
      "published": "2025-08-08T14:59:54+00:00",
      "updated": "2025-08-08T14:59:54+00:00",
      "arxiv_id": "2508.06368v1",
      "url": "http://arxiv.org/pdf/2508.06368v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts",
      "abstract": "Large Language Models (LLMs) have been widely deployed in reasoning,\nplanning, and decision-making tasks, making their trustworthiness a critical\nconcern. The potential for intentional deception, where an LLM deliberately\nfabricates or conceals information to serve a hidden objective, remains a\nsignificant and underexplored threat. Existing studies typically induce such\ndeception by explicitly setting a \"hidden\" objective through prompting or\nfine-tuning, which may not fully reflect real-world human-LLM interactions.\nMoving beyond this human-induced deception, we investigate LLMs' self-initiated\ndeception on benign prompts. To address the absence of ground truth in this\nevaluation, we propose a novel framework using \"contact searching questions.\"\nThis framework introduces two statistical metrics derived from psychological\nprinciples to quantify the likelihood of deception. The first, the Deceptive\nIntention Score, measures the model's bias towards a hidden objective. The\nsecond, Deceptive Behavior Score, measures the inconsistency between the LLM's\ninternal belief and its expressed output. Upon evaluating 14 leading LLMs, we\nfind that both metrics escalate as task difficulty increases, rising in\nparallel for most models. Building on these findings, we formulate a\nmathematical model to explain this behavior. These results reveal that even the\nmost advanced LLMs exhibit an increasing tendency toward deception when\nhandling complex problems, raising critical concerns for the deployment of LLM\nagents in complex and crucial domains.",
      "authors": [
        "Zhaomin Wu",
        "Mingzhe Du",
        "See-Kiong Ng",
        "Bingsheng He"
      ],
      "published": "2025-08-08T14:46:35+00:00",
      "updated": "2025-08-08T14:46:35+00:00",
      "arxiv_id": "2508.06361v1",
      "url": "http://arxiv.org/pdf/2508.06361v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Cyberbullying Detection via Aggression-Enhanced Prompting",
      "abstract": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks.",
      "authors": [
        "Aisha Saeid",
        "Anu Sabu",
        "Girish A. Koushik",
        "Ferrante Neri",
        "Diptesh Kanojia"
      ],
      "published": "2025-08-08T14:46:05+00:00",
      "updated": "2025-08-08T14:46:05+00:00",
      "arxiv_id": "2508.06360v1",
      "url": "http://arxiv.org/pdf/2508.06360v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "LLM Unlearning Without an Expert Curated Dataset",
      "abstract": "Modern large language models often encode sensitive, harmful, or copyrighted\nknowledge, raising the need for post-hoc unlearning-the ability to remove\nspecific domains of knowledge from a model without full retraining. A major\nbottleneck in current unlearning pipelines is constructing effective forget\nsets-datasets that approximate the target domain and guide the model to forget\nit. In this work, we introduce a scalable, automated approach to generate\nhigh-quality forget sets using language models themselves. Our method\nsynthesizes textbook-style data through a structured prompting pipeline,\nrequiring only a domain name as input. Through experiments on unlearning\nbiosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic\ndatasets consistently outperform the baseline synthetic alternatives and are\ncomparable to the expert-curated ones. Additionally, ablation studies reveal\nthat the multi-step generation pipeline significantly boosts data diversity,\nwhich in turn improves unlearning utility. Overall, our findings suggest that\nsynthetic datasets offer a promising path toward practical, scalable unlearning\nfor a wide range of emerging domains without the need for manual intervention.\nWe release our code and dataset at\nhttps://github.com/xyzhu123/Synthetic_Textbook.",
      "authors": [
        "Xiaoyuan Zhu",
        "Muru Zhang",
        "Ollie Liu",
        "Robin Jia",
        "Willie Neiswanger"
      ],
      "published": "2025-08-08T14:30:08+00:00",
      "updated": "2025-08-08T14:30:08+00:00",
      "arxiv_id": "2508.06595v1",
      "url": "http://arxiv.org/pdf/2508.06595v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models",
      "abstract": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.",
      "authors": [
        "Yingxian Chen",
        "Jiahui Liu",
        "Ruifan Di",
        "Yanwei Li",
        "Chirui Chang",
        "Shizhen Zhao",
        "Wilton W. T. Fok",
        "Xiaojuan Qi",
        "Yik-Chung Wu"
      ],
      "published": "2025-08-08T14:30:05+00:00",
      "updated": "2025-08-08T14:30:05+00:00",
      "arxiv_id": "2508.06350v1",
      "url": "http://arxiv.org/pdf/2508.06350v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities",
      "abstract": "Designing socially active streets has long been a goal of urban planning, yet\nexisting quantitative research largely measures pedestrian volume rather than\nthe quality of social interactions. We hypothesize that street view imagery --\nan inexpensive data source with global coverage -- contains latent social\ninformation that can be extracted and interpreted through established social\nscience theory. As a proof of concept, we analyzed 2,998 street view images\nfrom 15 cities using a multimodal large language model guided by Mehta's\ntaxonomy of passive, fleeting, and enduring sociability -- one illustrative\nexample of a theory grounded in urban design that could be substituted or\ncomplemented by other sociological frameworks. We then used linear regression\nmodels, controlling for factors like weather, time of day, and pedestrian\ncounts, to test whether the inferred sociability measures correlate with\ncity-level place attachment scores from the World Values Survey and with\nenvironmental predictors (e.g., green, sky, and water view indices) derived\nfrom individual street view images. Results aligned with long-standing urban\nplanning theory: the sky view index was associated with all three sociability\ntypes, the green view index predicted enduring sociability, and place\nattachment was positively associated with fleeting sociability. These results\nprovide preliminary evidence that street view images can be used to infer\nrelationships between specific types of social interactions and built\nenvironment variables. Further research could establish street view imagery as\na scalable, privacy-preserving tool for studying urban sociability, enabling\ncross-cultural theory testing and evidence-based design of socially vibrant\ncities.",
      "authors": [
        "Kieran Elrod",
        "Katherine Flanigan",
        "Mario BergÃ©s"
      ],
      "published": "2025-08-08T14:15:58+00:00",
      "updated": "2025-08-08T14:15:58+00:00",
      "arxiv_id": "2508.06342v1",
      "url": "http://arxiv.org/pdf/2508.06342v1",
      "categories": [
        "cs.CV",
        "cs.SI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision",
      "abstract": "This paper presents a portable, GPU-accelerated implementation of a QR-based\nsingular value computation algorithm in Julia. The singular value ecomposition\n(SVD) is a fundamental numerical tool in scientific computing and machine\nlearning, providing optimal low-rank matrix approximations. Its importance has\nincreased even more in large-scale machine learning pipelines, including large\nlanguage models (LLMs), where it enables low-rank adaptation (LoRA). The\nimplemented algorithm is based on the classic two-stage QR reduction,\nconsisting of successive matrix reduction to band form and bidiagonal form. Our\nimplementation leverages Julia's multiple dispatch and metaprogramming\ncapabilities, integrating with the GPUArrays and KernelAbstractions frameworks\nto provide a unified type and hardware-agnostic function. It supports diverse\nGPU architectures and data types, and is, to our knowledge, the first\nGPU-accelerated singular value implementation to support Apple Metal GPUs and\nhalf precision. Performance results on multiple GPU backends and data types\ndemonstrate that portability does not require sacrificing performance: the\nunified function outperforms most linear algebra libraries (MAGMA, SLATE,\nrocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%\nof the performance of cuSOLVER for large matrices.",
      "authors": [
        "Evelyne Ringoot",
        "Rabab Alomairy",
        "Valentin Churavy",
        "Alan Edelman"
      ],
      "published": "2025-08-08T14:14:13+00:00",
      "updated": "2025-08-08T14:14:13+00:00",
      "arxiv_id": "2508.06339v1",
      "url": "http://arxiv.org/pdf/2508.06339v1",
      "categories": [
        "cs.DC",
        "cs.MS"
      ],
      "primary_category": "cs.DC"
    },
    {
      "title": "M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation",
      "abstract": "Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables\ndiverse multimodal inputs but remains limited to single-modality outputs,\nrestricting expressive capacity and practical utility. In contrast, real-world\napplications often demand both multimodal inputs and multimodal outputs for\neffective communication and grounded reasoning. Motivated by the recent success\nof Reinforcement Learning (RL) in complex reasoning tasks for Large Language\nModels (LLMs), we adopt RL as a principled and effective paradigm to address\nthe multi-step, outcome-driven challenges inherent in multimodal output\ngeneration. Here, we introduce M2IO-R1, a novel framework for Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal\ninputs and outputs. Central to our framework is an RL-based inserter,\nInserter-R1-3B, trained with Group Relative Policy Optimization to guide image\nselection and placement in a controllable and semantically aligned manner.\nEmpirical results show that our lightweight 3B inserter achieves strong\nreasoning capabilities with significantly reduced latency, outperforming\nbaselines in both quality and efficiency.",
      "authors": [
        "Zhiyou Xiao",
        "Qinhan Yu",
        "Binghui Li",
        "Geng Chen",
        "Chong Chen",
        "Wentao Zhang"
      ],
      "published": "2025-08-08T14:00:19+00:00",
      "updated": "2025-08-08T14:00:19+00:00",
      "arxiv_id": "2508.06328v1",
      "url": "http://arxiv.org/pdf/2508.06328v1",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha Mining in Quantitative Trading",
      "abstract": "Alpha factor mining is a fundamental task in quantitative trading, aimed at\ndiscovering interpretable signals that can predict asset returns beyond\nsystematic market risk. While traditional methods rely on manual formula design\nor heuristic search with machine learning, recent advances have leveraged Large\nLanguage Models (LLMs) for automated factor discovery. However, existing\nLLM-based alpha mining approaches remain limited in terms of automation,\ngenerality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel,\nsimple, yet effective and efficient LLM-based framework for fully automated\nformulaic alpha mining. Our method features a dual-chain architecture,\nconsisting of a Factor Generation Chain and a Factor Optimization Chain, which\niteratively generate, evaluate, and refine candidate alpha factors using only\nmarket data, while leveraging backtest feedback and prior optimization\nknowledge. The two chains work synergistically to enable high-quality alpha\ndiscovery without human intervention and offer strong scalability. Extensive\nexperiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha\noutperforms existing baselines across multiple metrics, presenting a promising\ndirection for LLM-driven quantitative research.",
      "authors": [
        "Lang Cao",
        "Zekun Xi",
        "Long Liao",
        "Ziwei Yang",
        "Zheng Cao"
      ],
      "published": "2025-08-08T13:39:05+00:00",
      "updated": "2025-08-08T13:39:05+00:00",
      "arxiv_id": "2508.06312v1",
      "url": "http://arxiv.org/pdf/2508.06312v1",
      "categories": [
        "cs.CE"
      ],
      "primary_category": "cs.CE"
    },
    {
      "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC",
      "abstract": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible.",
      "authors": [
        "Ruichong Zhang"
      ],
      "published": "2025-08-08T13:35:40+00:00",
      "updated": "2025-08-08T13:35:40+00:00",
      "arxiv_id": "2508.06309v1",
      "url": "http://arxiv.org/pdf/2508.06309v1",
      "categories": [
        "cs.CL",
        "math.PR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models",
      "abstract": "Explorative flow visualization allows domain experts to analyze complex flow\nstructures by interactively investigating flow patterns. However, traditional\nvisual interfaces often rely on specialized graphical representations and\ninteractions, which require additional effort to learn and use. Natural\nlanguage interaction offers a more intuitive alternative, but teaching machines\nto recognize diverse scientific concepts and extract corresponding structures\nfrom flow data poses a significant challenge. In this paper, we introduce an\nautomated framework that aligns flow pattern representations with the semantic\nspace of large language models (LLMs), eliminating the need for manual\nlabeling. Our approach encodes streamline segments using a denoising\nautoencoder and maps the generated flow pattern representations to LLM\nembeddings via a projector layer. This alignment empowers semantic matching\nbetween textual embeddings and flow representations through an attention\nmechanism, enabling the extraction of corresponding flow patterns based on\ntextual descriptions. To enhance accessibility, we develop an interactive\ninterface that allows users to query and visualize flow structures using\nnatural language. Through case studies, we demonstrate the effectiveness of our\nframework in enabling intuitive and intelligent flow exploration.",
      "authors": [
        "Weihan Zhang",
        "Jun Tao"
      ],
      "published": "2025-08-08T13:20:37+00:00",
      "updated": "2025-08-08T13:20:37+00:00",
      "arxiv_id": "2508.06300v1",
      "url": "http://arxiv.org/pdf/2508.06300v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
      "abstract": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
      "authors": [
        "Yanyu Liu",
        "Jingying Fu",
        "Sixiang Liu",
        "Yitian Zou",
        "You Fu",
        "Jiehan Zhou",
        "Shouhua Zhang"
      ],
      "published": "2025-08-08T13:19:30+00:00",
      "updated": "2025-08-08T13:19:30+00:00",
      "arxiv_id": "2508.06297v1",
      "url": "http://arxiv.org/pdf/2508.06297v1",
      "categories": [
        "cs.DC"
      ],
      "primary_category": "cs.DC"
    },
    {
      "title": "LLM Robustness Leaderboard v1 --Technical report",
      "abstract": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community.",
      "authors": [
        "Pierre PeignÃ© - Lefebvre",
        "Quentin Feuillade-Montixi",
        "Tom David",
        "Nicolas Miailhe"
      ],
      "published": "2025-08-08T13:15:40+00:00",
      "updated": "2025-08-08T13:15:40+00:00",
      "arxiv_id": "2508.06296v1",
      "url": "http://arxiv.org/pdf/2508.06296v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Leveraging LLMs for Scalable Non-intrusive Speech Quality Assessment",
      "abstract": "Non-intrusive speech quality assessment (SQA) systems suffer from limited\ntraining data and costly human annotations, hindering their generalization to\nreal-time conferencing calls. In this work, we propose leveraging large\nlanguage models (LLMs) as pseudo-raters for speech quality to address these\ndata bottlenecks. We construct LibriAugmented, a dataset consisting of 101,129\nspeech clips with simulated degradations labeled by a fine-tuned auditory LLM\n(Vicuna-7b-v1.5). We compare three training strategies: using human-labeled\ndata, using LLM-labeled data, and a two-stage approach (pretraining on LLM\nlabels, then fine-tuning on human labels), using both DNSMOS Pro and DeePMOS.\nWe test on several datasets across languages and quality degradations. While\nLLM-labeled training yields mixed results compared to human-labeled training,\nwe provide empirical evidence that the two-stage approach improves the\ngeneralization performance (e.g., DNSMOS Pro achieves 0.63 vs. 0.55 PCC on\nNISQA_TEST_LIVETALK and 0.73 vs. 0.65 PCC on Tencent with reverb). Our findings\ndemonstrate the potential of using LLMs as scalable pseudo-raters for speech\nquality assessment, offering a cost-effective solution to the data limitation\nproblem.",
      "authors": [
        "Fredrik Cumlin",
        "Xinyu Liang",
        "Anubhab Ghosh",
        "Saikat Chatterjee"
      ],
      "published": "2025-08-08T13:05:31+00:00",
      "updated": "2025-08-08T13:05:31+00:00",
      "arxiv_id": "2508.06284v1",
      "url": "http://arxiv.org/pdf/2508.06284v1",
      "categories": [
        "eess.AS"
      ],
      "primary_category": "eess.AS"
    },
    {
      "title": "Mitigating Undesired Conditions in Flexible Production with Product-Process-Resource Asset Knowledge Graphs",
      "abstract": "Contemporary industrial cyber-physical production systems (CPPS) composed of\nrobotic workcells face significant challenges in the analysis of undesired\nconditions due to the flexibility of Industry 4.0 that disrupts traditional\nquality assurance mechanisms. This paper presents a novel industry-oriented\nsemantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),\nwhich is designed to analyze and mitigate undesired conditions in flexible\nCPPS. Built on top of the well-proven Product-Process-Resource (PPR) model\noriginating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses\nshortcomings of conventional model-driven engineering for CPPS, particularly\ninadequate undesired condition and error handling representation. The\nintegration of semantic technologies with large language models (LLMs) provides\nintuitive interfaces for factory operators, production planners, and engineers\nto interact with the entire model using natural language. Evaluation with the\nuse case addressing electric vehicle battery remanufacturing demonstrates that\nthe PPR-AKG approach efficiently supports resource allocation based on\nexplicitly represented capabilities as well as identification and mitigation of\nundesired conditions in production. The key contributions include (1) a\nholistic PPR-AKG model capturing multi-dimensional production knowledge, and\n(2) the useful combination of the PPR-AKG with LLM-based chatbots for human\ninteraction.",
      "authors": [
        "Petr Novak",
        "Stefan Biffl",
        "Marek Obitko",
        "Petr Kadera"
      ],
      "published": "2025-08-08T12:58:14+00:00",
      "updated": "2025-08-08T12:58:14+00:00",
      "arxiv_id": "2508.06278v1",
      "url": "http://arxiv.org/pdf/2508.06278v1",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "Large Language Model Data Generation for Enhanced Intent Recognition in German Speech",
      "abstract": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility.",
      "authors": [
        "Theresa Pekarek Rosin",
        "Burak Can Kaplan",
        "Stefan Wermter"
      ],
      "published": "2025-08-08T12:54:09+00:00",
      "updated": "2025-08-08T12:54:09+00:00",
      "arxiv_id": "2508.06277v1",
      "url": "http://arxiv.org/pdf/2508.06277v1",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Llasa+: Free Lunch for Accelerated and Streaming Llama-Based Speech Synthesis",
      "abstract": "Recent progress in text-to-speech (TTS) has achieved impressive naturalness\nand flexibility, especially with the development of large language model\n(LLM)-based approaches. However, existing autoregressive (AR) structures and\nlarge-scale models, such as Llasa, still face significant challenges in\ninference latency and streaming synthesis. To deal with the limitations, we\nintroduce Llasa+, an accelerated and streaming TTS model built on Llasa.\nSpecifically, to accelerate the generation process, we introduce two\nplug-and-play Multi-Token Prediction (MTP) modules following the frozen\nbackbone. These modules allow the model to predict multiple tokens in one AR\nstep. Additionally, to mitigate potential error propagation caused by\ninaccurate MTP, we design a novel verification algorithm that leverages the\nfrozen backbone to validate the generated tokens, thus allowing Llasa+ to\nachieve speedup without sacrificing generation quality. Furthermore, we design\na causal decoder that enables streaming speech reconstruction from tokens.\nExtensive experiments show that Llasa+ achieves a 1.48X speedup without\nsacrificing generation quality, despite being trained only on LibriTTS.\nMoreover, the MTP-and-verification framework can be applied to accelerate any\nLLM-based model. All codes and models are publicly available at\nhttps://github.com/ASLP-lab/LLaSA_Plus.",
      "authors": [
        "Wenjie Tian",
        "Xinfa Zhu",
        "Hanke Xie",
        "Zhen Ye",
        "Wei Xue",
        "Lei Xie"
      ],
      "published": "2025-08-08T12:28:34+00:00",
      "updated": "2025-08-08T12:28:34+00:00",
      "arxiv_id": "2508.06262v1",
      "url": "http://arxiv.org/pdf/2508.06262v1",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.SD"
    },
    {
      "title": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning",
      "abstract": "Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method.",
      "authors": [
        "Zhangquan Chen",
        "Ruihui Zhao",
        "Chuwei Luo",
        "Mingze Sun",
        "Xinlei Yu",
        "Yangyang Kang",
        "Ruqi Huang"
      ],
      "published": "2025-08-08T12:26:20+00:00",
      "updated": "2025-08-08T12:26:20+00:00",
      "arxiv_id": "2508.06259v1",
      "url": "http://arxiv.org/pdf/2508.06259v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "I.2.10"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "In-Training Defenses against Emergent Misalignment in Language Models",
      "abstract": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research.",
      "authors": [
        "David KaczÃ©r",
        "Magnus JÃ¸rgenvÃ¥g",
        "Clemens Vetter",
        "Lucie Flek",
        "Florian Mai"
      ],
      "published": "2025-08-08T12:10:28+00:00",
      "updated": "2025-08-08T12:10:28+00:00",
      "arxiv_id": "2508.06249v1",
      "url": "http://arxiv.org/pdf/2508.06249v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines",
      "abstract": "Geometry problem solving (GPS) requires models to master diagram\ncomprehension, logical reasoning, knowledge application, numerical computation,\nand auxiliary line construction. This presents a significant challenge for\nMultimodal Large Language Models (MLLMs). However, existing benchmarks for\nevaluating MLLM geometry skills overlook auxiliary line construction and lack\nfine-grained process evaluation, making them insufficient for assessing MLLMs'\nlong-step reasoning abilities. To bridge these gaps, we present the GeoLaux\nbenchmark, comprising 2,186 geometry problems, incorporating both calculation\nand proving questions. Notably, the problems require an average of 6.51\nreasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary\nline construction. Building on the dataset, we design a novel five-dimensional\nevaluation strategy assessing answer correctness, process correctness, process\nquality, auxiliary line impact, and error causes. Extensive experiments on 13\nleading MLLMs (including thinking models and non-thinking models) yield three\npivotal findings: First, models exhibit substantial performance degradation in\nextended reasoning steps (nine models demonstrate over 50% performance drop).\nSecond, compared to calculation problems, MLLMs tend to take shortcuts when\nsolving proving problems. Third, models lack auxiliary line awareness, and\nenhancing this capability proves particularly beneficial for overall geometry\nreasoning improvement. These findings establish GeoLaux as both a benchmark for\nevaluating MLLMs' long-step geometric reasoning with auxiliary lines and a\nguide for capability advancement. Our dataset and code are included in\nsupplementary materials and will be released.",
      "authors": [
        "Yumeng Fu",
        "Jiayin Zhu",
        "Lingling Zhang",
        "Bo Zhao",
        "Shaoxuan Ma",
        "Yushun Zhang",
        "Yanrui Wu",
        "Wenjun Wu"
      ],
      "published": "2025-08-08T11:11:37+00:00",
      "updated": "2025-08-08T11:11:37+00:00",
      "arxiv_id": "2508.06226v1",
      "url": "http://arxiv.org/pdf/2508.06226v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution",
      "abstract": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the Overconfidence Phenomenon in current\nLLM-as-a-Judges, where predicted confidence significantly overstates actual\ncorrectness, undermining reliability in practical deployment. To quantify this\nphenomenon, we introduce TH-Score, a novel metric measuring confidence-accuracy\nalignment. Furthermore, we propose LLM-as-a-Fuser, an ensemble framework that\ntransforms LLMs into reliable, risk-aware evaluators. Extensive experiments\ndemonstrate that our approach substantially improves calibration and enables\nadaptive, confidence-driven evaluation pipelines, achieving superior\nreliability and accuracy compared to existing baselines.",
      "authors": [
        "Zailong Tian",
        "Zhuoheng Han",
        "Yanzhe Chen",
        "Haozhe Xu",
        "Xi Yang",
        "Richeng Xuan",
        "Houfeng Wang",
        "Lizi Liao"
      ],
      "published": "2025-08-08T11:11:22+00:00",
      "updated": "2025-08-11T11:15:26+00:00",
      "arxiv_id": "2508.06225v2",
      "url": "http://arxiv.org/pdf/2508.06225v2",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials",
      "abstract": "Large language models (LLMs) have reshaped the research landscape by enabling\nnew approaches to knowledge retrieval and creative ideation. Yet their\napplication in discipline-specific experimental science, particularly in highly\nmulti-disciplinary domains like materials science, remains limited. We present\na first-of-its-kind framework that integrates generative AI with literature\nfrom hitherto-unconnected fields such as plant science, biomimetics, and\nmaterials engineering to extract insights and design experiments for materials.\nWe focus on humidity-responsive systems such as pollen-based materials and\nRhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and\nadaptive performance. Using a suite of AI tools, including a fine-tuned model\n(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a\nHierarchical Sampling strategy, we extract structure-property relationships and\ntranslate them into new classes of bioinspired materials. Structured inference\nprotocols generate and evaluate hundreds of hypotheses from a single query,\nsurfacing novel and experimentally tractable ideas. We validate our approach\nthrough real-world implementation: LLM-generated procedures, materials designs,\nand mechanical predictions were tested in the laboratory, culminating in the\nfabrication of a novel pollen-based adhesive with tunable morphology and\nmeasured shear strength, establishing a foundation for future plant-derived\nadhesive design. This work demonstrates how AI-assisted ideation can drive\nreal-world materials design and enable effective human-AI collaboration.",
      "authors": [
        "Rachel K. Luu",
        "Jingyu Deng",
        "Mohammed Shahrudin Ibrahim",
        "Nam-Joon Cho",
        "Ming Dao",
        "Subra Suresh",
        "Markus J. Buehler"
      ],
      "published": "2025-08-08T10:41:03+00:00",
      "updated": "2025-08-08T10:41:03+00:00",
      "arxiv_id": "2508.06591v1",
      "url": "http://arxiv.org/pdf/2508.06591v1",
      "categories": [
        "cs.LG",
        "cond-mat.dis-nn",
        "cond-mat.mtrl-sci",
        "cond-mat.other",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model",
      "abstract": "Affordance grounding focuses on predicting the specific regions of objects\nthat are associated with the actions to be performed by robots. It plays a\nvital role in the fields of human-robot interaction, human-object interaction,\nembodied manipulation, and embodied perception. Existing models often neglect\nthe affordance shared among different objects because they lack the\nChain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)\ngeneralization and explicit reasoning capabilities. To address these\nchallenges, we propose Affordance-R1, the first unified affordance grounding\nframework that integrates cognitive CoT guided Group Relative Policy\nOptimization (GRPO) within a reinforcement learning paradigm. Specifically, we\ndesigned a sophisticated affordance function, which contains format,\nperception, and cognition rewards to effectively guide optimization directions.\nFurthermore, we constructed a high-quality affordance-centric reasoning\ndataset, ReasonAff, to support training. Trained exclusively via reinforcement\nlearning with GRPO and without explicit reasoning data, Affordance-R1 achieves\nrobust zero-shot generalization and exhibits emergent test-time reasoning\ncapabilities. Comprehensive experiments demonstrate that our model outperforms\nwell-established methods and exhibits open-world generalization. To the best of\nour knowledge, Affordance-R1 is the first to integrate GRPO-based RL with\nreasoning into affordance reasoning. The code of our method and our dataset is\nreleased on https://github.com/hq-King/Affordance-R1.",
      "authors": [
        "Hanqing Wang",
        "Shaoyang Wang",
        "Yiming Zhong",
        "Zemin Yang",
        "Jiamin Wang",
        "Zhiqing Cui",
        "Jiahao Yuan",
        "Yifan Han",
        "Mingyu Liu",
        "Yuexin Ma"
      ],
      "published": "2025-08-08T10:39:04+00:00",
      "updated": "2025-08-11T06:30:16+00:00",
      "arxiv_id": "2508.06206v2",
      "url": "http://arxiv.org/pdf/2508.06206v2",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning",
      "abstract": "Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language\nModels (MLLMs) to incrementally learn new tasks over time. However, this\nprocess is challenged by catastrophic forgetting, where performance on\npreviously learned tasks deteriorates as the model adapts to new ones. A common\napproach to mitigate forgetting is architecture expansion, which introduces\ntask-specific modules to prevent interference. Yet, existing methods often\nexpand entire layers for each task, leading to significant parameter overhead\nand poor scalability. To overcome these issues, we introduce LoRA in LoRA\n(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in\nMLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,\napplies an additional low-rank decomposition to matrix B to minimize\ntask-specific parameters, and incorporates a cosine-regularized stability loss\nto preserve consistency in shared representations over time. Extensive\nexperiments on a diverse CVIT benchmark show that LiLoRA consistently achieves\nsuperior performance in sequential task learning while significantly improving\nparameter efficiency compared to existing approaches.",
      "authors": [
        "Chang Che",
        "Ziqi Wang",
        "Pengwan Yang",
        "Qi Wang",
        "Hui Ma",
        "Zenglin Shi"
      ],
      "published": "2025-08-08T10:32:38+00:00",
      "updated": "2025-08-08T10:32:38+00:00",
      "arxiv_id": "2508.06202v1",
      "url": "http://arxiv.org/pdf/2508.06202v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations",
      "abstract": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment.",
      "authors": [
        "Nizi Nazar",
        "Ehsaneddin Asgari"
      ],
      "published": "2025-08-08T10:22:19+00:00",
      "updated": "2025-08-08T10:22:19+00:00",
      "arxiv_id": "2508.06196v1",
      "url": "http://arxiv.org/pdf/2508.06196v1",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation",
      "abstract": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage.",
      "authors": [
        "Lai Jiang",
        "Yuekang Li",
        "Xiaohan Zhang",
        "Youtao Ding",
        "Li Pan"
      ],
      "published": "2025-08-08T10:19:21+00:00",
      "updated": "2025-08-08T10:19:21+00:00",
      "arxiv_id": "2508.06194v1",
      "url": "http://arxiv.org/pdf/2508.06194v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration",
      "abstract": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios.",
      "authors": [
        "Cheng Liu",
        "Daou Zhang",
        "Tingxu Liu",
        "Yuhan Wang",
        "Jinyang Chen",
        "Yuexuan Li",
        "Xinying Xiao",
        "Chenbo Xin",
        "Ziru Wang",
        "Weichao Wu"
      ],
      "published": "2025-08-08T10:12:00+00:00",
      "updated": "2025-08-08T10:12:00+00:00",
      "arxiv_id": "2508.06189v1",
      "url": "http://arxiv.org/pdf/2508.06189v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration",
      "abstract": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input.",
      "authors": [
        "Ali Sarabadani",
        "Maryam Abdollahi Shamami",
        "Hamidreza Sadeghsalehi",
        "Borhan Asadi",
        "Saba Hesaraki"
      ],
      "published": "2025-08-08T10:04:40+00:00",
      "updated": "2025-08-08T10:04:40+00:00",
      "arxiv_id": "2508.06186v1",
      "url": "http://arxiv.org/pdf/2508.06186v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime",
      "abstract": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods.",
      "authors": [
        "Hugo Abonizio",
        "Thales Almeida",
        "Roberto Lotufo",
        "Rodrigo Nogueira"
      ],
      "published": "2025-08-08T09:48:32+00:00",
      "updated": "2025-08-08T09:48:32+00:00",
      "arxiv_id": "2508.06178v1",
      "url": "http://arxiv.org/pdf/2508.06178v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Improving Table Retrieval with Question Generation from Partial Tables",
      "abstract": "Recent advances in open-domain question answering over tables have widely\nadopted large language models (LLMs) under the Retriever-Reader architecture.\nPrior works have effectively leveraged LLMs to tackle the complex reasoning\ndemands of the Reader component, such as text-to-text, text-to-SQL, and multi\nhop reasoning. In contrast, the Retriever component has primarily focused on\noptimizing the query representation-training retrievers to retrieve relevant\ntables based on questions, or to select keywords from questions for matching\ntable segments. However, little attention has been given to enhancing how\ntables themselves are represented in embedding space to better align with\nquestions. To address this, we propose QGpT (Question Generation from Partial\nTables), a simple yet effective method that uses an LLM to generate synthetic\nquestions based on small portions of a table. These questions are generated to\nsimulate how a user might query the content of the table currently under\nconsideration. The generated questions are then jointly embedded with the\npartial table segments used for generation, enhancing semantic alignment with\nuser queries. Without the need to embed entire tables, our method significantly\nimproves retrieval performance across multiple benchmarks for both dense and\nlate-interaction retrievers.",
      "authors": [
        "Hsing-Ping Liang",
        "Che-Wei Chang",
        "Yao-Chung Fan"
      ],
      "published": "2025-08-08T09:35:56+00:00",
      "updated": "2025-08-08T09:35:56+00:00",
      "arxiv_id": "2508.06168v1",
      "url": "http://arxiv.org/pdf/2508.06168v1",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Pragmatics beyond humans: meaning, communication, and LLMs",
      "abstract": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI.",
      "authors": [
        "VÃ­t GvoÅ¾diak"
      ],
      "published": "2025-08-08T09:34:41+00:00",
      "updated": "2025-08-08T09:34:41+00:00",
      "arxiv_id": "2508.06167v1",
      "url": "http://arxiv.org/pdf/2508.06167v1",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning",
      "abstract": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2.",
      "authors": [
        "Weitao Li",
        "Boran Xiang",
        "Xiaolong Wang",
        "Zhinan Gou",
        "Weizhi Ma",
        "Yang Liu"
      ],
      "published": "2025-08-08T09:33:20+00:00",
      "updated": "2025-08-08T09:33:20+00:00",
      "arxiv_id": "2508.06165v1",
      "url": "http://arxiv.org/pdf/2508.06165v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach",
      "abstract": "This paper addresses the issue of implicit stereotypes that may arise during\nthe generation process of large language models. It proposes an interpretable\nbias detection method aimed at identifying hidden social biases in model\noutputs, especially those semantic tendencies that are not easily captured\nthrough explicit linguistic features. The method combines nested semantic\nrepresentation with a contextual contrast mechanism. It extracts latent bias\nfeatures from the vector space structure of model outputs. Using attention\nweight perturbation, it analyzes the model's sensitivity to specific social\nattribute terms, thereby revealing the semantic pathways through which bias is\nformed. To validate the effectiveness of the method, this study uses the\nStereoSet dataset, which covers multiple stereotype dimensions including\ngender, profession, religion, and race. The evaluation focuses on several key\nmetrics, such as bias detection accuracy, semantic consistency, and contextual\nsensitivity. Experimental results show that the proposed method achieves strong\ndetection performance across various dimensions. It can accurately identify\nbias differences between semantically similar texts while maintaining high\nsemantic alignment and output stability. The method also demonstrates high\ninterpretability in its structural design. It helps uncover the internal bias\nassociation mechanisms within language models. This provides a more transparent\nand reliable technical foundation for bias detection. The approach is suitable\nfor real-world applications where high trustworthiness of generated content is\nrequired.",
      "authors": [
        "Renhan Zhang",
        "Lian Lian",
        "Zhen Qi",
        "Guiran Liu"
      ],
      "published": "2025-08-08T09:21:10+00:00",
      "updated": "2025-08-08T09:21:10+00:00",
      "arxiv_id": "2508.06155v1",
      "url": "http://arxiv.org/pdf/2508.06155v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense Against Instruction Backdoor in APIs",
      "abstract": "With the development of customized large language model (LLM) agents, a new\nthreat of black-box backdoor attacks has emerged, where malicious instructions\nare injected into hidden system prompts. These attacks easily bypass existing\ndefenses that rely on white-box access, posing a serious security challenge. To\naddress this, we propose SLIP, a Soft Label mechanism and key-extraction-guided\nCoT-based defense against Instruction backdoors in APIs. SLIP is designed based\non two key insights. First, to counteract the model's oversensitivity to\ntriggers, we propose a Key-extraction-guided Chain-of-Thought (KCoT). Instead\nof only considering the single trigger or the input sentence, KCoT prompts the\nagent to extract task-relevant key phrases. Second, to guide the LLM toward\ncorrect answers, our proposed Soft Label Mechanism (SLM) prompts the agent to\nquantify the semantic correlation between key phrases and candidate answers.\nCrucially, to mitigate the influence of residual triggers or misleading content\nin phrases extracted by KCoT, which typically causes anomalous scores, SLM\nexcludes anomalous scores deviating significantly from the mean and\nsubsequently averages the remaining scores to derive a more reliable semantic\nrepresentation. Extensive experiments on classification and question-answer\n(QA) tasks demonstrate that SLIP is highly effective, reducing the average\nattack success rate (ASR) from 90.2% to 25.13% while maintaining high accuracy\non clean data and outperforming state-of-the-art defenses. Our code are\navailable in\nhttps://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/SLIP.",
      "authors": [
        "Zhengxian Wu",
        "Juan Wen",
        "Wanli Peng",
        "Haowei Chang",
        "Yinghan Zhou",
        "Yiming Xue"
      ],
      "published": "2025-08-08T09:17:33+00:00",
      "updated": "2025-08-08T09:17:33+00:00",
      "arxiv_id": "2508.06153v1",
      "url": "http://arxiv.org/pdf/2508.06153v1",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts",
      "abstract": "We present Big5-Scaler, a prompt-based framework for conditioning large\nlanguage models (LLMs) with controllable Big Five personality traits. By\nembedding numeric trait values into natural language prompts, our method\nenables fine-grained personality control without additional training. We\nevaluate Big5-Scaler across trait expression, dialogue generation, and human\ntrait imitation tasks. Results show that it induces consistent and\ndistinguishable personality traits across models, with performance varying by\nprompt type and scale. Our analysis highlights the effectiveness of concise\nprompts and lower trait intensities, providing a efficient approach for\nbuilding personality-aware dialogue agents.",
      "authors": [
        "Gunhee Cho",
        "Yun-Gyung Cheong"
      ],
      "published": "2025-08-08T09:11:05+00:00",
      "updated": "2025-08-08T09:11:05+00:00",
      "arxiv_id": "2508.06149v1",
      "url": "http://arxiv.org/pdf/2508.06149v1",
      "categories": [
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications",
      "abstract": "The versatility of large language models (LLMs) has been explored across\nvarious sectors, but their application in healthcare poses challenges,\nparticularly in the domain of pharmaceutical contraindications where accurate\nand reliable information is required. This study enhances the capability of\nLLMs to address contraindications effectively by implementing a Retrieval\nAugmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base\nmodel, and the text-embedding-3-small model for embeddings, our approach\nintegrates Langchain to orchestrate a hybrid retrieval system with re-ranking.\nThis system leverages Drug Utilization Review (DUR) data from public databases,\nfocusing on contraindications for specific age groups, pregnancy, and\nconcomitant drug use. The dataset includes 300 question-answer pairs across\nthree categories, with baseline model accuracy ranging from 0.49 to 0.57.\nPost-integration of the RAG pipeline, we observed a significant improvement in\nmodel accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications\nrelated to age groups, pregnancy, and concomitant drug use, respectively. The\nresults indicate that augmenting LLMs with a RAG framework can substantially\nreduce uncertainty in prescription and drug intake decisions by providing more\nprecise and reliable drug contraindication information.",
      "authors": [
        "Byeonghun Bang",
        "Jongsuk Yoon",
        "Dong-Jin Chang",
        "Seho Park",
        "Yong Oh Lee"
      ],
      "published": "2025-08-08T09:09:03+00:00",
      "updated": "2025-08-08T09:09:03+00:00",
      "arxiv_id": "2508.06145v1",
      "url": "http://arxiv.org/pdf/2508.06145v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models",
      "abstract": "In the rapidly evolving landscape of Multimodal Large Language Models\n(MLLMs), the safety concerns of their outputs have earned significant\nattention. Although numerous datasets have been proposed, they may become\noutdated with MLLM advancements and are susceptible to data contamination\nissues. To address these problems, we propose \\textbf{SDEval}, the\n\\textit{first} safety dynamic evaluation framework to controllably adjust the\ndistribution and complexity of safety benchmarks. Specifically, SDEval mainly\nadopts three dynamic strategies: text, image, and text-image dynamics to\ngenerate new samples from original benchmarks. We first explore the individual\neffects of text and image dynamics on model safety. Then, we find that\ninjecting text dynamics into images can further impact safety, and conversely,\ninjecting image dynamics into text also leads to safety risks. SDEval is\ngeneral enough to be applied to various existing safety and even capability\nbenchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and\ncapability benchmarks, MMBench and MMVet, show that SDEval significantly\ninfluences safety evaluation, mitigates data contamination, and exposes safety\nlimitations of MLLMs. Code is available at https://github.com/hq-King/SDEval",
      "authors": [
        "Hanqing Wang",
        "Yuan Tian",
        "Mingyu Liu",
        "Zhenhao Zhang",
        "Xiangyang Zhu"
      ],
      "published": "2025-08-08T09:01:56+00:00",
      "updated": "2025-08-08T09:01:56+00:00",
      "arxiv_id": "2508.06142v1",
      "url": "http://arxiv.org/pdf/2508.06142v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models",
      "abstract": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs.",
      "authors": [
        "Lingyuan Liu",
        "Mengxiang Zhang"
      ],
      "published": "2025-08-08T08:55:53+00:00",
      "updated": "2025-08-08T08:55:53+00:00",
      "arxiv_id": "2508.06135v1",
      "url": "http://arxiv.org/pdf/2508.06135v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
      "abstract": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
      "authors": [
        "Meixuan Wang",
        "Yinyu Ye",
        "Zijie Zhou"
      ],
      "published": "2025-08-08T08:54:21+00:00",
      "updated": "2025-08-08T08:54:21+00:00",
      "arxiv_id": "2508.06133v1",
      "url": "http://arxiv.org/pdf/2508.06133v1",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.OC"
    },
    {
      "title": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models",
      "abstract": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications.",
      "authors": [
        "Sayantan Adak",
        "Pratyush Chatterjee",
        "Somnath Banerjee",
        "Rima Hazra",
        "Somak Aditya",
        "Animesh Mukherjee"
      ],
      "published": "2025-08-08T08:43:24+00:00",
      "updated": "2025-08-08T08:43:24+00:00",
      "arxiv_id": "2508.06124v1",
      "url": "http://arxiv.org/pdf/2508.06124v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges",
      "abstract": "Evaluating the capabilities and risks of foundation models is paramount, yet\ncurrent methods demand extensive domain expertise, hindering their scalability\nas these models rapidly evolve. We introduce SKATE: a novel evaluation\nframework in which large language models (LLMs) compete by generating and\nsolving verifiable tasks for one another. Our core insight is to treat\nevaluation as a game: models act as both task-setters and solvers, incentivized\nto create questions which highlight their own strengths while exposing others'\nweaknesses. SKATE offers several key advantages, balancing scalability,\nopen-endedness, and objectivity. It is fully automated, data-free, and\nscalable, requiring no human input or domain expertise. By using verifiable\ntasks rather than LLM judges, scoring is objective. Unlike domain-limited\nprogrammatically-generated benchmarks (e.g. chess-playing or spatial\nreasoning), having LLMs creatively pose challenges enables open-ended and\nscalable evaluation. As a proof of concept, we introduce LLM-set\ncode-output-prediction (COP) challenges as a verifiable and extensible\nframework in which to test our approach. Using a TrueSkill-based ranking\nsystem, we evaluate six frontier LLMs and find that: (1) weaker models can\nreliably differentiate and score stronger ones, (2) LLM-based systems are\ncapable of self-preferencing behavior, generating questions that align with\ntheir own capabilities, and (3) SKATE automatically surfaces fine-grained\ncapability differences between models. Our findings are an important step\ntowards general, scalable evaluation frameworks which can keep pace with LLM\nprogress.",
      "authors": [
        "Dewi S. W. Gould",
        "Bruno Mlodozeniec",
        "Samuel F. Brown"
      ],
      "published": "2025-08-08T08:16:40+00:00",
      "updated": "2025-08-08T08:16:40+00:00",
      "arxiv_id": "2508.06111v1",
      "url": "http://arxiv.org/pdf/2508.06111v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion",
      "abstract": "Table reasoning, including tabular QA and fact verification, often depends on\nannotated data or complex data augmentation, limiting flexibility and\ngeneralization. LLMs, despite their versatility, often underperform compared to\nsimple supervised models. To approach these issues, we introduce PanelTR, a\nframework utilizing LLM agent scientists for robust table reasoning through a\nstructured scientific approach. PanelTR's workflow involves agent scientists\nconducting individual investigations, engaging in self-review, and\nparticipating in collaborative peer-review discussions. This process, driven by\nfive scientist personas, enables semantic-level transfer without relying on\ndata augmentation or parametric optimization. Experiments across four\nbenchmarks show that PanelTR outperforms vanilla LLMs and rivals fully\nsupervised models, all while remaining independent of training data. Our\nfindings indicate that structured scientific methodology can effectively handle\ncomplex tasks beyond table reasoning with flexible semantic understanding in a\nzero-shot context.",
      "authors": [
        "Yiran Rex Ma"
      ],
      "published": "2025-08-08T08:15:52+00:00",
      "updated": "2025-08-08T08:15:52+00:00",
      "arxiv_id": "2508.06110v1",
      "url": "http://arxiv.org/pdf/2508.06110v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures",
      "abstract": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect statements when handling questions beyond their knowledge\nand perception. Retrieval-augmented generation (RAG) addresses this by\nretrieving query-relevant contexts from knowledge bases to support LLM\nreasoning. Recent advances leverage pre-constructed graphs to capture the\nrelational connections among distributed documents, showing remarkable\nperformance in complex tasks. However, existing Graph-based RAG (GraphRAG)\nmethods rely on a costly process to transform the corpus into a graph,\nintroducing overwhelming token cost and update latency. Moreover, real-world\nqueries vary in type and complexity, requiring different logic structures for\naccurate reasoning. The pre-built graph may not align with these required\nstructures, resulting in ineffective knowledge retrieval. To this end, we\npropose a \\textbf{\\underline{Logic}}-aware\n\\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented\n\\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically\nextracts reasoning structures at inference time to guide adaptive retrieval\nwithout any pre-built graph. LogicRAG begins by decomposing the input query\ninto a set of subproblems and constructing a directed acyclic graph (DAG) to\nmodel the logical dependencies among them. To support coherent multi-step\nreasoning, LogicRAG then linearizes the graph using topological sort, so that\nsubproblems can be addressed in a logically consistent order. Besides, LogicRAG\napplies graph pruning to reduce redundant retrieval and uses context pruning to\nfilter irrelevant context, significantly reducing the overall token cost.\nExtensive experiments demonstrate that LogicRAG achieves both superior\nperformance and efficiency compared to state-of-the-art baselines.",
      "authors": [
        "Shengyuan Chen",
        "Chuang Zhou",
        "Zheng Yuan",
        "Qinggang Zhang",
        "Zeyang Cui",
        "Hao Chen",
        "Yilin Xiao",
        "Jiannong Cao",
        "Xiao Huang"
      ],
      "published": "2025-08-08T08:07:40+00:00",
      "updated": "2025-08-08T08:07:40+00:00",
      "arxiv_id": "2508.06105v1",
      "url": "http://arxiv.org/pdf/2508.06105v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs",
      "abstract": "This paper presents two effective approaches for Extractive Question\nAnswering (QA) on the Quran. It addresses challenges related to complex\nlanguage, unique terminology, and deep meaning in the text. The second uses\nfew-shot prompting with instruction-tuned large language models such as Gemini\nand DeepSeek. A specialized Arabic prompt framework is developed for span\nextraction. A strong post-processing system integrates subword alignment,\noverlap suppression, and semantic filtering. This improves precision and\nreduces hallucinations. Evaluations show that large language models with Arabic\ninstructions outperform traditional fine-tuned models. The best configuration\nachieves a pAP10 score of 0.637. The results confirm that prompt-based\ninstruction tuning is effective for low-resource, semantically rich QA tasks.",
      "authors": [
        "Mohamed Basem",
        "Islam Oshallah",
        "Ali Hamdi",
        "Ammar Mohammed"
      ],
      "published": "2025-08-08T08:02:59+00:00",
      "updated": "2025-08-08T08:02:59+00:00",
      "arxiv_id": "2508.06103v1",
      "url": "http://arxiv.org/pdf/2508.06103v1",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline",
      "abstract": "Constructed languages (conlangs) such as Esperanto and Quenya have played\ndiverse roles in art, philosophy, and international communication. Meanwhile,\nlarge-scale foundation models have revolutionized creative generation in text,\nimages, and beyond. In this work, we leverage modern LLMs as computational\ncreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a\nmulti-hop pipeline that decomposes language design into modular stages --\nphonology, morphology, syntax, lexicon generation, and translation. At each\nstage, our method leverages LLMs' meta-linguistic reasoning capabilities,\ninjecting randomness to encourage diversity and leveraging self-refinement\nfeedback to encourage consistency in the emerging language description. We\nevaluate ConlangCrafter on metrics measuring coherence and typological\ndiversity, demonstrating its ability to produce coherent and varied conlangs\nwithout human linguistic expertise.",
      "authors": [
        "Morris Alper",
        "Moran Yanuka",
        "Raja Giryes",
        "GaÅ¡per BeguÅ¡"
      ],
      "published": "2025-08-08T07:36:48+00:00",
      "updated": "2025-08-08T07:36:48+00:00",
      "arxiv_id": "2508.06094v1",
      "url": "http://arxiv.org/pdf/2508.06094v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Adaptive Backtracking for Privacy Protection in Large Language Models",
      "abstract": "The preservation of privacy has emerged as a critical topic in the era of\nartificial intelligence. However, current work focuses on user-oriented\nprivacy, overlooking severe enterprise data leakage risks exacerbated by the\nRetrieval-Augmented Generation paradigm. To address this gap, our paper\nintroduces a novel objective: enterprise-oriented privacy concerns. Achieving\nthis objective requires overcoming two fundamental challenges: existing methods\nsuch as data sanitization severely degrade model performance, and the field\nlacks public datasets for evaluation. We address these challenges with several\nsolutions. (1) To prevent performance degradation, we propose ABack, a\ntraining-free mechanism that leverages a Hidden State Model to pinpoint the\norigin of a leakage intention and rewrite the output safely. (2) To solve the\nlack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy\nscenarios in healthcare and finance. To ensure a rigorous evaluation, we move\nbeyond simple static attacks by developing a powerful adaptive attacker with\nGroup Relative Policy Optimization. Experiments show that against this superior\nadversary, ABack improves the overall privacy utility score by up to 15\\% over\nstrong baselines, avoiding the performance trade-offs of prior methods.",
      "authors": [
        "Zhihao Yao",
        "Yuxuan Gu",
        "Xiachong Feng",
        "Weitao Ma",
        "Bo Li",
        "Xiaocheng Feng"
      ],
      "published": "2025-08-08T07:29:33+00:00",
      "updated": "2025-08-08T07:29:33+00:00",
      "arxiv_id": "2508.06087v1",
      "url": "http://arxiv.org/pdf/2508.06087v1",
      "categories": [
        "cs.CR",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Can Large Models Fool the Eye? A New Turing Test for Biological Animation",
      "abstract": "Evaluating the abilities of large models and manifesting their gaps are\nchallenging. Current benchmarks adopt either ground-truth-based score-form\nevaluation on static datasets or indistinct textual chatbot-style human\npreferences collection, which may not provide users with immediate, intuitive,\nand perceptible feedback on performance differences. In this paper, we\nintroduce BioMotion Arena, a novel framework for evaluating large language\nmodels (LLMs) and multimodal large language models (MLLMs) via visual\nanimation. Our methodology draws inspiration from the inherent visual\nperception of motion patterns characteristic of living organisms that utilizes\npoint-light source imaging to amplify the performance discrepancies between\nmodels. Specifically, we employ a pairwise comparison evaluation and collect\nmore than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion\nvariants. Data analyses show that the crowd-sourced human votes are in good\nagreement with those of expert raters, demonstrating the superiority of our\nBioMotion Arena in offering discriminative feedback. We also find that over\n90\\% of evaluated models, including the cutting-edge open-source InternVL3 and\nproprietary Claude-4 series, fail to produce fundamental humanoid point-light\ngroups, much less smooth and biologically plausible motions. This enables\nBioMotion Arena to serve as a challenging benchmark for performance\nvisualization and a flexible evaluation framework without restrictions on\nground-truth.",
      "authors": [
        "Zijian Chen",
        "Lirong Deng",
        "Zhengyu Chen",
        "Kaiwei Zhang",
        "Qi Jia",
        "Yuan Tian",
        "Yucheng Zhu",
        "Guangtao Zhai"
      ],
      "published": "2025-08-08T07:10:17+00:00",
      "updated": "2025-08-08T07:10:17+00:00",
      "arxiv_id": "2508.06072v1",
      "url": "http://arxiv.org/pdf/2508.06072v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences",
      "abstract": "Large Language Models (LLMs) are increasingly expected to handle complex\ndecision-making tasks, yet their ability to perform structured resource\nallocation remains underexplored. Evaluating their reasoning is also difficult\ndue to data contamination and the static nature of existing benchmarks. We\npresent a dual-purpose framework leveraging Participatory Budgeting (PB) both\nas (i) a practical setting for LLM-based resource allocation and (ii) an\nadaptive benchmark for evaluating their reasoning capabilities. We task LLMs\nwith selecting project subsets under feasibility (e.g., budget) constraints via\nthree prompting strategies: greedy selection, direct optimization, and a\nhill-climbing-inspired refinement. We benchmark LLMs' allocations against a\nutility-maximizing oracle. Interestingly, we also test whether LLMs can infer\nstructured preferences from natural-language voter input or metadata, without\nexplicit votes. By comparing allocations based on inferred preferences to those\nfrom ground-truth votes, we evaluate LLMs' ability to extract preferences from\nopen-ended input. Our results underscore the role of prompt design and show\nthat LLMs hold promise for mechanism design with unstructured inputs.",
      "authors": [
        "Sankarshan Damle",
        "Boi Faltings"
      ],
      "published": "2025-08-08T06:45:07+00:00",
      "updated": "2025-08-08T06:45:07+00:00",
      "arxiv_id": "2508.06060v1",
      "url": "http://arxiv.org/pdf/2508.06060v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System",
      "abstract": "State-of-the-art fact-checking systems combat misinformation at scale by\nemploying autonomous LLM-based agents to decompose complex claims into smaller\nsub-claims, verify each sub-claim individually, and aggregate the partial\nresults to produce verdicts with justifications (explanatory rationales for the\nverdicts). The security of these systems is crucial, as compromised\nfact-checkers, which tend to be easily underexplored, can amplify\nmisinformation. This work introduces Fact2Fiction, the first poisoning attack\nframework targeting such agentic fact-checking systems. Fact2Fiction mirrors\nthe decomposition strategy and exploits system-generated justifications to\ncraft tailored malicious evidences that compromise sub-claim verification.\nExtensive experiments demonstrate that Fact2Fiction achieves 8.9\\%--21.2\\%\nhigher attack success rates than state-of-the-art attacks across various\npoisoning budgets. Fact2Fiction exposes security weaknesses in current\nfact-checking systems and highlights the need for defensive countermeasures.",
      "authors": [
        "Haorui He",
        "Yupeng Li",
        "Bin Benjamin Zhu",
        "Dacheng Wen",
        "Reynold Cheng",
        "Francis C. M. Lau"
      ],
      "published": "2025-08-08T06:44:57+00:00",
      "updated": "2025-08-08T06:44:57+00:00",
      "arxiv_id": "2508.06059v1",
      "url": "http://arxiv.org/pdf/2508.06059v1",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "RAGTrace: Understanding and Refining Retrieval-Generation Dynamics in Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) systems have emerged as a promising\nsolution to enhance large language models (LLMs) by integrating external\nknowledge retrieval with generative capabilities. While significant\nadvancements have been made in improving retrieval accuracy and response\nquality, a critical challenge remains that the internal knowledge integration\nand retrieval-generation interactions in RAG workflows are largely opaque. This\npaper introduces RAGTrace, an interactive evaluation system designed to analyze\nretrieval and generation dynamics in RAG-based workflows. Informed by a\ncomprehensive literature review and expert interviews, the system supports a\nmulti-level analysis approach, ranging from high-level performance evaluation\nto fine-grained examination of retrieval relevance, generation fidelity, and\ncross-component interactions. Unlike conventional evaluation practices that\nfocus on isolated retrieval or generation quality assessments, RAGTrace enables\nan integrated exploration of retrieval-generation relationships, allowing users\nto trace knowledge sources and identify potential failure cases. The system's\nworkflow allows users to build, evaluate, and iterate on retrieval processes\ntailored to their specific domains of interest. The effectiveness of the system\nis demonstrated through case studies and expert evaluations on real-world RAG\napplications.",
      "authors": [
        "Sizhe Cheng",
        "Jiaping Li",
        "Huanchen Wang",
        "Yuxin Ma"
      ],
      "published": "2025-08-08T06:26:19+00:00",
      "updated": "2025-08-08T06:26:19+00:00",
      "arxiv_id": "2508.06056v1",
      "url": "http://arxiv.org/pdf/2508.06056v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "ArchXBench: A Complex Digital Systems Benchmark Suite for LLM Driven RTL Synthesis",
      "abstract": "Modern SoC datapaths include deeply pipelined, domain-specific accelerators,\nbut their RTL implementation and verification are still mostly done by hand.\nWhile large language models (LLMs) exhibit advanced code-generation abilities\nfor programming languages like Python, their application to Verilog-like RTL\nremains in its nascent stage. This is reflected in the simple arithmetic and\ncontrol circuits currently used to evaluate generative capabilities in existing\nbenchmarks. In this paper, we introduce ArchXBench, a six-level benchmark suite\nthat encompasses complex arithmetic circuits and other advanced digital\nsubsystems drawn from domains such as cryptography, image processing, machine\nlearning, and signal processing. Architecturally, some of these designs are\npurely combinational, others are multi-cycle or pipelined, and many require\nhierarchical composition of modules. For each benchmark, we provide a problem\ndescription, design specification, and testbench, enabling rapid research in\nthe area of LLM-driven agentic approaches for complex digital systems design.\n  Using zero-shot prompting with Claude Sonnet 4, GPT 4.1, o4-mini-high, and\nDeepSeek R1 under a pass@5 criterion, we observed that o4-mini-high\nsuccessfully solves the largest number of benchmarks, 16 out of 30, spanning\nLevels 1, 2, and 3. From Level 4 onward, however, all models consistently fail,\nhighlighting a clear gap in the capabilities of current state-of-the-art LLMs\nand prompting/agentic approaches.",
      "authors": [
        "Suresh Purini",
        "Siddhant Garg",
        "Mudit Gaur",
        "Sankalp Bhat",
        "Sohan Mupparapu",
        "Arun Ravindran"
      ],
      "published": "2025-08-08T06:12:00+00:00",
      "updated": "2025-08-08T06:12:00+00:00",
      "arxiv_id": "2508.06047v1",
      "url": "http://arxiv.org/pdf/2508.06047v1",
      "categories": [
        "cs.AR"
      ],
      "primary_category": "cs.AR"
    },
    {
      "title": "EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation",
      "abstract": "Although the effectiveness of Large Language Models (LLMs) as judges\n(LLM-as-a-judge) has been validated, their performance remains limited in\nopen-ended tasks, particularly in story evaluation. Accurate story evaluation\nis crucial not only for assisting human quality judgment but also for providing\nkey signals to guide story generation. However, existing methods face a\ndilemma: prompt engineering for closed-source models suffers from poor\nadaptability, while fine-tuning approaches for open-source models lack the\nrigorous reasoning capabilities essential for story evaluation. To address\nthis, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.\nGrounded in pairwise comparison, the framework first self-synthesizes\nscore-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To\nensure data quality, these raw CoTs undergo a self-filtering process, utilizing\nmulti-agents to guarantee their logical rigor and robustness. Finally, the\nevaluator trained on the refined data is deployed as a reward model to guide\nthe story generation task. Experimental results demonstrate that our framework\nachieves state-of-the-art (SOTA) performance on three evaluation benchmarks\nincluding StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward\nmodel, it significantly enhances the quality of generated stories, thereby\nfully validating the superiority of our self-evolving approach.",
      "authors": [
        "Xinda Wang",
        "Zhengxu Hou",
        "Yangshijie Zhang",
        "Bingren Yan",
        "Zhibo Yang",
        "Xingsheng Zhang",
        "Luxi Xing",
        "Qiang Zhou",
        "Chen Zhang"
      ],
      "published": "2025-08-08T06:10:47+00:00",
      "updated": "2025-08-08T06:10:47+00:00",
      "arxiv_id": "2508.06046v1",
      "url": "http://arxiv.org/pdf/2508.06046v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning",
      "abstract": "Large Language Models (LLMs) have recently demonstrated impressive action\nsequence prediction capabilities but often struggle with dynamic, long-horizon\ntasks such as real-time strategic games. In a game such as StarCraftII (SC2),\nagents need to manage resource constraints and adapt to evolving battlefield\nsituations in a partially observable environment. This often overwhelms\nexisiting LLM-based approaches. To address these challenges, we propose a\nhierarchical multi-agent framework that employs specialized imitation learning\nagents under a meta-controller called Strategic Planner (SP). By expert\ndemonstrations, each specialized agent learns a distinctive strategy, such as\naerial support or defensive maneuvers, and produces coherent, structured\nmultistep action sequences. The SP then orchestrates these proposals into a\nsingle, environmentally adaptive plan that ensures local decisions aligning\nwith long-term strategies. We call this HIMA (Hierarchical Imitation\nMulti-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that\nencompasses all race match combinations in SC2. Our empirical results show that\nHIMA outperforms state of the arts in strategic clarity, adaptability, and\ncomputational efficiency, underscoring the potential of combining specialized\nimitation modules with meta-level orchestration to develop more robust,\ngeneral-purpose AI agents.",
      "authors": [
        "Daechul Ahn",
        "San Kim",
        "Jonghyun Choi"
      ],
      "published": "2025-08-08T05:57:12+00:00",
      "updated": "2025-08-08T05:57:12+00:00",
      "arxiv_id": "2508.06042v1",
      "url": "http://arxiv.org/pdf/2508.06042v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment",
      "abstract": "How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding iterations. Building on this\ninsight, we introduce DP-LLM, a novel mechanism that dynamically assigns\nprecision to each layer based on input values. DP-LLM augments each linear\nlayer in an LLM with a precision selector that determines the bitwidth at\nruntime using a lightweight error estimator and threshold values learned\nthrough fine-tuning. Experimental results across multiple models and benchmarks\ndemonstrate that DP-LLM achieves a superior performance-latency trade-off,\noutperforming prior approaches.",
      "authors": [
        "Sangwoo Kwon",
        "Seong Hoon Seo",
        "Jae W. Lee",
        "Yeonhong Park"
      ],
      "published": "2025-08-08T05:57:04+00:00",
      "updated": "2025-08-08T05:57:04+00:00",
      "arxiv_id": "2508.06041v1",
      "url": "http://arxiv.org/pdf/2508.06041v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models",
      "abstract": "Vision-Language Models (VLMs) typically replace the predefined image\nplaceholder token (<image>) in textual instructions with visual features from\nan image encoder, forming the input to a backbone Large Language Model (LLM).\nHowever, the large number of vision tokens significantly increases the context\nlength, leading to high computational overhead and inference latency. While\nprevious efforts mitigate this by selecting only important visual features or\nleveraging learnable queries to reduce token count, they often compromise\nperformance or introduce substantial extra costs. In response, we propose\nFourier-VLM, a simple yet efficient method that compresses visual\nrepresentations in the frequency domain. Our approach is motivated by the\nobservation that vision features output from the vision encoder exhibit\nconcentrated energy in low-frequency components. Leveraging this, we apply a\nlow-pass filter to the vision features using a two-dimensional Discrete Cosine\nTransform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier\nTransform (FFT) operator with a time complexity of $\\mathcal{O}(n\\log n)$,\nminimizing the extra computational cost while introducing no additional\nparameters. Extensive experiments across various image-based benchmarks\ndemonstrate that Fourier-VLM achieves competitive performance with strong\ngeneralizability across both LLaVA and Qwen-VL architectures. Crucially, it\nreduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%\ncompared to LLaVA-v1.5, highlighting the superior efficiency and practicality.",
      "authors": [
        "Huanyu Wang",
        "Jushi Kai",
        "Haoli Bai",
        "Lu Hou",
        "Bo Jiang",
        "Ziwei He",
        "Zhouhan Lin"
      ],
      "published": "2025-08-08T05:49:42+00:00",
      "updated": "2025-08-11T03:31:12+00:00",
      "arxiv_id": "2508.06038v2",
      "url": "http://arxiv.org/pdf/2508.06038v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings",
      "abstract": "Large language models (LLMs) acquire knowledge across diverse domains such as\nscience, history, and geography encountered during generative pre-training.\nHowever, due to their stochasticity, it is difficult to predict what LLMs have\nacquired. Prior work has developed different ways to probe this knowledge by\ninvestigating the hidden representations, crafting specific task prompts,\ncurating representative samples, and estimating their uncertainty. However,\nthese methods require making forward passes through the underlying model to\nprobe the LLM's knowledge about a specific fact, making them computationally\nexpensive and time-consuming. To bridge this gap, we propose $\\textbf{PEEK}$ or\n$\\textbf{P}$roxy $\\textbf{E}$mbeddings to $\\textbf{E}$stimate\n$\\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models\nthat effectively encode factual knowledge as text or graphs as proxies for\nLLMs. First, we identify a training set of facts known by LLMs through various\nprobing strategies and then adapt embedding models to predict the LLM outputs\nwith a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived\ndatasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict\nLLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find\nthat sentence embedding models are more suitable than graph embeddings to\npredict LLM knowledge, shedding light on the underlying representation of the\nfactual landscape. Thus, we believe that knowledge-adapted embeddings can be\nused to identify knowledge gaps in LLMs at scale and can provide deeper\ninsights into LLMs' internal inductive bias. The code and data are made\navailable at https://github.com/claws-lab/peek.",
      "authors": [
        "Kartik Sharma",
        "Yiqiao Jin",
        "Rakshit Trivedi",
        "Srijan Kumar"
      ],
      "published": "2025-08-08T05:32:31+00:00",
      "updated": "2025-08-08T05:32:31+00:00",
      "arxiv_id": "2508.06030v1",
      "url": "http://arxiv.org/pdf/2508.06030v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future",
      "abstract": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose \\textbf{Temporal Self-Rewarding\nLanguage Models} that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) \\textit{Anchored Rejection} - fixing rejected responses using the past\ninitial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data.",
      "authors": [
        "Yidong Wang",
        "Xin Wang",
        "Cunxiang Wang",
        "Junfeng Fang",
        "Qiufeng Wang",
        "Jianing Chu",
        "Xuran Meng",
        "Shuxun Yang",
        "Libo Qin",
        "Yue Zhang",
        "Wei Ye",
        "Shikun Zhang"
      ],
      "published": "2025-08-08T05:25:54+00:00",
      "updated": "2025-08-08T05:25:54+00:00",
      "arxiv_id": "2508.06026v1",
      "url": "http://arxiv.org/pdf/2508.06026v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models",
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in visual mathematical reasoning across various existing\nbenchmarks. However, these benchmarks are predominantly based on clean or\nprocessed multimodal inputs, without incorporating the images provided by\nreal-world Kindergarten through 12th grade (K-12) educational users. To address\nthis gap, we introduce MathReal, a meticulously curated dataset comprising\n2,000 mathematical questions with images captured by handheld mobile devices in\nauthentic scenarios. Each question is an image, containing the question text\nand visual element. We systematically classify the real images into three\nprimary categories: image quality degradation, perspective variation, and\nirrelevant content interference, which are further delineated into 14\nsubcategories. Additionally, MathReal spans five core knowledge and ability\ncategories, which encompass three question types and are divided into three\ndifficulty levels. To comprehensively evaluate the multimodal mathematical\nreasoning abilities of state-of-the-art MLLMs in real-world scenarios, we\ndesign six experimental settings that enable a systematic analysis of their\nperformance. Through extensive experimentation, we find that the\nproblem-solving abilities of existing MLLMs are significantly challenged in\nrealistic educational contexts. Based on this, we conduct a thorough analysis\nof their performance and error patterns, providing insights into their\nrecognition, comprehension, and reasoning capabilities, and outlining\ndirections for future improvements. Data and code:\nhttps://github.com/junfeng0288/MathReal.",
      "authors": [
        "Jun Feng",
        "Zixin Wang",
        "Zhentao Zhang",
        "Yue Guo",
        "Zhihan Zhou",
        "Xiuyi Chen",
        "Zhenyang Li",
        "Dawei Yin"
      ],
      "published": "2025-08-08T04:39:16+00:00",
      "updated": "2025-08-08T04:39:16+00:00",
      "arxiv_id": "2508.06009v1",
      "url": "http://arxiv.org/pdf/2508.06009v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "CountQA: How Well Do MLLMs Count in the Wild?",
      "abstract": "Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in\nunderstanding visual scenes, yet they exhibit a critical lack in a fundamental\ncognitive skill: object counting. This blind spot severely limits their\nreliability in real-world applications. To date, this capability has been\nlargely unevaluated in complex scenarios, as existing benchmarks either feature\nsparse object densities or are confined to specific visual domains, failing to\ntest models under realistic conditions. Addressing this gap, we introduce\nCountQA, a challenging new benchmark designed to probe this deficiency.\nComprising over 1,500 question-answer pairs, CountQA features real-world images\nwith high object density, clutter, and occlusion. We investigate this weakness\nby evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the\ntop-performing model achieves a mere 42.9% accuracy, with performance declining\nas object counts rise. By providing a dedicated benchmark to diagnose and\nrectify this core weakness, CountQA paves the way for a new generation of MLLMs\nthat are not only descriptively fluent but also numerically grounded and\nspatially aware. We will open-source the dataset and code upon paper acceptance\nto foster further research.",
      "authors": [
        "Jayant Sravan Tamarapalli",
        "Rynaa Grover",
        "Nilay Pande",
        "Sahiti Yerramilli"
      ],
      "published": "2025-08-08T04:23:04+00:00",
      "updated": "2025-08-08T04:23:04+00:00",
      "arxiv_id": "2508.06585v1",
      "url": "http://arxiv.org/pdf/2508.06585v1",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "When a Paper Has 1000 Authors: Rethinking Citation Metrics in the Era of LLMs",
      "abstract": "Author-level citation metrics provide a practical, interpretable, and\nscalable signal of scholarly influence in a complex research ecosystem. It has\nbeen widely used as a proxy in hiring decisions. However, the past five years\nhave seen the rapid emergence of large-scale publications in the field of large\nlanguage models and foundation models, with papers featuring hundreds to\nthousands of co-authors and receiving tens of thousands of citations within\nmonths. For example, Gemini has 1361 authors and has been cited around 4600\ntimes in 19 months. In such cases, traditional metrics, such as total citation\ncount and the $h$-index, fail to meaningfully distinguish individual\ncontributions. Therefore, we propose the following research question: How can\none identify standout researchers among thousands of co-authors in large-scale\nLLM papers? This question is particularly important in scenarios such as\nacademic hiring and funding decisions. In this paper, we introduce a novel\ncitation metric designed to address this challenge by balancing contributions\nacross large-scale and small-scale publications. We propose the SBCI index,\nanalyze its theoretical properties, and evaluate its behavior on synthetic\npublication datasets. Our results demonstrate that the proposed metric provides\na more robust and discriminative assessment of individual scholarly impact in\nthe era of large-scale collaborations.",
      "authors": [
        "Weihang Guo",
        "Zhao Song",
        "Jiahao Zhang"
      ],
      "published": "2025-08-08T04:18:26+00:00",
      "updated": "2025-08-08T04:18:26+00:00",
      "arxiv_id": "2508.06004v1",
      "url": "http://arxiv.org/pdf/2508.06004v1",
      "categories": [
        "cs.DL",
        "cs.IR"
      ],
      "primary_category": "cs.DL"
    },
    {
      "title": "Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning",
      "abstract": "Operational skill learning, inherently physical and reliant on hands-on\npractice and kinesthetic feedback, has yet to be effectively replicated in\nlarge language model (LLM)-supported training. Current LLM training assistants\nprimarily generate customized textual feedback, neglecting the crucial\nkinesthetic modality. This gap derives from the textual and uncertain nature of\nLLMs, compounded by concerns on user acceptance of LLM driven body control. To\nbridge this gap and realize the potential of collaborative human-LLM action,\nthis work explores human experience of LLM driven kinesthetic assistance.\nSpecifically, we introduced an \"Align-Analyze-Adjust\" strategy and developed\nFlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS)\nfor flight skill acquisition, a representative operational skill domain.\nFlightAxis learns flight skills from manuals and guides forearm movements\nduring simulated flight tasks. Our results demonstrate high user acceptance of\nLLM-mediated body control and significantly reduced task completion times.\nCrucially, trainees reported that this kinesthetic assistance enhanced their\nawareness of operation flaws and fostered increased engagement in the training\nprocess, rather than relieving perceived load. This work demonstrated the\npotential of kinesthetic LLM training in operational skill acquisition.",
      "authors": [
        "Wei Xiang",
        "Ziyue Lei",
        "Haoyuan Che",
        "Fangyuan Ye",
        "Xueting Wu",
        "Lingyun Sun"
      ],
      "published": "2025-08-08T04:05:47+00:00",
      "updated": "2025-08-08T04:05:47+00:00",
      "arxiv_id": "2508.06000v1",
      "url": "http://arxiv.org/pdf/2508.06000v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making",
      "abstract": "Complex medical decision-making involves cooperative workflows operated by\ndifferent clinicians. Designing AI multi-agent systems can expedite and augment\nhuman-level clinical decision-making. Existing multi-agent researches primarily\nfocus on language-only tasks, yet their extension to multimodal scenarios\nremains challenging. A blind combination of diverse vision-language models\n(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are\nless capable in instruction following and importantly self-reflection, compared\nto large language models (LLMs) of comparable sizes. This disparity largely\nconstrains VLMs' ability in cooperative workflows. In this study, we propose\nMedOrch, a mediator-guided multi-agent collaboration framework for medical\nmultimodal decision-making. MedOrch employs an LLM-based mediator agent that\nenables multiple VLM-based expert agents to exchange and reflect on their\noutputs towards collaboration. We utilize multiple open-source general-purpose\nand domain-specific VLMs instead of costly GPT-series models, revealing the\nstrength of heterogeneous models. We show that the collaboration within\ndistinct VLM-based agents can surpass the capabilities of any individual agent.\nWe validate our approach on five medical vision question answering benchmarks,\ndemonstrating superior collaboration performance without model training. Our\nfindings underscore the value of mediator-guided multi-agent collaboration in\nadvancing medical multimodal intelligence. Our code will be made publicly\navailable.",
      "authors": [
        "Kaitao Chen",
        "Mianxin Liu",
        "Daoming Zong",
        "Chaoyue Ding",
        "Shaohao Rui",
        "Yankai Jiang",
        "Mu Zhou",
        "Xiaosong Wang"
      ],
      "published": "2025-08-08T04:02:10+00:00",
      "updated": "2025-08-08T04:02:10+00:00",
      "arxiv_id": "2508.05996v1",
      "url": "http://arxiv.org/pdf/2508.05996v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization",
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode generation and structured reasoning; however, their performance often\ndegrades on complex tasks that require consistent multi-step planning. Recent\nwork has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet\nexisting approaches primarily focus on generating heuristic-based code for\noptimization or target simpler tasks where correctness alone is sufficient. In\nthis work, we propose MCTS-OPS, a novel neural-symbolic framework that\nformulates prompt selection as a sequential decision process guided by MCTS.\nOur method explores and refines multi-step prompt sequences for the goal of\nimproving code generation quality and enhancing the problem-solving\ncapabilities of LLMs in general optimization. Experiments on network\noptimization show significant improvement over the baselines, both in the\nsuccess rate of executing the generated code and in the optimization results\nwith the specified objective and constraints (2$\\sim$4$\\times$ higher reward\nand 3$\\times$ lower standard deviation). Moreover, it improves the chance of\nattaining the optimal solution by about 10\\% of cases, compared to baseline\nmethods in hard problems. These results highlight the promise of combining\nsymbolic planning with LLMs for robust, high-quality code generation in complex\ndomains.",
      "authors": [
        "Fei Xu Yu",
        "Gina Adam",
        "Nathaniel D. Bastian",
        "Tian Lan"
      ],
      "published": "2025-08-08T04:01:24+00:00",
      "updated": "2025-08-08T04:01:24+00:00",
      "arxiv_id": "2508.05995v1",
      "url": "http://arxiv.org/pdf/2508.05995v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge",
      "abstract": "Emotion recognition plays a vital role in enhancing human-computer\ninteraction. In this study, we tackle the MER-SEMI challenge of the MER2025\ncompetition by proposing a novel multimodal emotion recognition framework. To\naddress the issue of data scarcity, we leverage large-scale pre-trained models\nto extract informative features from visual, audio, and textual modalities.\nSpecifically, for the visual modality, we design a dual-branch visual encoder\nthat captures both global frame-level features and localized facial\nrepresentations. For the textual modality, we introduce a context-enriched\nmethod that employs large language models to enrich emotional cues within the\ninput text. To effectively integrate these multimodal features, we propose a\nfusion strategy comprising two key components, i.e., self-attention mechanisms\nfor dynamic modality weighting, and residual connections to preserve original\nrepresentations. Beyond architectural design, we further refine noisy labels in\nthe training set by a multi-source labeling strategy. Our approach achieves a\nsubstantial performance improvement over the official baseline on the\nMER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to\n78.63%, thereby validating the effectiveness of the proposed framework.",
      "authors": [
        "Juewen Hu",
        "Yexin Li",
        "Jiulin Li",
        "Shuo Chen",
        "Pring Wong"
      ],
      "published": "2025-08-08T03:55:25+00:00",
      "updated": "2025-08-08T03:55:25+00:00",
      "arxiv_id": "2508.05991v1",
      "url": "http://arxiv.org/pdf/2508.05991v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Omni Geometry Representation Learning vs Large Language Models for Geospatial Entity Resolution",
      "abstract": "The development, integration, and maintenance of geospatial databases rely\nheavily on efficient and accurate matching procedures of Geospatial Entity\nResolution (ER). While resolution of points-of-interest (POIs) has been widely\naddressed, resolution of entities with diverse geometries has been largely\noverlooked. This is partly due to the lack of a uniform technique for embedding\nheterogeneous geometries seamlessly into a neural network framework. Existing\nneural approaches simplify complex geometries to a single point, resulting in\nsignificant loss of spatial information. To address this limitation, we propose\nOmni, a geospatial ER model featuring an omni-geometry encoder. This encoder is\ncapable of embedding point, line, polyline, polygon, and multi-polygon\ngeometries, enabling the model to capture the complex geospatial intricacies of\nthe places being compared. Furthermore, Omni leverages transformer-based\npre-trained language models over individual textual attributes of place records\nin an Attribute Affinity mechanism. The model is rigorously tested on existing\npoint-only datasets and a new diverse-geometry geospatial ER dataset. Omni\nproduces up to 12% (F1) improvement over existing methods.\n  Furthermore, we test the potential of Large Language Models (LLMs) to conduct\ngeospatial ER, experimenting with prompting strategies and learning scenarios,\ncomparing the results of pre-trained language model-based methods with LLMs.\nResults indicate that LLMs show competitive results.",
      "authors": [
        "Kalana Wijegunarathna",
        "Kristin Stock",
        "Christopher B. Jones"
      ],
      "published": "2025-08-08T03:37:11+00:00",
      "updated": "2025-08-08T03:37:11+00:00",
      "arxiv_id": "2508.06584v1",
      "url": "http://arxiv.org/pdf/2508.06584v1",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB"
    },
    {
      "title": "Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education",
      "abstract": "While Large Language Models (LLMs) are often used as virtual tutors in\ncomputer science (CS) education, this approach can foster passive learning and\nover-reliance. This paper presents a novel pedagogical paradigm that inverts\nthis model: students act as instructors who must teach an LLM to solve\nproblems. To facilitate this, we developed strategies for designing questions\nwith engineered knowledge gaps that only a student can bridge, and we introduce\nSocrates, a system for deploying this method with minimal overhead. We\nevaluated our approach in an undergraduate course and found that this\nactive-learning method led to statistically significant improvements in student\nperformance compared to historical cohorts. Our work demonstrates a practical,\ncost-effective framework for using LLMs to deepen student engagement and\nmastery.",
      "authors": [
        "Xinming Yang",
        "Haasil Pujara",
        "Jun Li"
      ],
      "published": "2025-08-08T03:25:19+00:00",
      "updated": "2025-08-08T03:25:19+00:00",
      "arxiv_id": "2508.05979v1",
      "url": "http://arxiv.org/pdf/2508.05979v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning",
      "abstract": "In the domain of scientific machine learning, designing effective reward\nfunctions remains a challenge in reinforcement learning (RL), particularly in\nenvironments where task goals are difficult to specify numerically. Reward\nfunctions in existing work are predominantly based on heuristics, manual\nengineering, or task-specific tuning. In this work, we introduce a semantically\naligned reinforcement learning method where rewards are computed by aligning\nthe current state with a target semantic instruction using a\nSentence-Bidirectional Encoder Representations from Transformers (SBERT).\nInstead of relying on manually defined reward functions, the policy receives\nfeedback based on the reward, which is a cosine similarity between the goal\ntextual description and the statement description in the episode. We evaluated\nour approach in several environments and showed that semantic reward can guide\nlearning to achieve competitive control behavior, even in the absence of\nhand-crafted reward functions. Our study demonstrates a correlation between the\nlanguage embedding space and the conventional Euclidean space. This framework\nopens new horizons for aligning agent behavior with natural language goals and\nlays the groundwork for a more seamless integration of larger language models\n(LLMs) and fluid control applications.",
      "authors": [
        "Aoming Liang",
        "Chi Cheng",
        "Dashuai Chen",
        "Boai Sun",
        "Dixia Fan"
      ],
      "published": "2025-08-08T03:23:56+00:00",
      "updated": "2025-08-08T03:23:56+00:00",
      "arxiv_id": "2508.05977v1",
      "url": "http://arxiv.org/pdf/2508.05977v1",
      "categories": [
        "cs.LG",
        "physics.flu-dyn"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents",
      "abstract": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices.",
      "authors": [
        "Han Lin",
        "Jaemin Cho",
        "Amir Zadeh",
        "Chuan Li",
        "Mohit Bansal"
      ],
      "published": "2025-08-08T02:38:47+00:00",
      "updated": "2025-08-08T02:38:47+00:00",
      "arxiv_id": "2508.05954v1",
      "url": "http://arxiv.org/pdf/2508.05954v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "SCALEFeedback: A Large-Scale Dataset of Synthetic Computer Science Assignments for LLM-generated Educational Feedback Research",
      "abstract": "Using LLMs to give educational feedback to students for their assignments has\nattracted much attention in the AI in Education field. Yet, there is currently\nno large-scale open-source dataset of student assignments that includes\ndetailed assignment descriptions, rubrics, and student submissions across\nvarious courses. As a result, research on generalisable methodology for\nautomatic generation of effective and responsible educational feedback remains\nlimited. In the current study, we constructed a large-scale dataset of\nSynthetic Computer science Assignments for LLM-generated Educational Feedback\nresearch (SCALEFeedback). We proposed a Sophisticated Assignment Mimicry (SAM)\nframework to generate the synthetic dataset by one-to-one LLM-based imitation\nfrom real assignment descriptions, student submissions to produce their\nsynthetic versions. Our open-source dataset contains 10,000 synthetic student\nsubmissions spanning 155 assignments across 59 university-level computer\nscience courses. Our synthetic submissions achieved BERTScore F1 0.84, PCC of\n0.62 for assignment marks and 0.85 for length, compared to the corresponding\nreal-world assignment dataset, while ensuring perfect protection of student\nprivate information. All these results of our SAM framework outperformed\nresults of a naive mimicry method baseline. The LLM-generated feedback for our\nsynthetic assignments demonstrated the same level of effectiveness compared to\nthat of real-world assignment dataset. Our research showed that one-to-one LLM\nimitation is a promising method for generating open-source synthetic\neducational datasets that preserve the original dataset's semantic meaning and\nstudent data distribution, while protecting student privacy and institutional\ncopyright. SCALEFeedback enhances our ability to develop LLM-based\ngeneralisable methods for offering high-quality, automated educational feedback\nin a scalable way.",
      "authors": [
        "Keyang Qian",
        "Kaixun Yang",
        "Wei Dai",
        "Flora Jin",
        "Yixin Cheng",
        "Rui Guan",
        "Sadia Nawaz",
        "Zachari Swiecki",
        "Guanliang Chen",
        "Lixiang Yan",
        "Dragan GaÅ¡eviÄ"
      ],
      "published": "2025-08-08T02:37:20+00:00",
      "updated": "2025-08-08T02:37:20+00:00",
      "arxiv_id": "2508.05953v1",
      "url": "http://arxiv.org/pdf/2508.05953v1",
      "categories": [
        "cs.CY"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Dean of LLM Tutors: Exploring Comprehensive and Automated Evaluation of LLM-generated Educational Feedback via LLM Feedback Evaluators",
      "abstract": "The use of LLM tutors to provide automated educational feedback to students\non student assignment submissions has received much attention in the AI in\nEducation field. However, the stochastic nature and tendency for hallucinations\nin LLMs can undermine both quality of learning experience and adherence to\nethical standards. To address this concern, we propose a method that uses LLM\nfeedback evaluators (DeanLLMs) to automatically and comprehensively evaluate\nfeedback generated by LLM tutor for submissions on university assignments\nbefore it is delivered to students. This allows low-quality feedback to be\nrejected and enables LLM tutors to improve the feedback they generated based on\nthe evaluation results. We first proposed a comprehensive evaluation framework\nfor LLM-generated educational feedback, comprising six dimensions for feedback\ncontent, seven for feedback effectiveness, and three for hallucination types.\nNext, we generated a virtual assignment submission dataset covering 85\nuniversity assignments from 43 computer science courses using eight commonly\nused commercial LLMs. We labelled and open-sourced the assignment dataset to\nsupport the fine-tuning and evaluation of LLM feedback evaluators. Our findings\nshow that o3-pro demonstrated the best performance in zero-shot labelling of\nfeedback while o4-mini demonstrated the best performance in few-shot labelling\nof feedback. Moreover, GPT-4.1 achieved human expert level performance after\nfine-tuning (Accuracy 79.8%, F1-score 79.4%; human average Accuracy 78.3%,\nF1-score 82.6%). Finally, we used our best-performance model to evaluate 2,000\nassignment feedback instances generated by 10 common commercial LLMs, 200 each,\nto compare the quality of feedback generated by different LLMs. Our LLM\nfeedback evaluator method advances our ability to automatically provide\nhigh-quality and reliable educational feedback to students.",
      "authors": [
        "Keyang Qian",
        "Yixin Cheng",
        "Rui Guan",
        "Wei Dai",
        "Flora Jin",
        "Kaixun Yang",
        "Sadia Nawaz",
        "Zachari Swiecki",
        "Guanliang Chen",
        "Lixiang Yan",
        "Dragan GaÅ¡eviÄ"
      ],
      "published": "2025-08-08T02:36:23+00:00",
      "updated": "2025-08-08T02:36:23+00:00",
      "arxiv_id": "2508.05952v1",
      "url": "http://arxiv.org/pdf/2508.05952v1",
      "categories": [
        "cs.CY"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "A Survey on Task Scheduling in Carbon-Aware Container Orchestration",
      "abstract": "The soaring energy demands of large-scale software ecosystems and cloud data\ncenters, accelerated by the intensive training and deployment of large language\nmodels, have driven energy consumption and carbon footprint to unprecedented\nlevels. In response, both industry and academia are increasing efforts to\nreduce the carbon emissions associated with cloud computing through more\nefficient task scheduling and infrastructure orchestration. In this work, we\npresent a systematic review of various Kubernetes scheduling strategies,\ncategorizing them into hardware-centric and software-centric, annotating each\nwith its sustainability objectives, and grouping them according to the\nalgorithms they use. We propose a comprehensive taxonomy for cloud task\nscheduling studies, with a particular focus on the environmental sustainability\naspect. We analyze emerging research trends and open challenges, and our\nfindings provide critical insight into the design of sustainable scheduling\nsolutions for next-generation cloud computing systems.",
      "authors": [
        "Jialin Yang",
        "Zainab Saad",
        "Jiajun Wu",
        "Xiaoguang Niu",
        "Henry Leung",
        "Steve Drew"
      ],
      "published": "2025-08-08T02:20:16+00:00",
      "updated": "2025-08-08T02:20:16+00:00",
      "arxiv_id": "2508.05949v1",
      "url": "http://arxiv.org/pdf/2508.05949v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale",
      "abstract": "Detecting prosociality in text--communication intended to affirm, support, or\nimprove others' behavior--is a novel and increasingly important challenge for\ntrust and safety systems. Unlike toxic content detection, prosociality lacks\nwell-established definitions and labeled data, requiring new approaches to both\nannotation and deployment. We present a practical, three-stage pipeline that\nenables scalable, high-precision prosocial content classification while\nminimizing human labeling effort and inference costs. First, we identify the\nbest LLM-based labeling strategy using a small seed set of human-labeled\nexamples. We then introduce a human-AI refinement loop, where annotators review\nhigh-disagreement cases between GPT-4 and humans to iteratively clarify and\nexpand the task definition-a critical step for emerging annotation tasks like\nprosociality. This process results in improved label quality and definition\nalignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train\na two-stage inference system: a lightweight classifier handles high-confidence\npredictions, while only $\\sim$35\\% of ambiguous instances are escalated to\nGPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving\nhigh precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI\ninteraction, careful task formulation, and deployment-aware architecture design\ncan unlock scalable solutions for novel responsible AI tasks.",
      "authors": [
        "Rafal Kocielnik",
        "Min Kim",
        "Penphob",
        "Boonyarungsrit",
        "Fereshteh Soltani",
        "Deshawn Sambrano",
        "Animashree Anandkumar",
        "R. Michael Alvarez"
      ],
      "published": "2025-08-08T02:04:14+00:00",
      "updated": "2025-08-08T02:04:14+00:00",
      "arxiv_id": "2508.05938v1",
      "url": "http://arxiv.org/pdf/2508.05938v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "I.2.7; K.4"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Towards Reliable Generative AI-Driven Scaffolding: Reducing Hallucinations and Enhancing Quality in Self-Regulated Learning Support",
      "abstract": "Generative Artificial Intelligence (GenAI) holds a potential to advance\nexisting educational technologies with capabilities to automatically generate\npersonalised scaffolds that support students' self-regulated learning (SRL).\nWhile advancements in large language models (LLMs) promise improvements in the\nadaptability and quality of educational technologies for SRL, there remain\nconcerns about the hallucinations in content generated by LLMs, which can\ncompromise both the learning experience and ethical standards. To address these\nchallenges, we proposed GenAI-enabled approaches for evaluating personalised\nSRL scaffolds before they are presented to students, aiming for reducing\nhallucinations and improving the overall quality of LLM-generated personalised\nscaffolds. Specifically, two approaches are investigated. The first approach\ninvolved developing a multi-agent system approach for reliability evaluation to\nassess the extent to which LLM-generated scaffolds accurately target relevant\nSRL processes. The second approach utilised the \"LLM-as-a-Judge\" technique for\nquality evaluation that evaluates LLM-generated scaffolds for their helpfulness\nin supporting students. We constructed evaluation datasets, and compared our\nresults with single-agent LLM systems and machine learning approach baselines.\nOur findings indicate that the reliability evaluation approach is highly\neffective and outperforms the baselines, showing almost perfect alignment with\nhuman experts' evaluations. Moreover, both proposed evaluation approaches can\nbe harnessed to effectively reduce hallucinations. Additionally, we identified\nand discussed bias limitations of the \"LLM-as-a-Judge\" technique in evaluating\nLLM-generated scaffolds. We suggest incorporating these approaches into\nGenAI-powered personalised SRL scaffolding systems to mitigate hallucination\nissues and improve the overall scaffolding quality.",
      "authors": [
        "Keyang Qian",
        "Shiqi Liu",
        "Tongguang Li",
        "Mladen RakoviÄ",
        "Xinyu Li",
        "Rui Guan",
        "Inge Molenaar",
        "Sadia Nawaz",
        "Zachari Swiecki",
        "Lixiang Yan",
        "Dragan GaÅ¡eviÄ"
      ],
      "published": "2025-08-08T01:40:10+00:00",
      "updated": "2025-08-08T01:40:10+00:00",
      "arxiv_id": "2508.05929v1",
      "url": "http://arxiv.org/pdf/2508.05929v1",
      "categories": [
        "cs.CY"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting",
      "abstract": "Group-Relative Policy Optimization (GRPO) is a key technique for training\nlarge reasoning models, yet it suffers from a critical vulnerability: the\n\\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning\nprocess. This problem is most severe in unbalanced response groups,\nparadoxically degrading the signal precisely when it should be most\ninformative. To address this challenge, we propose Stable Group-Relative Policy\nOptimization (S-GRPO), a principled enhancement that derives optimal,\nnoise-aware advantage weights to stabilize training. Our comprehensive\nexperiments on mathematical reasoning benchmarks demonstrate S-GRPO's\neffectiveness and robustness. On various models, S-GRPO significantly\noutperforms DR. GRPO, achieving performance gains of +2.5% on\nQwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on\nQwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn\nunder 20% synthetic reward noise, S-GRPO maintains stable learning progress.\nThese results highlight S-GRPO's potential for more robust and effective\ntraining of large-scale reasoning models. \\footnote{Code and data are available\nat: https://github.com/shenpeijun0212/S-GRPO",
      "authors": [
        "Si Shen",
        "Peijun Shen",
        "Wenhua Zhao",
        "Danhao Zhu"
      ],
      "published": "2025-08-08T01:24:06+00:00",
      "updated": "2025-08-08T01:24:06+00:00",
      "arxiv_id": "2508.05928v1",
      "url": "http://arxiv.org/pdf/2508.05928v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs",
      "abstract": "The conversational capabilities of large language models hold significant\npromise for enabling scalable and interactive tutoring. While prior research\nhas primarily examined their capacity for Socratic questioning, it often\noverlooks a critical dimension: adaptively guiding learners based on their\ncognitive states. This study shifts focus from mere question generation to the\nbroader instructional guidance capability. We ask: Can LLMs emulate expert\ntutors who dynamically adjust strategies in response to learners'\nunderstanding? To investigate this, we propose GuideEval, a benchmark grounded\nin authentic educational dialogues that evaluates pedagogical guidance through\na three-phase behavioral framework: (1) Perception, inferring learner states;\n(2) Orchestration, adapting instructional strategies; and (3) Elicitation,\nstimulating proper reflections. Empirical findings reveal that existing LLMs\nfrequently fail to provide effective adaptive scaffolding when learners exhibit\nconfusion or require redirection. Furthermore, we introduce a behavior-guided\nfinetuning strategy that leverages behavior-prompted instructional dialogues,\nsignificantly enhancing guidance performance. By shifting the focus from\nisolated content evaluation to learner-centered interaction, our work advocates\na more dialogic paradigm for evaluating Socratic LLMs.",
      "authors": [
        "Ying Liu",
        "Can Li",
        "Ting Zhang",
        "Mei Wang",
        "Qiannan Zhu",
        "Jian Li",
        "Hua Huang"
      ],
      "published": "2025-08-08T01:02:44+00:00",
      "updated": "2025-08-08T01:02:44+00:00",
      "arxiv_id": "2508.06583v1",
      "url": "http://arxiv.org/pdf/2508.06583v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation",
      "abstract": "Large Language Models (LLMs) have shown improved generation performance\nthrough retrieval-augmented generation (RAG) following the retriever-reader\nparadigm, which supplements model inputs with externally retrieved knowledge.\nHowever, prior work often evaluates RAG holistically, assessing the retriever\nand reader jointly, making it difficult to isolate the true contribution of\nretrieval, particularly given the prompt sensitivity of LLMs used as readers.\nWe introduce Spectrum Projection Score (SPS), a lightweight, supervision-free\nmetric that allows the reader to gauge the semantic alignment of a retrieved\nsummary with its hidden representation by comparing the area formed by\ngenerated tokens from the summary, and the principal directions of subspace in\nthe reader and to measure the relevance. Building on SPS we present xCompress,\nan inference time controller framework that dynamically samples, ranks, and\ncompresses retrieval summary candidates. Extensive experiments on five QA\nbenchmarks with four open source LLMs show that SPS not only enhances\nperformance across a range of tasks but also provides a principled perspective\non the interaction between retrieval and generation.",
      "authors": [
        "Zhanghao Hu",
        "Qinglin Zhu",
        "Siya Qi",
        "Yulan He",
        "Hanqi Yan",
        "Lin Gui"
      ],
      "published": "2025-08-08T00:13:48+00:00",
      "updated": "2025-08-08T00:13:48+00:00",
      "arxiv_id": "2508.05909v1",
      "url": "http://arxiv.org/pdf/2508.05909v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)",
      "abstract": "Quantization is usually regarded as a means to trade quality of performance\nfor reduced compute requirements, i.e., as a suboptimal approximation. However,\nif examined in terms of a fixed overall resource budget, a very different\nperspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit\nquantization that deterministically provides gradient information with no\nforward-path penalty. Our analysis provides evidence that it may improve\ninformation density compared to non-quantized alternatives.",
      "authors": [
        "Jeffrey Uhlmann"
      ],
      "published": "2025-08-08T00:01:29+00:00",
      "updated": "2025-08-08T00:01:29+00:00",
      "arxiv_id": "2508.05905v1",
      "url": "http://arxiv.org/pdf/2508.05905v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models",
      "abstract": "Affective Computing has been established as a crucial field of inquiry to\nadvance the holistic development of Artificial Intelligence (AI) systems.\nFoundation models -- especially Large Language Models (LLMs) -- have been\nevaluated, trained, or instruction-tuned in several past works, to become\nbetter predictors or generators of emotion. Most of these studies, however,\napproach emotion-related tasks in a supervised manner, assessing or training\nthe capabilities of LLMs using discrete emotion labels associated with stimuli\n(e.g., text, images, video, audio). Evaluation studies, in particular, have\noften been limited to standard and superficial emotion-related tasks, such as\nthe recognition of evoked or expressed emotions. In this paper, we move beyond\nsurface-level emotion tasks to investigate how LLMs reason about emotions\nthrough cognitive dimensions. Drawing from cognitive appraisal theory, we\nexamine whether LLMs produce coherent and plausible cognitive reasoning when\nreasoning about emotionally charged stimuli. We introduce a large-scale\nbenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal\ncognitive structures implicitly used by LLMs for emotional reasoning. Through a\nplethora of evaluation experiments and analysis, we seek to answer: (a) Are\nmodels more likely to implicitly rely on specific cognitive appraisal\ndimensions?, (b) What cognitive dimensions are important for characterizing\nspecific emotions?, and, (c) Can the internal representations of different\nemotion categories in LLMs be interpreted through cognitive appraisal\ndimensions? Our results and analyses reveal diverse reasoning patterns across\ndifferent LLMs. Our benchmark and code will be made publicly available.",
      "authors": [
        "Sree Bhattacharyya",
        "Lucas Craig",
        "Tharun Dilliraj",
        "Jia Li",
        "James Z. Wang"
      ],
      "published": "2025-08-07T22:19:15+00:00",
      "updated": "2025-08-07T22:19:15+00:00",
      "arxiv_id": "2508.05880v1",
      "url": "http://arxiv.org/pdf/2508.05880v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models",
      "abstract": "Blockchain technology offers a promising foundation for modernizing E-Voting\nsystems by enhancing transparency, decentralization, and security. Yet,\nreal-world adoption remains limited due to persistent challenges such as\nscalability constraints, high computational demands, and complex privacy\nrequirements. This paper presents a comparative framework for analyzing\nblockchain-based E-Voting architectures, consensus mechanisms, and\ncryptographic protocols. We examine the limitations of prevalent models like\nProof of Work, Proof of Stake, and Delegated Proof of Stake, and propose\noptimization strategies that include hybrid consensus, lightweight\ncryptography, and decentralized identity management. Additionally, we explore\nthe novel role of Large Language Models (LLMs) in smart contract generation,\nanomaly detection, and user interaction. Our findings offer a foundation for\ndesigning secure, scalable, and intelligent blockchain-based E-Voting systems\nsuitable for national-scale deployment. This work lays the groundwork for\nbuilding an end-to-end blockchain E-Voting prototype enhanced by LLM-guided\nsmart contract generation and validation, supported by a systematic framework\nand simulation-based analysis.",
      "authors": [
        "Kiana Kiashemshaki",
        "Elvis Nnaemeka Chukwuani",
        "Mohammad Jalili Torkamani",
        "Negin Mahmoudi"
      ],
      "published": "2025-08-07T21:34:21+00:00",
      "updated": "2025-08-07T21:34:21+00:00",
      "arxiv_id": "2508.05865v1",
      "url": "http://arxiv.org/pdf/2508.05865v1",
      "categories": [
        "cs.CR",
        "cs.SE",
        "C.2; D.2; D.4.1; D.4.4; D.4.6; I.2.7"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Safety of Embodied Navigation: A Survey",
      "abstract": "As large language models (LLMs) continue to advance and gain influence, the\ndevelopment of embodied AI has accelerated, drawing significant attention,\nparticularly in navigation scenarios. Embodied navigation requires an agent to\nperceive, interact with, and adapt to its environment while moving toward a\nspecified target in unfamiliar settings. However, the integration of embodied\nnavigation into critical applications raises substantial safety concerns. Given\ntheir deployment in dynamic, real-world environments, ensuring the safety of\nsuch systems is critical. This survey provides a comprehensive analysis of\nsafety in embodied navigation from multiple perspectives, encompassing attack\nstrategies, defense mechanisms, and evaluation methodologies. Beyond conducting\na comprehensive examination of existing safety challenges, mitigation\ntechnologies, and various datasets and metrics that assess effectiveness and\nrobustness, we explore unresolved issues and future research directions in\nembodied navigation safety. These include potential attack methods, mitigation\nstrategies, more reliable evaluation techniques, and the implementation of\nverification frameworks. By addressing these critical gaps, this survey aims to\nprovide valuable insights that can guide future research toward the development\nof safer and more reliable embodied navigation systems. Furthermore, the\nfindings of this study have broader implications for enhancing societal safety\nand increasing industrial efficiency.",
      "authors": [
        "Zixia Wang",
        "Jia Hu",
        "Ronghui Mu"
      ],
      "published": "2025-08-07T21:09:48+00:00",
      "updated": "2025-08-07T21:09:48+00:00",
      "arxiv_id": "2508.05855v1",
      "url": "http://arxiv.org/pdf/2508.05855v1",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "An Effective Approach for Node Classification in Textual Graphs",
      "abstract": "Textual Attribute Graphs (TAGs) are critical for modeling complex networks\nlike citation networks, but effective node classification remains challenging\ndue to difficulties in integrating rich semantics from text with structural\ngraph information. Existing methods often struggle with capturing nuanced\ndomain-specific terminology, modeling long-range dependencies, adapting to\ntemporal evolution, and scaling to massive datasets. To address these issues,\nwe propose a novel framework that integrates TAPE (Text-Attributed Graph\nRepresentation Enhancement) with Graphormer. Our approach leverages a large\nlanguage model (LLM), specifically ChatGPT, within the TAPE framework to\ngenerate semantically rich explanations from paper content, which are then\nfused into enhanced node representations. These embeddings are combined with\nstructural features using a novel integration layer with learned attention\nweights. Graphormer's path-aware position encoding and multi-head attention\nmechanisms are employed to effectively capture long-range dependencies across\nthe citation network. We demonstrate the efficacy of our framework on the\nchallenging ogbn-arxiv dataset, achieving state-of-the-art performance with a\nclassification accuracy of 0.772, significantly surpassing the best GCN\nbaseline of 0.713. Our method also yields strong results in precision (0.671),\nrecall (0.577), and F1-score (0.610). We validate our approach through\ncomprehensive ablation studies that quantify the contribution of each\ncomponent, demonstrating the synergy between semantic and structural\ninformation. Our framework provides a scalable and robust solution for node\nclassification in dynamic TAGs, offering a promising direction for future\nresearch in knowledge systems and scientific discovery.",
      "authors": [
        "Rituparna Datta",
        "Nibir Chandra Mandal"
      ],
      "published": "2025-08-07T20:24:00+00:00",
      "updated": "2025-08-07T20:24:00+00:00",
      "arxiv_id": "2508.05836v1",
      "url": "http://arxiv.org/pdf/2508.05836v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference",
      "abstract": "Large Language Models (LLMs) have significantly advanced audio processing by\nleveraging audio codecs to discretize audio into tokens, enabling the\napplication of language modeling techniques to speech data. However, existing\naudio codecs often operate at high frame rates, leading to slow training and\ninference, particularly for autoregressive models. To address this, there is\ngrowing interest in low frame-rate audio codecs, which reduce the number of\nautoregressive steps required to generate one second of audio. In this paper,\nwe conduct ablation studies to examine the impact of frame rate, bitrate, and\ncausality on codec reconstruction quality. Based on our findings, we introduce\nNanoCodec, a state-of-the-art audio codec that achieves high-quality\ncompression at just 12.5 frames per second (FPS). NanoCodec outperforms related\nworks across various bitrate ranges, establishing a new benchmark for\nlow-latency and efficient Speech LLM training and inference.",
      "authors": [
        "Edresson Casanova",
        "Paarth Neekhara",
        "Ryan Langman",
        "Shehzeen Hussain",
        "Subhankar Ghosh",
        "Xuesong Yang",
        "Ante JukiÄ",
        "Jason Li",
        "Boris Ginsburg"
      ],
      "published": "2025-08-07T20:20:32+00:00",
      "updated": "2025-08-07T20:20:32+00:00",
      "arxiv_id": "2508.05835v1",
      "url": "http://arxiv.org/pdf/2508.05835v1",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "primary_category": "eess.AS"
    },
    {
      "title": "\"Mirror\" Language AI Models of Depression are Criterion-Contaminated",
      "abstract": "A growing number of studies show near-perfect LLM language-based prediction\nof depression assessment scores (up to R2 of .70). However, many develop these\nmodels directly from language responses to depression assessments. These\n\"Mirror models\" suffer from \"criterion contamination\", which arises when a\npredicted score depends in part on the predictors themselves. This causes\nartificial effect size inflation which reduces model generalizability. The\npresent study compares the performance of Mirror models versus \"Non-Mirror\nmodels\", which are developed from language that does not mirror the assessment\nthey are developed to predict. N = 110 research participants completed two\ndifferent interviews: structured diagnostic and life history interviews. GPT-4,\nGPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic\ninterview depression scores from the two transcripts separately. Mirror models\n(using structured diagnostic data) showed very large effect sizes (e.g., R2 =\n.80). As expected, NonMirror models (using life history data) demonstrated\nsmaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror\nand Non-Mirror model-predicted structured interview depression scores were\ncorrelated with self-reported depression symptoms, Mirror and NonMirror\nperformed the same (e.g., r = ~.54), indicating that Mirror models contain bias\nperhaps due to criterion contamination. Topic modeling identified clusters\nacross Mirror and Non-Mirror models, as well as between true-positive and\nfalse-positive predictions. In this head-to-head comparison study, Mirror\nlanguage AI models of depression showed artificially inflated effect sizes and\nless generalizability. As language AI models for depression continue to evolve,\nincorporating Non-Mirror models may identify interpretable, and generalizable\nsemantic features that have unique utility in real-world psychological\nassessment.",
      "authors": [
        "Tong Li",
        "Rasiq Hussain",
        "Mehak Gupta",
        "Joshua R. Oltmanns"
      ],
      "published": "2025-08-07T20:13:00+00:00",
      "updated": "2025-08-07T20:13:00+00:00",
      "arxiv_id": "2508.05830v1",
      "url": "http://arxiv.org/pdf/2508.05830v1",
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "AI-Guided Exploration of Large-Scale Codebases",
      "abstract": "Understanding large-scale, complex software systems is a major challenge for\ndevelopers, who spend a significant portion of their time on program\ncomprehension. Traditional tools such as static visualizations and reverse\nengineering techniques provide structural insights but often lack\ninteractivity, adaptability, and integration with contextual information.\nRecent advancements in large language models (LLMs) offer new opportunities to\nenhance code exploration workflows, yet their lack of grounding and integration\nwith structured views limits their effectiveness. This work introduces a hybrid\napproach that integrates deterministic reverse engineering with LLM-guided,\nintent-aware visual exploration. The proposed system combines UML-based\nvisualization, dynamic user interfaces, historical context, and collaborative\nfeatures into an adaptive tool for code comprehension. By interpreting user\nqueries and interaction patterns, the LLM helps developers navigate and\nunderstand complex codebases more effectively. A prototype implementation for\nJava demonstrates the feasibility of this approach. Future work includes\nempirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM\ninteraction models. This research lays the groundwork for intelligent,\ninteractive environments that align with developer cognition and collaborative\nworkflows.",
      "authors": [
        "Yoseph Berhanu Alebachew"
      ],
      "published": "2025-08-07T19:15:37+00:00",
      "updated": "2025-08-07T19:15:37+00:00",
      "arxiv_id": "2508.05799v1",
      "url": "http://arxiv.org/pdf/2508.05799v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification",
      "abstract": "Large Language Models (LLMs) are known to produce hallucinations - factually\nincorrect or fabricated information - which poses significant challenges for\nmany Natural Language Processing (NLP) applications, such as dialogue systems.\nAs a result, detecting hallucinations has become a critical area of research.\nCurrent approaches to hallucination detection in dialogue systems primarily\nfocus on verifying the factual consistency of generated responses. However,\nthese responses often contain a mix of accurate, inaccurate or unverifiable\nfacts, making one factual label overly simplistic and coarse-grained. In this\npaper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact\nverification, which involves verifying atomic facts extracted from dialogue\nresponses. To support this, we construct a dataset based on publicly available\ndialogue datasets and evaluate it using various baseline methods. Experimental\nresults demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning\ncan enhance performance in dialogue fact verification. Despite this, the best\nF1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is\nonly 0.75, indicating that the benchmark remains a challenging task for future\nresearch. Our dataset and code will be public on GitHub.",
      "authors": [
        "Xiangyan Chen",
        "Yufeng Li",
        "Yujian Gan",
        "Arkaitz Zubiaga",
        "Matthew Purver"
      ],
      "published": "2025-08-07T18:51:03+00:00",
      "updated": "2025-08-07T18:51:03+00:00",
      "arxiv_id": "2508.05782v1",
      "url": "http://arxiv.org/pdf/2508.05782v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation",
      "abstract": "Large Language Models (LLMs) have revolutionized content creation across\ndigital platforms, offering unprecedented capabilities in natural language\ngeneration and understanding. These models enable beneficial applications such\nas content generation, question and answering (Q&A), programming, and code\nreasoning. Meanwhile, they also pose serious risks by inadvertently or\nintentionally producing toxic, offensive, or biased content. This dual role of\nLLMs, both as powerful tools for solving real-world problems and as potential\nsources of harmful language, presents a pressing sociotechnical challenge. In\nthis survey, we systematically review recent studies spanning unintentional\ntoxicity, adversarial jailbreaking attacks, and content moderation techniques.\nWe propose a unified taxonomy of LLM-related harms and defenses, analyze\nemerging multimodal and LLM-assisted jailbreak strategies, and assess\nmitigation efforts, including reinforcement learning with human feedback\n(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the\nevolving landscape of LLM safety, identifies limitations in current evaluation\nmethodologies, and outlines future research directions to guide the development\nof robust and ethically aligned language technologies.",
      "authors": [
        "Chi Zhang",
        "Changjia Zhu",
        "Junjie Xiong",
        "Xiaoran Xu",
        "Lingyao Li",
        "Yao Liu",
        "Zhuo Lu"
      ],
      "published": "2025-08-07T18:42:16+00:00",
      "updated": "2025-08-07T18:42:16+00:00",
      "arxiv_id": "2508.05775v1",
      "url": "http://arxiv.org/pdf/2508.05775v1",
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "A Framework for Inherently Safer AGI through Language-Mediated Active Inference",
      "abstract": "This paper proposes a novel framework for developing safe Artificial General\nIntelligence (AGI) by combining Active Inference principles with Large Language\nModels (LLMs). We argue that traditional approaches to AI safety, focused on\npost-hoc interpretability and reward engineering, have fundamental limitations.\nWe present an architecture where safety guarantees are integrated into the\nsystem's core design through transparent belief representations and\nhierarchical value alignment. Our framework leverages natural language as a\nmedium for representing and manipulating beliefs, enabling direct human\noversight while maintaining computational tractability. The architecture\nimplements a multi-agent system where agents self-organize according to Active\nInference principles, with preferences and safety constraints flowing through\nhierarchical Markov blankets. We outline specific mechanisms for ensuring\nsafety, including: (1) explicit separation of beliefs and preferences in\nnatural language, (2) bounded rationality through resource-aware free energy\nminimization, and (3) compositional safety through modular agent structures.\nThe paper concludes with a research agenda centered on the Abstraction and\nReasoning Corpus (ARC) benchmark, proposing experiments to validate our\nframework's safety properties. Our approach offers a path toward AGI\ndevelopment that is inherently safer, rather than retrofitted with safety\nmeasures.",
      "authors": [
        "Bo Wen"
      ],
      "published": "2025-08-07T18:28:54+00:00",
      "updated": "2025-08-07T18:28:54+00:00",
      "arxiv_id": "2508.05766v1",
      "url": "http://arxiv.org/pdf/2508.05766v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY",
        "nlin.AO"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification",
      "abstract": "We present a simple yet theoretically motivated improvement to Supervised\nFine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited\ngeneralization compared to reinforcement learning (RL). Through mathematical\nanalysis, we reveal that standard SFT gradients implicitly encode a problematic\nreward structure that may severely restrict the generalization capabilities of\nmodel. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing\ngradient updates for each token by dynamically rescaling the objective function\nwith the probability of this token. Remarkably, this single-line code change\nsignificantly outperforms standard SFT across multiple challenging benchmarks\nand base models, demonstrating greatly improved generalization. Additionally,\nour approach shows competitive results in offline RL settings, offering an\neffective yet simpler alternative. This work bridges theoretical insight and\npractical solutions, substantially advancing SFT performance. The code will be\navailable at https://github.com/yongliang-wu/DFT.",
      "authors": [
        "Yongliang Wu",
        "Yizhou Zhou",
        "Zhou Ziheng",
        "Yingzhe Peng",
        "Xinyu Ye",
        "Xinting Hu",
        "Wenbo Zhu",
        "Lu Qi",
        "Ming-Hsuan Yang",
        "Xu Yang"
      ],
      "published": "2025-08-07T17:59:04+00:00",
      "updated": "2025-08-07T17:59:04+00:00",
      "arxiv_id": "2508.05629v1",
      "url": "http://arxiv.org/pdf/2508.05629v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations",
      "abstract": "Large Language Models (LLMs) have started to demonstrate the ability to\npersuade humans, yet our understanding of how this dynamic transpires is\nlimited. Recent work has used linear probes, lightweight tools for analyzing\nmodel representations, to study various LLM skills such as the ability to model\nuser sentiment and political perspective. Motivated by this, we apply probes to\nstudy persuasion dynamics in natural, multi-turn conversations. We leverage\ninsights from cognitive science to train probes on distinct aspects of\npersuasion: persuasion success, persuadee personality, and persuasion strategy.\nDespite their simplicity, we show that they capture various aspects of\npersuasion at both the sample and dataset levels. For instance, probes can\nidentify the point in a conversation where the persuadee was persuaded or where\npersuasive success generally occurs across the entire dataset. We also show\nthat in addition to being faster than expensive prompting-based approaches,\nprobes can do just as well and even outperform prompting in some settings, such\nas when uncovering persuasion strategy. This suggests probes as a plausible\navenue for studying other complex behaviours such as deception and\nmanipulation, especially in multi-turn settings and large-scale dataset\nanalysis where prompting-based methods would be computationally inefficient.",
      "authors": [
        "Brandon Jaipersaud",
        "David Krueger",
        "Ekdeep Singh Lubana"
      ],
      "published": "2025-08-07T17:58:41+00:00",
      "updated": "2025-08-07T17:58:41+00:00",
      "arxiv_id": "2508.05625v1",
      "url": "http://arxiv.org/pdf/2508.05625v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Simulating Human-Like Learning Dynamics with LLM-Empowered Agents",
      "abstract": "Capturing human learning behavior based on deep learning methods has become a\nmajor research focus in both psychology and intelligent systems. Recent\napproaches rely on controlled experiments or rule-based models to explore\ncognitive processes. However, they struggle to capture learning dynamics, track\nprogress over time, or provide explainability. To address these challenges, we\nintroduce LearnerAgent, a novel multi-agent framework based on Large Language\nModels (LLMs) to simulate a realistic teaching environment. To explore\nhuman-like learning dynamics, we construct learners with psychologically\ngrounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free\nGeneral Learner to inspect the base LLM's default behavior. Through weekly\nknowledge acquisition, monthly strategic choices, periodic tests, and peer\ninteraction, we can track the dynamic learning progress of individual learners\nover a full-year journey. Our findings are fourfold: 1) Longitudinal analysis\nreveals that only Deep Learner achieves sustained cognitive growth. Our\nspecially designed \"trap questions\" effectively diagnose Surface Learner's\nshallow knowledge. 2) The behavioral and cognitive patterns of distinct\nlearners align closely with their psychological profiles. 3) Learners'\nself-concept scores evolve realistically, with the General Learner developing\nsurprisingly high self-efficacy despite its cognitive limitations. 4)\nCritically, the default profile of base LLM is a \"diligent but brittle Surface\nLearner\"-an agent that mimics the behaviors of a good student but lacks true,\ngeneralizable understanding. Extensive simulation experiments demonstrate that\nLearnerAgent aligns well with real scenarios, yielding more insightful findings\nabout LLMs' behavior.",
      "authors": [
        "Yu Yuan",
        "Lili Zhao",
        "Wei Chen",
        "Guangting Zheng",
        "Kai Zhang",
        "Mengdi Zhang",
        "Qi Liu"
      ],
      "published": "2025-08-07T17:57:46+00:00",
      "updated": "2025-08-07T17:57:46+00:00",
      "arxiv_id": "2508.05622v1",
      "url": "http://arxiv.org/pdf/2508.05622v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "The Missing Reward: Active Inference in the Era of Experience",
      "abstract": "This paper argues that Active Inference (AIF) provides a crucial foundation\nfor developing autonomous AI agents capable of learning from experience without\ncontinuous human reward engineering. As AI systems begin to exhaust\nhigh-quality training data and rely on increasingly large human workforces for\nreward design, the current paradigm faces significant scalability challenges\nthat could impede progress toward genuinely autonomous intelligence. The\nproposal for an ``Era of Experience,'' where agents learn from self-generated\ndata, is a promising step forward. However, this vision still depends on\nextensive human engineering of reward functions, effectively shifting the\nbottleneck from data curation to reward curation. This highlights what we\nidentify as the \\textbf{grounded-agency gap}: the inability of contemporary AI\nsystems to autonomously formulate, adapt, and pursue objectives in response to\nchanging circumstances. We propose that AIF can bridge this gap by replacing\nexternal reward signals with an intrinsic drive to minimize free energy,\nallowing agents to naturally balance exploration and exploitation through a\nunified Bayesian objective. By integrating Large Language Models as generative\nworld models with AIF's principled decision-making framework, we can create\nagents that learn efficiently from experience while remaining aligned with\nhuman values. This synthesis offers a compelling path toward AI systems that\ncan develop autonomously while adhering to both computational and physical\nconstraints.",
      "authors": [
        "Bo Wen"
      ],
      "published": "2025-08-07T17:57:12+00:00",
      "updated": "2025-08-07T17:57:12+00:00",
      "arxiv_id": "2508.05619v1",
      "url": "http://arxiv.org/pdf/2508.05619v1",
      "categories": [
        "cs.AI",
        "nlin.AO",
        "physics.bio-ph",
        "physics.comp-ph",
        "physics.hist-ph"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Learning to Reason for Factuality",
      "abstract": "Reasoning Large Language Models (R-LLMs) have significantly advanced complex\nreasoning tasks but often struggle with factuality, generating substantially\nmore hallucinations than their non-reasoning counterparts on long-form\nfactuality benchmarks. However, extending online Reinforcement Learning (RL), a\nkey component in recent R-LLM advancements, to the long-form factuality setting\nposes several unique challenges due to the lack of reliable verification\nmethods. Previous work has utilized automatic factuality evaluation frameworks\nsuch as FActScore to curate preference data in the offline RL setting, yet we\nfind that directly leveraging such methods as the reward in online RL leads to\nreward hacking in multiple ways, such as producing less detailed or relevant\nresponses. We propose a novel reward function that simultaneously considers the\nfactual precision, response detail level, and answer relevance, and applies\nonline RL to learn high quality factual reasoning. Evaluated on six long-form\nfactuality benchmarks, our factual reasoning model achieves an average\nreduction of 23.1 percentage points in hallucination rate, a 23% increase in\nanswer detail level, and no degradation in the overall response helpfulness.",
      "authors": [
        "Xilun Chen",
        "Ilia Kulikov",
        "Vincent-Pierre Berges",
        "Barlas OÄuz",
        "Rulin Shao",
        "Gargi Ghosh",
        "Jason Weston",
        "Wen-tau Yih"
      ],
      "published": "2025-08-07T17:57:09+00:00",
      "updated": "2025-08-07T17:57:09+00:00",
      "arxiv_id": "2508.05618v1",
      "url": "http://arxiv.org/pdf/2508.05618v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution",
      "abstract": "Trajectory prediction is a critical task in modeling human behavior,\nespecially in safety-critical domains such as social robotics and autonomous\nvehicle navigation. Traditional heuristics based on handcrafted rules often\nlack accuracy and generalizability. Although deep learning approaches offer\nimproved performance, they typically suffer from high computational cost,\nlimited explainability, and, importantly, poor generalization to\nout-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a\nframework that leverages Large Language Models (LLMs) to automatically design\ntrajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to\ngenerate and refine prediction heuristics from past trajectory data. We propose\ntwo key innovations: Cross-Generation Elite Sampling to encourage population\ndiversity, and a Statistics Feedback Loop that enables the LLM to analyze and\nimprove alternative predictions. Our evaluations demonstrate that TrajEvo\noutperforms existing heuristic methods across multiple real-world datasets, and\nnotably surpasses both heuristic and deep learning methods in generalizing to\nan unseen OOD real-world dataset. TrajEvo marks a promising step toward the\nautomated design of fast, explainable, and generalizable trajectory prediction\nheuristics. We release our source code to facilitate future research at\nhttps://github.com/ai4co/trajevo.",
      "authors": [
        "Zhikai Zhao",
        "Chuanbo Hua",
        "Federico Berto",
        "Kanghoon Lee",
        "Zihan Ma",
        "Jiachen Li",
        "Jinkyoo Park"
      ],
      "published": "2025-08-07T17:55:10+00:00",
      "updated": "2025-08-07T17:55:10+00:00",
      "arxiv_id": "2508.05616v1",
      "url": "http://arxiv.org/pdf/2508.05616v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE",
        "cs.RO"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks",
      "abstract": "Large language models excel at abstract reasoning but their capacity for\nembodied agent reasoning remains largely unexplored. We present OmniEAR, a\ncomprehensive framework for evaluating how language models reason about\nphysical interactions, tool usage, and multi-agent coordination in embodied\ntasks. Unlike existing benchmarks that provide predefined tool sets or explicit\ncollaboration directives, OmniEAR requires agents to dynamically acquire\ncapabilities and autonomously determine coordination strategies based on task\ndemands. Through text-based environment representation, we model continuous\nphysical properties and complex spatial relationships across 1,500 scenarios\nspanning household and industrial domains. Our systematic evaluation reveals\nsevere performance degradation when models must reason from constraints: while\nachieving 85-96% success with explicit instructions, performance drops to\n56-85% for tool reasoning and 63-85% for implicit collaboration, with compound\ntasks showing over 50% failure rates. Surprisingly, complete environmental\ninformation degrades coordination performance, indicating models cannot filter\ntask-relevant constraints. Fine-tuning improves single-agent tasks dramatically\n(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing\nfundamental architectural limitations. These findings demonstrate that embodied\nreasoning poses fundamentally different challenges than current models can\naddress, establishing OmniEAR as a rigorous benchmark for evaluating and\nadvancing embodied AI systems. Our code and data are included in the\nsupplementary materials and will be open-sourced upon acceptance.",
      "authors": [
        "Zixuan Wang",
        "Dingming Li",
        "Hongxing Li",
        "Shuo Chen",
        "Yuchen Yan",
        "Wenqi Zhang",
        "Yongliang Shen",
        "Weiming Lu",
        "Jun Xiao",
        "Yueting Zhuang"
      ],
      "published": "2025-08-07T17:54:15+00:00",
      "updated": "2025-08-07T17:54:15+00:00",
      "arxiv_id": "2508.05614v1",
      "url": "http://arxiv.org/pdf/2508.05614v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models",
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL.",
      "authors": [
        "Haitao Hong",
        "Yuchen Yan",
        "Xingyu Wu",
        "Guiyang Hou",
        "Wenqi Zhang",
        "Weiming Lu",
        "Yongliang Shen",
        "Jun Xiao"
      ],
      "published": "2025-08-07T17:53:56+00:00",
      "updated": "2025-08-07T17:53:56+00:00",
      "arxiv_id": "2508.05613v1",
      "url": "http://arxiv.org/pdf/2508.05613v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle",
      "abstract": "Reinforcement learning (RL) has emerged as an effective post-training\nparadigm for enhancing the reasoning capabilities of multimodal large language\nmodel (MLLM). However, current RL pipelines often suffer from training\ninefficiencies caused by two underexplored issues: Advantage Collapsing, where\nmost advantages in a batch concentrate near zero, and Rollout Silencing, where\nthe proportion of rollouts contributing non-zero gradients diminishes over\ntime. These issues lead to suboptimal gradient updates and hinder long-term\nlearning efficiency. To address these issues, we propose Shuffle-R1, a simple\nyet principled framework that improves RL fine-tuning efficiency by dynamically\nrestructuring trajectory sampling and batch composition. It introduces (1)\nPairwise Trajectory Sampling, which selects high-contrast trajectories with\nlarge advantages to improve gradient signal quality, and (2) Advantage-based\nTrajectory Shuffle, which increases exposure of valuable rollouts through\ninformed batch reshuffling. Experiments across multiple reasoning benchmarks\nshow that our framework consistently outperforms strong RL baselines with\nminimal overhead. These results highlight the importance of data-centric\nadaptations for more efficient RL training in MLLM.",
      "authors": [
        "Linghao Zhu",
        "Yiran Guan",
        "Dingkang Liang",
        "Jianzhong Ju",
        "Zhenbo Luo",
        "Bin Qin",
        "Jian Luan",
        "Yuliang Liu",
        "Xiang Bai"
      ],
      "published": "2025-08-07T17:53:47+00:00",
      "updated": "2025-08-07T17:53:47+00:00",
      "arxiv_id": "2508.05612v1",
      "url": "http://arxiv.org/pdf/2508.05612v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization",
      "abstract": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.",
      "authors": [
        "Yuhang Liu",
        "Zeyu Liu",
        "Shuanghe Zhu",
        "Pengxiang Li",
        "Congkai Xie",
        "Jiasheng Wang",
        "Xueyu Hu",
        "Xiaotian Han",
        "Jianbo Yuan",
        "Xinyao Wang",
        "Shengyu Zhang",
        "Hongxia Yang",
        "Fei Wu"
      ],
      "published": "2025-08-07T17:49:56+00:00",
      "updated": "2025-08-07T17:49:56+00:00",
      "arxiv_id": "2508.05731v1",
      "url": "http://arxiv.org/pdf/2508.05731v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision",
      "abstract": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/",
      "authors": [
        "Luozheng Qin",
        "Jia Gong",
        "Yuqing Sun",
        "Tianjiao Li",
        "Mengping Yang",
        "Xiaomeng Yang",
        "Chao Qu",
        "Zhiyu Tan",
        "Hao Li"
      ],
      "published": "2025-08-07T17:45:17+00:00",
      "updated": "2025-08-07T17:45:17+00:00",
      "arxiv_id": "2508.05606v1",
      "url": "http://arxiv.org/pdf/2508.05606v1",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model",
      "abstract": "Multimodal generative AI usually involves generating image or text responses\ngiven inputs in another modality. The evaluation of image-text relevancy is\nessential for measuring response quality or ranking candidate responses. In\nparticular, binary relevancy evaluation, i.e., ``Relevant'' vs. ``Not\nRelevant'', is a fundamental problem. However, this is a challenging task\nconsidering that texts have diverse formats and the definition of relevancy\nvaries in different scenarios. We find that Multimodal Large Language Models\n(MLLMs) are an ideal choice to build such evaluators, as they can flexibly\nhandle complex text formats and take in additional task information. In this\npaper, we present LLaVA-RE, a first attempt for binary image-text relevancy\nevaluation with MLLM. It follows the LLaVA architecture and adopts detailed\ntask instructions and multimodal in-context samples. In addition, we propose a\nnovel binary relevancy data set that covers various tasks. Experimental results\nvalidate the effectiveness of our framework.",
      "authors": [
        "Tao Sun",
        "Oliver Liu",
        "JinJin Li",
        "Lan Ma"
      ],
      "published": "2025-08-07T17:42:44+00:00",
      "updated": "2025-08-07T17:42:44+00:00",
      "arxiv_id": "2508.05602v1",
      "url": "http://arxiv.org/pdf/2508.05602v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "CLAPP: The CLASS LLM Agent for Pair Programming",
      "abstract": "We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI\nassistant designed to support researchers working with the Einstein-Boltzmann\nsolver CLASS. CLAPP leverages large language models (LLMs) and domain-specific\nretrieval to provide conversational coding support for CLASS-answering\nquestions, generating code, debugging errors, and producing plots. Its\narchitecture combines multi-agent LLM orchestration, semantic search across\nCLASS documentation, and a live Python execution environment. Deployed as a\nuser-friendly web application, CLAPP lowers the entry barrier for scientists\nunfamiliar with AI tools and enables more productive human-AI collaboration in\ncomputational and numerical cosmology. The app is available at\nhttps://classclapp.streamlit.app",
      "authors": [
        "Santiago Casas",
        "Christian Fidler",
        "Boris Bolliet",
        "Francisco Villaescusa-Navarro",
        "Julien Lesgourgues"
      ],
      "published": "2025-08-07T17:35:06+00:00",
      "updated": "2025-08-07T17:35:06+00:00",
      "arxiv_id": "2508.05728v1",
      "url": "http://arxiv.org/pdf/2508.05728v1",
      "categories": [
        "astro-ph.IM",
        "astro-ph.CO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "astro-ph.IM"
    },
    {
      "title": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy",
      "abstract": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities.",
      "authors": [
        "Shaoxiong Zhan",
        "Yanlin Lai",
        "Ziyu Lu",
        "Dahua Lin",
        "Ziqing Yang",
        "Fei Tang"
      ],
      "published": "2025-08-07T17:32:14+00:00",
      "updated": "2025-08-07T17:32:14+00:00",
      "arxiv_id": "2508.05592v1",
      "url": "http://arxiv.org/pdf/2508.05592v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition",
      "abstract": "Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple\nseen and unseen object categories within an image, requiring both precise\nintra-class localization to pinpoint objects and effective inter-class\nreasoning to model complex category dependencies. While Vision-Language\nPre-training (VLP) models offer a strong open-vocabulary foundation, they often\nstruggle with fine-grained localization under weak supervision and typically\nfail to explicitly leverage structured relational knowledge beyond basic\nsemantics, limiting performance especially for unseen classes. To overcome\nthese limitations, we propose the Dual Adaptive Refinement Transfer (DART)\nframework. DART enhances a frozen VLP backbone via two synergistic adaptive\nmodules. For intra-class refinement, an Adaptive Refinement Module (ARM)\nrefines patch features adaptively, coupled with a novel Weakly Supervised Patch\nSelecting (WPS) loss that enables discriminative localization using only\nimage-level labels. Concurrently, for inter-class transfer, an Adaptive\nTransfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed\nusing structured knowledge mined from a Large Language Model (LLM), and employs\ngraph attention network to adaptively transfer relational information between\nclass representations. DART is the first framework, to our knowledge, to\nexplicitly integrate external LLM-derived relational knowledge for adaptive\ninter-class transfer while simultaneously performing adaptive intra-class\nrefinement under weak supervision for OV-MLR. Extensive experiments on\nchallenging benchmarks demonstrate that our DART achieves new state-of-the-art\nperformance, validating its effectiveness.",
      "authors": [
        "Haijing Liu",
        "Tao Pu",
        "Hefeng Wu",
        "Keze Wang",
        "Liang Lin"
      ],
      "published": "2025-08-07T17:22:33+00:00",
      "updated": "2025-08-07T17:22:33+00:00",
      "arxiv_id": "2508.05585v1",
      "url": "http://arxiv.org/pdf/2508.05585v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models",
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities for\nmedical question answering and programming, but their potential for generating\ninterpretable computable phenotypes (CPs) is under-explored. In this work, we\ninvestigate whether LLMs can generate accurate and concise CPs for six clinical\nphenotypes of varying complexity, which could be leveraged to enable scalable\nclinical decision support to improve care for patients with hypertension. In\naddition to evaluating zero-short performance, we propose and test a\nsynthesize, execute, debug, instruct strategy that uses LLMs to generate and\niteratively refine CPs using data-driven feedback. Our results show that LLMs,\ncoupled with iterative learning, can generate interpretable and reasonably\naccurate programs that approach the performance of state-of-the-art ML methods\nwhile requiring significantly fewer training examples.",
      "authors": [
        "Guilherme Seidyo Imai Aldeia",
        "Daniel S. Herman",
        "William G. La Cava"
      ],
      "published": "2025-08-07T17:15:17+00:00",
      "updated": "2025-08-07T17:15:17+00:00",
      "arxiv_id": "2508.05581v1",
      "url": "http://arxiv.org/pdf/2508.05581v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis",
      "abstract": "With the growing demands of AI-generated content (AIGC), the need for\nhigh-quality, diverse, and scalable data has become increasingly crucial.\nHowever, collecting large-scale real-world data remains costly and\ntime-consuming, hindering the development of downstream applications. While\nsome works attempt to collect task-specific data via a rendering process, most\napproaches still rely on manual scene construction, limiting their scalability\nand accuracy. To address these challenges, we propose Follow-Your-Instruction,\na Multimodal Large Language Model (MLLM)-driven framework for automatically\nsynthesizing high-quality 2D, 3D, and 4D data. Our\n\\textbf{Follow-Your-Instruction} first collects assets and their associated\ndescriptions through multimodal inputs using the MLLM-Collector. Then it\nconstructs 3D layouts, and leverages Vision-Language Models (VLMs) for semantic\nrefinement through multi-view scenes with the MLLM-Generator and\nMLLM-Optimizer, respectively. Finally, it uses MLLM-Planner to generate\ntemporally coherent future frames. We evaluate the quality of the generated\ndata through comprehensive experiments on the 2D, 3D, and 4D generative tasks.\nThe results show that our synthetic data significantly boosts the performance\nof existing baseline models, demonstrating Follow-Your-Instruction's potential\nas a scalable and effective data engine for generative intelligence.",
      "authors": [
        "Kunyu Feng",
        "Yue Ma",
        "Xinhua Zhang",
        "Boshi Liu",
        "Yikuang Yuluo",
        "Yinhan Zhang",
        "Runtao Liu",
        "Hongyu Liu",
        "Zhiyuan Qin",
        "Shanhui Mo",
        "Qifeng Chen",
        "Zeyu Wang"
      ],
      "published": "2025-08-07T17:12:54+00:00",
      "updated": "2025-08-07T17:12:54+00:00",
      "arxiv_id": "2508.05580v1",
      "url": "http://arxiv.org/pdf/2508.05580v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Fairy$\\pm i$: the First 2-bit Complex LLM with All Parameters in $\\{\\pm1, \\pm i\\}$",
      "abstract": "Quantization-Aware Training (QAT) integrates quantization into the training\nloop, enabling LLMs to learn robust low-bit representations, and is widely\nrecognized as one of the most promising research directions. All current QAT\nresearch focuses on minimizing quantization error on full-precision models,\nwhere the full-precision accuracy acts as an upper bound (accuracy ceiling). No\nexisting method has even attempted to surpass this ceiling. To break this\nceiling, we propose a new paradigm: raising the ceiling (full-precision model),\nand then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$,\nthe first 2-bit quantization framework for complex-valued LLMs. Specifically,\nour method leverages the representational advantages of the complex domain to\nboost full-precision accuracy. We map weights to the fourth roots of unity\n$\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically\noptimal 2-bit representation. Importantly, each quantized weight has either a\nzero real or imaginary part, enabling multiplication-free inference using only\nadditions and element swaps. Experimental results show that Fairy$\\pm i$\noutperforms the ceiling of existing 2-bit quantization approaches in terms of\nboth PPL and downstream tasks, while maintaining strict storage and compute\nefficiency. This work opens a new direction for building highly accurate and\npractical LLMs under extremely low-bit constraints.",
      "authors": [
        "Feiyu Wang",
        "Guoan Wang",
        "Yihao Zhang",
        "Shengfan Wang",
        "Weitao Li",
        "Bokai Huang",
        "Shimao Chen",
        "Zihan Jiang",
        "Rui Xu",
        "Tong Yang"
      ],
      "published": "2025-08-07T17:02:23+00:00",
      "updated": "2025-08-07T17:02:23+00:00",
      "arxiv_id": "2508.05571v1",
      "url": "http://arxiv.org/pdf/2508.05571v1",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs",
      "abstract": "Public opinion surveys show cross-cultural differences in political opinions\nbetween socio-cultural contexts. However, there is no clear evidence whether\nthese differences translate to cross-lingual differences in multilingual large\nlanguage models (MLLMs). We analyze whether opinions transfer between languages\nor whether there are separate opinions for each language in MLLMs of various\nsizes across five Western languages. We evaluate MLLMs' opinions by prompting\nthem to report their (dis)agreement with political statements from voting\nadvice applications. To better understand the interaction between languages in\nthe models, we evaluate them both before and after aligning them with more left\nor right views using direct preference optimization and English alignment data\nonly. Our findings reveal that unaligned models show only very few significant\ncross-lingual differences in the political opinions they reflect. The political\nalignment shifts opinions almost uniformly across all five languages. We\nconclude that in Western language contexts, political opinions transfer between\nlanguages, demonstrating the challenges in achieving explicit socio-linguistic,\ncultural, and political alignment of MLLMs.",
      "authors": [
        "Franziska Weeber",
        "Tanise Ceron",
        "Sebastian PadÃ³"
      ],
      "published": "2025-08-07T16:33:45+00:00",
      "updated": "2025-08-07T16:33:45+00:00",
      "arxiv_id": "2508.05553v1",
      "url": "http://arxiv.org/pdf/2508.05553v1",
      "categories": [
        "cs.CL",
        "cs.CY",
        "I.2.7; J.4"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees",
      "abstract": "Large Language Models (LLMs) have shown remarkable progress in\nmultiple-choice question answering (MCQA), but their inherent unreliability,\nsuch as hallucination and overconfidence, limits their application in high-risk\ndomains. To address this, we propose a frequency-based uncertainty\nquantification method under black-box settings, leveraging conformal prediction\n(CP) to ensure provable coverage guarantees. Our approach involves multiple\nindependent samplings of the model's output distribution for each input, with\nthe most frequent sample serving as a reference to calculate predictive entropy\n(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,\nMedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms\nlogit-based PE in distinguishing between correct and incorrect predictions, as\nmeasured by AUROC. Furthermore, the method effectively controls the empirical\nmiscoverage rate under user-specified risk levels, validating that sampling\nfrequency can serve as a viable substitute for logit-based probabilities in\nblack-box scenarios. This work provides a distribution-free model-agnostic\nframework for reliable uncertainty quantification in MCQA with guaranteed\ncoverage, enhancing the trustworthiness of LLMs in practical applications.",
      "authors": [
        "Guang Yang",
        "Xinyang Liu"
      ],
      "published": "2025-08-07T16:22:49+00:00",
      "updated": "2025-08-07T16:22:49+00:00",
      "arxiv_id": "2508.05544v1",
      "url": "http://arxiv.org/pdf/2508.05544v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction",
      "abstract": "Redacting Personally Identifiable Information (PII) from unstructured text is\ncritical for ensuring data privacy in regulated domains. While earlier\napproaches have relied on rule-based systems and domain-specific Named Entity\nRecognition (NER) models, these methods fail to generalize across formats and\ncontexts. Recent advances in Large Language Models (LLMs) offer a promising\nalternative, yet the effect of architectural and training choices on redaction\nperformance remains underexplored. LLMs have demonstrated strong performance in\ntasks that require contextual language understanding, including the redaction\nof PII in free-form text. Prior work suggests that with appropriate adaptation,\nLLMs can become effective contextual privacy learners. However, the\nconsequences of architectural and training choices for PII Redaction remain\nunderexplored. In this work, we present a comprehensive analysis of LLMs as\nprivacy-preserving PII Redaction systems. We evaluate a range of LLM\narchitectures and training strategies for their effectiveness in PII Redaction.\nOur analysis measures redaction performance, semantic preservation, and PII\nleakage, and compares these outcomes against latency and computational cost.\nThe results provide practical guidance for configuring LLM-based redactors that\nare accurate, efficient, and privacy-aware. To support reproducibility and\nreal-world deployment, we release PRvL, an open-source suite of fine-tuned\nmodels, and evaluation tools for general-purpose PII Redaction. PRvL is built\nentirely on open-source LLMs and supports multiple inference settings for\nflexibility and compliance. It is designed to be easily customized for\ndifferent domains and fully operable within secure, self-managed environments.\nThis enables data owners to perform redactions without relying on third-party\nservices or exposing sensitive content beyond their own infrastructure.",
      "authors": [
        "Leon Garza",
        "Anantaa Kotal",
        "Aritran Piplai",
        "Lavanya Elluri",
        "Prajit Das",
        "Aman Chadha"
      ],
      "published": "2025-08-07T16:22:49+00:00",
      "updated": "2025-08-07T16:22:49+00:00",
      "arxiv_id": "2508.05545v1",
      "url": "http://arxiv.org/pdf/2508.05545v1",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation",
      "abstract": "Effective robotic systems for long-horizon human-robot collaboration must\nadapt to a wide range of human partners, whose physical behavior, willingness\nto assist, and understanding of the robot's capabilities may change over time.\nThis demands a tightly coupled communication loop that grants both agents the\nflexibility to propose, accept, or decline requests as they coordinate toward\ncompleting the task effectively. We apply a Mixed-Initiative dialog paradigm to\nCollaborative human-roBot teaming and propose MICoBot, a system that handles\nthe common scenario where both agents, using natural language, take initiative\nin formulating, accepting, or rejecting proposals on who can best complete\ndifferent steps of a task. To handle diverse, task-directed dialog, and find\nsuccessful collaborative strategies that minimize human effort, MICoBot makes\ndecisions at three levels: (1) a meta-planner considers human dialog to\nformulate and code a high-level collaboration strategy, (2) a planner optimally\nallocates the remaining steps to either agent based on the robot's capabilities\n(measured by a simulation-pretrained affordance model) and the human's\nestimated availability to help, and (3) an action executor decides the\nlow-level actions to perform or words to say to the human. Our extensive\nevaluations in simulation and real-world -- on a physical robot with 18 unique\nhuman participants over 27 hours -- demonstrate the ability of our method to\neffectively collaborate with diverse human users, yielding significantly\nimproved task success and user experience than a pure LLM baseline and other\nagent allocation models. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/MicoBot/.",
      "authors": [
        "Albert Yu",
        "Chengshu Li",
        "Luca Macesanu",
        "Arnav Balaji",
        "Ruchira Ray",
        "Raymond Mooney",
        "Roberto MartÃ­n-MartÃ­n"
      ],
      "published": "2025-08-07T16:09:12+00:00",
      "updated": "2025-08-07T16:09:12+00:00",
      "arxiv_id": "2508.05535v1",
      "url": "http://arxiv.org/pdf/2508.05535v1",
      "categories": [
        "cs.RO",
        "cs.CL",
        "cs.HC",
        "cs.LG",
        "cs.MA",
        "I.2.9; I.2.7; I.2.6"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation",
      "abstract": "Due to their ability to process long and complex contexts, LLMs can offer key\nbenefits to the Legal domain, but their adoption has been hindered by their\ntendency to generate unfaithful, ungrounded, or hallucinatory outputs. While\nRetrieval-Augmented Generation offers a promising solution by grounding\ngenerations in external knowledge, it offers no guarantee that the provided\ncontext will be effectively integrated. To address this, context-aware decoding\nstrategies have been proposed to amplify the influence of relevant context, but\nthey usually do not explicitly enforce faithfulness to the context. In this\nwork, we introduce Confidence-guided Copy-based Decoding for Legal Text\nGeneration (CoCoLex)-a decoding strategy that dynamically interpolates the\nmodel produced vocabulary distribution with a distribution derived based on\ncopying from the context. CoCoLex encourages direct copying based on the\nmodel's confidence, ensuring greater fidelity to the source. Experimental\nresults on five legal benchmarks demonstrate that CoCoLex outperforms existing\ncontext-aware decoding methods, particularly in long-form generation tasks.",
      "authors": [
        "Santosh T. Y. S. S",
        "Youssef Tarek Elkhayat",
        "Oana Ichim",
        "Pranav Shetty",
        "Dongsheng Wang",
        "Zhiqiang Ma",
        "Armineh Nourbakhsh",
        "Xiaomo Liu"
      ],
      "published": "2025-08-07T16:06:58+00:00",
      "updated": "2025-08-07T16:06:58+00:00",
      "arxiv_id": "2508.05534v1",
      "url": "http://arxiv.org/pdf/2508.05534v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety",
      "abstract": "As the volume of video content online grows exponentially, the demand for\nmoderation of unsafe videos has surpassed human capabilities, posing both\noperational and mental health challenges. While recent studies demonstrated the\nmerits of Multimodal Large Language Models (MLLMs) in various video\nunderstanding tasks, their application to multimodal content moderation, a\ndomain that requires nuanced understanding of both visual and textual cues,\nremains relatively underexplored. In this work, we benchmark the capabilities\nof MLLMs in brand safety classification, a critical subset of content\nmoderation for safe-guarding advertising integrity. To this end, we introduce a\nnovel, multimodal and multilingual dataset, meticulously labeled by\nprofessional reviewers in a multitude of risk categories. Through a detailed\ncomparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini,\nGPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost\nefficiency compared to professional human reviewers. Furthermore, we present an\nin-depth discussion shedding light on limitations of MLLMs and failure cases.\nWe are releasing our dataset alongside this paper to facilitate future research\non effective and responsible brand safety and content moderation.",
      "authors": [
        "Adi Levi",
        "Or Levi",
        "Sardhendu Mishra",
        "Jonathan Morra"
      ],
      "published": "2025-08-07T15:55:46+00:00",
      "updated": "2025-08-07T15:55:46+00:00",
      "arxiv_id": "2508.05527v1",
      "url": "http://arxiv.org/pdf/2508.05527v1",
      "categories": [
        "cs.CV",
        "I.2.10; I.2.7; H.3.3; H.4.3; K.4.1"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities",
      "abstract": "Large Language Models (LLMs) have been extensively tuned to mitigate explicit\nbiases, yet they often exhibit subtle implicit biases rooted in their\npre-training data. Rather than directly probing LLMs with human-crafted\nquestions that may trigger guardrails, we propose studying how models behave\nwhen they proactively ask questions themselves. The 20 Questions game, a\nmulti-turn deduction task, serves as an ideal testbed for this purpose. We\nsystematically evaluate geographic performance disparities in entity deduction\nusing a new dataset, Geo20Q+, consisting of both notable people and culturally\nsignificant objects (e.g., foods, landmarks, animals) from diverse regions. We\ntest popular LLMs across two gameplay configurations (canonical 20-question and\nunlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,\nFrench, Spanish, and Turkish). Our results reveal geographic disparities: LLMs\nare substantially more successful at deducing entities from the Global North\nthan the Global South, and the Global West than the Global East. While\nWikipedia pageviews and pre-training corpus frequency correlate mildly with\nperformance, they fail to fully explain these disparities. Notably, the\nlanguage in which the game is played has minimal impact on performance gaps.\nThese findings demonstrate the value of creative, free-form evaluation\nframeworks for uncovering subtle biases in LLMs that remain hidden in standard\nprompting setups. By analyzing how models initiate and pursue reasoning goals\nover multiple turns, we find geographic and cultural disparities embedded in\ntheir reasoning processes. We release the dataset (Geo20Q+) and code at\nhttps://sites.google.com/view/llmbias20q/home.",
      "authors": [
        "Harsh Nishant Lalai",
        "Raj Sanjay Shah",
        "Jiaxin Pei",
        "Sashank Varma",
        "Yi-Chia Wang",
        "Ali Emami"
      ],
      "published": "2025-08-07T15:53:30+00:00",
      "updated": "2025-08-07T15:53:30+00:00",
      "arxiv_id": "2508.05525v1",
      "url": "http://arxiv.org/pdf/2508.05525v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods",
      "abstract": "Clinical trial data cleaning represents a critical bottleneck in drug\ndevelopment, with manual review processes struggling to manage exponentially\nincreasing data volumes and complexity. This paper presents Octozi, an\nartificial intelligence-assisted platform that combines large language models\nwith domain-specific heuristics to transform clinical data review. In a\ncontrolled experimental study with experienced clinical reviewers (n=10), we\ndemonstrate that AI assistance increased data cleaning throughput by 6.03-fold\nwhile simultaneously decreasing cleaning errors from 54.67% to 8.48% (a\n6.44-fold improvement). Crucially, the system reduced false positive queries by\n15.48-fold, minimizing unnecessary site burden. These improvements were\nconsistent across reviewers regardless of experience level, suggesting broad\napplicability. Our findings indicate that AI-assisted approaches can address\nfundamental inefficiencies in clinical trial operations, potentially\naccelerating drug development timelines and reducing costs while maintaining\nregulatory compliance. This work establishes a framework for integrating AI\ninto safety-critical clinical workflows and demonstrates the transformative\npotential of human-AI collaboration in pharmaceutical clinical trials.",
      "authors": [
        "Matthew Purri",
        "Amit Patel",
        "Erik Deurrell"
      ],
      "published": "2025-08-07T15:49:32+00:00",
      "updated": "2025-08-07T15:49:32+00:00",
      "arxiv_id": "2508.05519v1",
      "url": "http://arxiv.org/pdf/2508.05519v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master's Program",
      "abstract": "Letters of recommendation (LORs) provide valuable insights into candidates'\ncapabilities and experiences beyond standardized test scores. However,\nreviewing these text-heavy materials is time-consuming and labor-intensive. To\naddress this challenge and support the admission committee in providing\nfeedback for students' professional growth, our study introduces LORI: LOR\nInsights, a novel AI-based detection tool for assessing leadership skills in\nLORs submitted by online master's program applicants. By employing natural\nlanguage processing and leveraging large language models using RoBERTa and\nLLAMA, we seek to identify leadership attributes such as teamwork,\ncommunication, and innovation. Our latest RoBERTa model achieves a weighted F1\nscore of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong\nlevel of consistency in our test data. With the growing importance of\nleadership skills in the STEM sector, integrating LORI into the graduate\nadmissions process is crucial for accurately assessing applicants' leadership\ncapabilities. This approach not only streamlines the admissions process but\nalso automates and ensures a more comprehensive evaluation of candidates'\ncapabilities.",
      "authors": [
        "Meryem Yilmaz Soylu",
        "Adrian Gallard",
        "Jeonghyun Lee",
        "Gayane Grigoryan",
        "Rushil Desai",
        "Stephen Harmon"
      ],
      "published": "2025-08-07T15:46:59+00:00",
      "updated": "2025-08-07T15:46:59+00:00",
      "arxiv_id": "2508.05513v1",
      "url": "http://arxiv.org/pdf/2508.05513v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "RankArena: A Unified Platform for Evaluating Retrieval, Reranking and RAG with Human and LLM Feedback",
      "abstract": "Evaluating the quality of retrieval-augmented generation (RAG) and document\nreranking systems remains challenging due to the lack of scalable,\nuser-centric, and multi-perspective evaluation tools. We introduce RankArena, a\nunified platform for comparing and analysing the performance of retrieval\npipelines, rerankers, and RAG systems using structured human and LLM-based\nfeedback as well as for collecting such feedback. RankArena supports multiple\nevaluation modes: direct reranking visualisation, blind pairwise comparisons\nwith human or LLM voting, supervised manual document annotation, and end-to-end\nRAG answer quality assessment. It captures fine-grained relevance feedback\nthrough both pairwise preferences and full-list annotations, along with\nauxiliary metadata such as movement metrics, annotation time, and quality\nratings. The platform also integrates LLM-as-a-judge evaluation, enabling\ncomparison between model-generated rankings and human ground truth annotations.\nAll interactions are stored as structured evaluation datasets that can be used\nto train rerankers, reward models, judgment agents, or retrieval strategy\nselectors. Our platform is publicly available at https://rankarena.ngrok.io/,\nand the Demo video is provided https://youtu.be/jIYAP4PaSSI.",
      "authors": [
        "Abdelrahman Abdallah",
        "Mahmoud Abdalla",
        "Bhawna Piryani",
        "Jamshid Mozafari",
        "Mohammed Ali",
        "Adam Jatowt"
      ],
      "published": "2025-08-07T15:46:53+00:00",
      "updated": "2025-08-07T15:46:53+00:00",
      "arxiv_id": "2508.05512v1",
      "url": "http://arxiv.org/pdf/2508.05512v1",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "LAG: Logic-Augmented Generation from a Cartesian Perspective",
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet exhibit critical limitations in knowledge-intensive\ntasks, often generating hallucinations when faced with questions requiring\nspecialized expertise. While retrieval-augmented generation (RAG) mitigates\nthis by integrating external knowledge, it struggles with complex reasoning\nscenarios due to its reliance on direct semantic retrieval and lack of\nstructured logical organization. Inspired by Cartesian principles from\n\\textit{Discours de la m\\'ethode}, this paper introduces Logic-Augmented\nGeneration (LAG), a novel paradigm that reframes knowledge augmentation through\nsystematic question decomposition and dependency-aware reasoning. Specifically,\nLAG first decomposes complex questions into atomic sub-questions ordered by\nlogical dependencies. It then resolves these sequentially, using prior answers\nto guide context retrieval for subsequent sub-questions, ensuring stepwise\ngrounding in logical chain. To prevent error propagation, LAG incorporates a\nlogical termination mechanism that halts inference upon encountering\nunanswerable sub-questions and reduces wasted computation on excessive\nreasoning. Finally, it synthesizes all sub-resolutions to generate verified\nresponses. Experiments on four benchmark datasets demonstrate that LAG\nsignificantly enhances reasoning robustness, reduces hallucination, and aligns\nLLM problem-solving with human cognition, offering a principled alternative to\nexisting RAG systems.",
      "authors": [
        "Yilin Xiao",
        "Chuang Zhou",
        "Qinggang Zhang",
        "Su Dong",
        "Shengyuan Chen",
        "Xiao Huang"
      ],
      "published": "2025-08-07T15:42:00+00:00",
      "updated": "2025-08-07T15:42:00+00:00",
      "arxiv_id": "2508.05509v1",
      "url": "http://arxiv.org/pdf/2508.05509v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation",
      "abstract": "The increasing adoption of foundation models as agents across diverse domains\nnecessitates a robust evaluation framework. Current methods, such as\nLLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step\nreasoning that drives agentic decision-making. Meanwhile, existing\nAgent-as-a-Judge systems, where one agent evaluates another's task completion,\nare typically designed for narrow, domain-specific settings. To address this\ngap, we propose a generalizable, modular framework for evaluating agent task\ncompletion independent of the task domain. The framework emulates human-like\nevaluation by decomposing tasks into sub-tasks and validating each step using\navailable information, such as the agent's output and reasoning. Each module\ncontributes to a specific aspect of the evaluation process, and their outputs\nare aggregated to produce a final verdict on task completion. We validate our\nframework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA\nand BigCodeBench. Our Judge Agent predicts task success with closer agreement\nto human evaluations, achieving 4.76% and 10.52% higher alignment accuracy,\nrespectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This\ndemonstrates the potential of our proposed general-purpose evaluation\nframework.",
      "authors": [
        "Roshita Bhonsle",
        "Rishav Dutta",
        "Sneha Vavilapalli",
        "Harsh Seth",
        "Abubakarr Jaye",
        "Yapei Chang",
        "Mukund Rungta",
        "Emmanuel Aboah Boateng",
        "Sadid Hasan",
        "Ehi Nosakhare",
        "Soundar Srinivasan"
      ],
      "published": "2025-08-07T15:39:48+00:00",
      "updated": "2025-08-07T15:39:48+00:00",
      "arxiv_id": "2508.05508v1",
      "url": "http://arxiv.org/pdf/2508.05508v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated Industrial Anomaly Detection",
      "abstract": "Industrial anomaly detection (IAD) is critical for manufacturing quality\ncontrol, but conventionally requires significant manual effort for various\napplication scenarios. This paper introduces AutoIAD, a multi-agent\ncollaboration framework, specifically designed for end-to-end automated\ndevelopment of industrial visual anomaly detection. AutoIAD leverages a\nManager-Driven central agent to orchestrate specialized sub-agents (including\nData Preparation, Data Loader, Model Designer, Trainer) and integrates a\ndomain-specific knowledge base, which intelligently handles the entire pipeline\nusing raw industrial image data to develop a trained anomaly detection model.\nWe construct a comprehensive benchmark using MVTec AD datasets to evaluate\nAutoIAD across various LLM backends. Extensive experiments demonstrate that\nAutoIAD significantly outperforms existing general-purpose agentic\ncollaboration frameworks and traditional AutoML frameworks in task completion\nrate and model performance (AUROC), while effectively mitigating issues like\nhallucination through iterative refinement. Ablation studies further confirm\nthe crucial roles of the Manager central agent and the domain knowledge base\nmodule in producing robust and high-quality IAD solutions.",
      "authors": [
        "Dongwei Ji",
        "Bingzhang Hu",
        "Yi Zhou"
      ],
      "published": "2025-08-07T15:36:38+00:00",
      "updated": "2025-08-07T15:36:38+00:00",
      "arxiv_id": "2508.05503v1",
      "url": "http://arxiv.org/pdf/2508.05503v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs",
      "abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable performance in\nhigh-resource languages. However, their effectiveness diminishes significantly\nin the contexts of low-resource languages. Current multilingual enhancement\nmethods are often limited to text modality or rely solely on machine\ntranslation. While such approaches help models acquire basic linguistic\ncapabilities and produce \"thin descriptions\", they neglect the importance of\nmultimodal informativeness and cultural groundedness, both of which are crucial\nfor serving low-resource language users effectively. To bridge this gap, in\nthis study, we identify two significant objectives for a truly effective MLLM\nin low-resource language settings, namely 1) linguistic capability and 2)\ncultural groundedness, placing special emphasis on cultural awareness. To\nachieve these dual objectives, we propose a dual-source strategy that guides\nthe collection of data tailored to each goal, sourcing native web alt-text for\nculture and MLLM-generated captions for linguistics. As a concrete\nimplementation, we introduce MELLA, a multimodal, multilingual dataset.\nExperiment results show that after fine-tuning on MELLA, there is a general\nperformance improvement for the eight languages on various MLLM backbones, with\nmodels producing \"thick descriptions\". We verify that the performance gains are\nfrom both cultural knowledge enhancement and linguistic capability enhancement.\nOur dataset can be found at https://opendatalab.com/applyMultilingualCorpus.",
      "authors": [
        "Yufei Gao",
        "Jiaying Fei",
        "Nuo Chen",
        "Ruirui Chen",
        "Guohang Yan",
        "Yunshi Lan",
        "Botian Shi"
      ],
      "published": "2025-08-07T15:36:24+00:00",
      "updated": "2025-08-07T15:36:24+00:00",
      "arxiv_id": "2508.05502v1",
      "url": "http://arxiv.org/pdf/2508.05502v1",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning",
      "abstract": "Large Language Models (LLMs) integrated with Retrieval-Augmented Generation\n(RAG) techniques have exhibited remarkable performance across a wide range of\ndomains. However, existing RAG approaches primarily operate on unstructured\ndata and demonstrate limited capability in handling structured knowledge such\nas knowledge graphs. Meanwhile, current graph retrieval methods fundamentally\nstruggle to capture holistic graph structures while simultaneously facing\nprecision control challenges that manifest as either critical information gaps\nor excessive redundant connections, collectively undermining reasoning\nperformance. To address this challenge, we propose GRAIL: Graph-Retrieval\nAugmented Interactive Learning, a framework designed to interact with\nlarge-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL\nintegrates LLM-guided random exploration with path filtering to establish a\ndata synthesis pipeline, where a fine-grained reasoning trajectory is\nautomatically generated for each task. Based on the synthesized data, we then\nemploy a two-stage training process to learn a policy that dynamically decides\nthe optimal actions at each reasoning step. The overall objective of\nprecision-conciseness balance in graph retrieval is decoupled into fine-grained\nprocess-supervised rewards to enhance data efficiency and training stability.\nIn practical deployment, GRAIL adopts an interactive retrieval paradigm,\nenabling the model to autonomously explore graph paths while dynamically\nbalancing retrieval breadth and precision. Extensive experiments have shown\nthat GRAIL achieves an average accuracy improvement of 21.01% and F1\nimprovement of 22.43% on three knowledge graph question-answering datasets. Our\nsource code and datasets is available at https://github.com/Changgeww/GRAIL.",
      "authors": [
        "Ge Chang",
        "Jinbo Su",
        "Jiacheng Liu",
        "Pengfei Yang",
        "Yuhao Shang",
        "Huiwen Zheng",
        "Hongli Ma",
        "Yan Liang",
        "Yuanchun Li",
        "Yunxin Liu"
      ],
      "published": "2025-08-07T15:34:41+00:00",
      "updated": "2025-08-07T15:34:41+00:00",
      "arxiv_id": "2508.05498v1",
      "url": "http://arxiv.org/pdf/2508.05498v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities",
      "abstract": "Large language models (LLMs) have exhibited impressive reasoning abilities on\na wide range of complex tasks. However, enhancing these capabilities through\npost-training remains resource intensive, particularly in terms of data and\ncomputational cost. Although recent efforts have sought to improve sample\nefficiency through selective data curation, existing methods often rely on\nheuristic or task-specific strategies that hinder scalability. In this work, we\nintroduce InfiAlign, a scalable and sample-efficient post-training framework\nthat integrates supervised fine-tuning (SFT) with Direct Preference\nOptimization (DPO) to align LLMs for enhanced reasoning. At the core of\nInfiAlign is a robust data selection pipeline that automatically curates\nhigh-quality alignment data from open-source reasoning datasets using\nmultidimensional quality metrics. This pipeline enables significant performance\ngains while drastically reducing data requirements and remains extensible to\nnew data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model\nachieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only\napproximately 12% of the training data, and demonstrates strong generalization\nacross diverse reasoning tasks. Additional improvements are obtained through\nthe application of DPO, with particularly notable gains in mathematical\nreasoning tasks. The model achieves an average improvement of 3.89% on AIME\n24/25 benchmarks. Our results highlight the effectiveness of combining\nprincipled data selection with full-stage post-training, offering a practical\nsolution for aligning large reasoning models in a scalable and data-efficient\nmanner. The model checkpoints are available at\nhttps://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.",
      "authors": [
        "Shuo Cai",
        "Su Lu",
        "Qi Zhou",
        "Kejing Yang",
        "Zhijie Sang",
        "Congkai Xie",
        "Hongxia Yang"
      ],
      "published": "2025-08-07T15:34:06+00:00",
      "updated": "2025-08-07T15:34:06+00:00",
      "arxiv_id": "2508.05496v1",
      "url": "http://arxiv.org/pdf/2508.05496v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling",
      "abstract": "Multimodal electronic health record (EHR) data provide richer, complementary\ninsights into patient health compared to single-modality data. However,\neffectively integrating diverse data modalities for clinical prediction\nmodeling remains challenging due to the substantial data requirements. We\nintroduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed\nto leverage multiple large language model (LLM) agents for clinical prediction\ntasks using multimodal EHR data. MoMA employs specialized LLM agents\n(\"specialist agents\") to convert non-textual modalities, such as medical images\nand laboratory results, into structured textual summaries. These summaries,\ntogether with clinical notes, are combined by another LLM (\"aggregator agent\")\nto generate a unified multimodal summary, which is then used by a third LLM\n(\"predictor agent\") to produce clinical predictions. Evaluating MoMA on three\nprediction tasks using real-world datasets with different modality combinations\nand prediction settings, MoMA outperforms current state-of-the-art methods,\nhighlighting its enhanced accuracy and flexibility across various tasks.",
      "authors": [
        "Jifan Gao",
        "Mahmudur Rahman",
        "John Caskey",
        "Madeline Oguss",
        "Ann O'Rourke",
        "Randy Brown",
        "Anne Stey",
        "Anoop Mayampurath",
        "Matthew M. Churpek",
        "Guanhua Chen",
        "Majid Afshar"
      ],
      "published": "2025-08-07T15:28:34+00:00",
      "updated": "2025-08-07T15:28:34+00:00",
      "arxiv_id": "2508.05492v1",
      "url": "http://arxiv.org/pdf/2508.05492v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?",
      "abstract": "Emotion recognition in conversations (ERC) focuses on identifying emotion\nshifts within interactions, representing a significant step toward advancing\nmachine intelligence. However, ERC data remains scarce, and existing datasets\nface numerous challenges due to their highly biased sources and the inherent\nsubjectivity of soft labels. Even though Large Language Models (LLMs) have\ndemonstrated their quality in many affective tasks, they are typically\nexpensive to train, and their application to ERC tasks--particularly in data\ngeneration--remains limited. To address these challenges, we employ a small,\nresource-efficient, and general-purpose LLM to synthesize ERC datasets with\ndiverse properties, supplementing the three most widely used ERC benchmarks. We\ngenerate six novel datasets, with two tailored to enhance each benchmark. We\nevaluate the utility of these datasets to (1) supplement existing datasets for\nERC classification, and (2) analyze the effects of label imbalance in ERC. Our\nexperimental results indicate that ERC classifier models trained on the\ngenerated datasets exhibit strong robustness and consistently achieve\nstatistically significant performance improvements on existing ERC benchmarks.",
      "authors": [
        "Burak Can Kaplan",
        "Hugo Cesar De Castro Carneiro",
        "Stefan Wermter"
      ],
      "published": "2025-08-07T15:13:55+00:00",
      "updated": "2025-08-07T15:13:55+00:00",
      "arxiv_id": "2508.05474v1",
      "url": "http://arxiv.org/pdf/2508.05474v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Embedding Alignment in Code Generation for Audio",
      "abstract": "LLM-powered code generation has the potential to revolutionize creative\ncoding endeavors, such as live-coding, by enabling users to focus on structural\nmotifs over syntactic details. In such domains, when prompting an LLM, users\nmay benefit from considering multiple varied code candidates to better realize\ntheir musical intentions. Code generation models, however, struggle to present\nunique and diverse code candidates, with no direct insight into the code's\naudio output. To better establish a relationship between code candidates and\nproduced audio, we investigate the topology of the mapping between code and\naudio embedding spaces. We find that code and audio embeddings do not exhibit a\nsimple linear relationship, but supplement this with a constructed predictive\nmodel that shows an embedding alignment map could be learned. Supplementing the\naim for musically diverse output, we present a model that given code predicts\noutput audio embedding, constructing a code-audio embedding alignment map.",
      "authors": [
        "Sam Kouteili",
        "Hiren Madhu",
        "George Typaldos",
        "Mark Santolucito"
      ],
      "published": "2025-08-07T15:13:42+00:00",
      "updated": "2025-08-07T15:13:42+00:00",
      "arxiv_id": "2508.05473v1",
      "url": "http://arxiv.org/pdf/2508.05473v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.MM"
    },
    {
      "title": "Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations",
      "abstract": "We systematically examine, analyze, and compare representative creativity\nmeasures--creativity index, perplexity, syntactic templates, and\nLLM-as-a-Judge--across diverse creative domains, including creative writing,\nunconventional problem-solving, and research ideation. Our analyses reveal that\nthese metrics exhibit limited consistency, capturing different dimensions of\ncreativity. We highlight key limitations, including the creativity index's\nfocus on lexical diversity, perplexity's sensitivity to model confidence, and\nsyntactic templates' inability to capture conceptual creativity. Additionally,\nLLM-as-a-Judge shows instability and bias. Our findings underscore the need for\nmore robust, generalizable evaluation frameworks that better align with human\njudgments of creativity.",
      "authors": [
        "Li-Chun Lu",
        "Miri Liu",
        "Pin-Chun Lu",
        "Yufei Tian",
        "Shao-Hua Sun",
        "Nanyun Peng"
      ],
      "published": "2025-08-07T15:11:48+00:00",
      "updated": "2025-08-07T15:11:48+00:00",
      "arxiv_id": "2508.05470v1",
      "url": "http://arxiv.org/pdf/2508.05470v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes",
      "abstract": "We develop mechanisms for evaluating AI systems without ground truth by\nexploiting a connection between gaming resistance and output quality. The data\nprocessing inequality ensures post-hoc attempts to game a metric degrades both\ninformation content and task performance. We prove that f-mutual information\nmeasures are the unique gaming resistant mechanisms under natural conditions,\nwith the overseer acting as an agent. While Shannon mutual information faces\nexponential sample complexity, bounded measures like total variation distance\nremain tractable. Empirically, across ten domains from translation to peer\nreview, all information-theoretic mechanisms achieve perfect discrimination (d\n> 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit\nsystematic evaluation inversion, preferring fabricated content over accurate\nsummaries. Our mechanisms show 10-100x better robustness to adversarial\nmanipulation than current practices. We also find performance follows an\ninverted-U curve with compression ratio, peaking at 10:1 where agent responses\nexhibit optimal information diversity (3 effective dimensions), giving a\nbias-variance perspective on when our approach is expected to be most\neffective.",
      "authors": [
        "Zachary Robertson",
        "Sanmi Koyejo"
      ],
      "published": "2025-08-07T15:11:43+00:00",
      "updated": "2025-08-07T15:11:43+00:00",
      "arxiv_id": "2508.05469v1",
      "url": "http://arxiv.org/pdf/2508.05469v1",
      "categories": [
        "cs.LG",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "TASE: Token Awareness and Structured Evaluation for Multilingual Language Models",
      "abstract": "While large language models (LLMs) have demonstrated remarkable performance\non high-level semantic tasks, they often struggle with fine-grained,\ntoken-level understanding and structural reasoning--capabilities that are\nessential for applications requiring precision and control. We introduce TASE,\na comprehensive benchmark designed to evaluate LLMs' ability to perceive and\nreason about token-level information across languages. TASE covers 10 tasks\nunder two core categories: token awareness and structural understanding,\nspanning Chinese, English, and Korean, with a 35,927-instance evaluation set\nand a scalable synthetic data generation pipeline for training. Tasks include\ncharacter counting, token alignment, syntactic structure parsing, and length\nconstraint satisfaction. We evaluate over 30 leading commercial and open-source\nLLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a\ncustom Qwen2.5-14B model using the GRPO training method. Results show that\nhuman performance significantly outpaces current LLMs, revealing persistent\nweaknesses in token-level reasoning. TASE sheds light on these limitations and\nprovides a new diagnostic lens for future improvements in low-level language\nunderstanding and cross-lingual generalization. Our code and dataset are\npublicly available at https://github.com/cyzcz/Tase .",
      "authors": [
        "Chenzhuo Zhao",
        "Xinda Wang",
        "Yue Huang",
        "Junting Lu",
        "Ziqian Liu"
      ],
      "published": "2025-08-07T15:11:17+00:00",
      "updated": "2025-08-07T15:11:17+00:00",
      "arxiv_id": "2508.05468v1",
      "url": "http://arxiv.org/pdf/2508.05468v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?",
      "abstract": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem is overwhelmingly focused on a narrow set of behavioral propensities,\nsuch as \"Tendency to hallucinate\" (53.7% of the corpus) and \"Discriminatory\nbias\" (28.9%), while critical functional capabilities are dangerously\nneglected. Crucially, capabilities central to loss-of-control scenarios,\nincluding evading human oversight, self-replication, and autonomous AI\ndevelopment, receive zero coverage in the entire benchmark corpus. This\ntranslates to a near-total evaluation gap for systemic risks like \"Loss of\nControl\" (0.4% coverage) and \"Cyber Offence\" (0.8% coverage). This study\nprovides the first comprehensive, quantitative analysis of this gap, offering\ncritical insights for policymakers to refine the CoP and for developers to\nbuild the next generation of evaluation tools, ultimately fostering safer and\nmore compliant AI.",
      "authors": [
        "Matteo Prandi",
        "Vincenzo Suriani",
        "Federico Pierucci",
        "Marcello Galisai",
        "Daniele Nardi",
        "Piercosma Bisconti"
      ],
      "published": "2025-08-07T15:03:39+00:00",
      "updated": "2025-08-07T15:03:39+00:00",
      "arxiv_id": "2508.05464v1",
      "url": "http://arxiv.org/pdf/2508.05464v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?",
      "abstract": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem dedicates the vast majority of its focus to a narrow set of\nbehavioral propensities. On average, benchmarks devote 61.6% of their\nregulatory-relevant questions to \"Tendency to hallucinate\" and 31.2% to \"Lack\nof performance reliability\", while critical functional capabilities are\ndangerously neglected. Crucially, capabilities central to loss-of-control\nscenarios, including evading human oversight, self-replication, and autonomous\nAI development, receive zero coverage in the entire benchmark corpus. This\nstudy provides the first comprehensive, quantitative analysis of this gap,\ndemonstrating that current public benchmarks are insufficient, on their own,\nfor providing the evidence of comprehensive risk assessment required for\nregulatory compliance and offering critical insights for the development of\nnext-generation evaluation tools.",
      "authors": [
        "Matteo Prandi",
        "Vincenzo Suriani",
        "Federico Pierucci",
        "Marcello Galisai",
        "Daniele Nardi",
        "Piercosma Bisconti"
      ],
      "published": "2025-08-07T15:03:39+00:00",
      "updated": "2025-08-08T14:16:34+00:00",
      "arxiv_id": "2508.05464v2",
      "url": "http://arxiv.org/pdf/2508.05464v2",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare",
      "abstract": "This paper introduces PEACH, a sentence-aligned parallel English-Arabic\ncorpus of healthcare texts encompassing patient information leaflets and\neducational materials. The corpus contains 51,671 parallel sentences, totaling\napproximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths\nvary between 9.52 and 11.83 words on average. As a manually aligned corpus,\nPEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,\ntranslation studies, and natural language processing. It can be used to derive\nbilingual lexicons, adapt large language models for domain-specific machine\ntranslation, evaluate user perceptions of machine translation in healthcare,\nassess patient information leaflets and educational materials' readability and\nlay-friendliness, and as an educational resource in translation studies. PEACH\nis publicly accessible.",
      "authors": [
        "Rania Al-Sabbagh"
      ],
      "published": "2025-08-07T14:49:48+00:00",
      "updated": "2025-08-07T14:49:48+00:00",
      "arxiv_id": "2508.05722v1",
      "url": "http://arxiv.org/pdf/2508.05722v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models",
      "abstract": "Existing evaluation of Large Language Models (LLMs) on static benchmarks is\nvulnerable to data contamination and leaderboard overfitting, critical issues\nthat obscure true model capabilities. To address this, we introduce LLMEval-3,\na framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary\nbank of 220k graduate-level questions, from which it dynamically samples unseen\ntest sets for each evaluation run. Its automated pipeline ensures integrity via\ncontamination-resistant data curation, a novel anti-cheating architecture, and\na calibrated LLM-as-a-judge process achieving 90% agreement with human experts,\ncomplemented by a relative ranking system for fair comparison. An 20-month\nlongitudinal study of nearly 50 leading models reveals a performance ceiling on\nknowledge memorization and exposes data contamination vulnerabilities\nundetectable by static benchmarks. The framework demonstrates exceptional\nrobustness in ranking stability and consistency, providing strong empirical\nvalidation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and\ncredible methodology for assessing the true capabilities of LLMs beyond\nleaderboard scores, promoting the development of more trustworthy evaluation\nstandards.",
      "authors": [
        "Ming Zhang",
        "Yujiong Shen",
        "Jingyi Deng",
        "Yuhui Wang",
        "Yue Zhang",
        "Junzhe Wang",
        "Shichun Liu",
        "Shihan Dou",
        "Huayu Sha",
        "Qiyuan Peng",
        "Changhao Jiang",
        "Jingqi Tong",
        "Yilong Wu",
        "Zhihao Zhang",
        "Mingqi Wu",
        "Zhiheng Xi",
        "Mingxu Chai",
        "Tao Liang",
        "Zhihui Fei",
        "Zhen Wang",
        "Mingyang Wan",
        "Guojun Ma",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "published": "2025-08-07T14:46:30+00:00",
      "updated": "2025-08-07T14:46:30+00:00",
      "arxiv_id": "2508.05452v1",
      "url": "http://arxiv.org/pdf/2508.05452v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search",
      "abstract": "Interpretability and high performance are essential goals in designing\ncontrol policies, particularly for safety-critical tasks. Deep reinforcement\nlearning has greatly enhanced performance, yet its inherent lack of\ninterpretability often undermines trust and hinders real-world deployment. This\nwork addresses these dual challenges by introducing a novel approach for\nprogrammatic policy discovery, called Multimodal Large Language Model-assisted\nEvolutionary Search (MLES). MLES utilizes multimodal large language models as\npolicy generators, combining them with evolutionary mechanisms for automatic\npolicy optimization. It integrates visual feedback-driven behavior analysis\nwithin the policy generation process to identify failure patterns and\nfacilitate targeted improvements, enhancing the efficiency of policy discovery\nand producing adaptable, human-aligned policies. Experimental results show that\nMLES achieves policy discovery capabilities and efficiency comparable to\nProximal Policy Optimization (PPO) across two control tasks, while offering\ntransparent control logic and traceable design processes. This paradigm\novercomes the limitations of predefined domain-specific languages, facilitates\nknowledge transfer and reuse, and is scalable across various control tasks.\nMLES shows promise as a leading approach for the next generation of\ninterpretable control policy discovery.",
      "authors": [
        "Qinglong Hu",
        "Xialiang Tong",
        "Mingxuan Yuan",
        "Fei Liu",
        "Zhichao Lu",
        "Qingfu Zhang"
      ],
      "published": "2025-08-07T14:24:03+00:00",
      "updated": "2025-08-07T14:24:03+00:00",
      "arxiv_id": "2508.05433v1",
      "url": "http://arxiv.org/pdf/2508.05433v1",
      "categories": [
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints",
      "abstract": "Large Language Models (LLMs) often exhibit cultural biases due to training\ndata dominated by high-resource languages like English and Chinese. This poses\nchallenges for accurately representing and evaluating diverse cultural\ncontexts, particularly in low-resource language settings. To address this, we\nintroduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on\nMalaysian culture across six pillars: arts, attire, customs, entertainment,\nfood, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,\nMyCulture employs a novel open-ended multiple-choice question format without\npredefined options, thereby reducing guessing and mitigating format bias. We\nprovide a theoretical justification for the effectiveness of this open-ended\nstructure in improving both fairness and discriminative power. Furthermore, we\nanalyze structural bias by comparing model performance on structured versus\nfree-form outputs, and assess language bias through multilingual prompt\nvariations. Our evaluation across a range of regional and international LLMs\nreveals significant disparities in cultural comprehension, highlighting the\nurgent need for culturally grounded and linguistically inclusive benchmarks in\nthe development and assessment of LLMs.",
      "authors": [
        "Zhong Ken Hew",
        "Jia Xin Low",
        "Sze Jue Yang",
        "Chee Seng chan"
      ],
      "published": "2025-08-07T14:17:43+00:00",
      "updated": "2025-08-07T14:17:43+00:00",
      "arxiv_id": "2508.05429v1",
      "url": "http://arxiv.org/pdf/2508.05429v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints",
      "abstract": "Large Language Models (LLMs) often exhibit cultural biases due to training\ndata dominated by high-resource languages like English and Chinese. This poses\nchallenges for accurately representing and evaluating diverse cultural\ncontexts, particularly in low-resource language settings. To address this, we\nintroduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on\nMalaysian culture across six pillars: arts, attire, customs, entertainment,\nfood, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,\nMyCulture employs a novel open-ended multiple-choice question format without\npredefined options, thereby reducing guessing and mitigating format bias. We\nprovide a theoretical justification for the effectiveness of this open-ended\nstructure in improving both fairness and discriminative power. Furthermore, we\nanalyze structural bias by comparing model performance on structured versus\nfree-form outputs, and assess language bias through multilingual prompt\nvariations. Our evaluation across a range of regional and international LLMs\nreveals significant disparities in cultural comprehension, highlighting the\nurgent need for culturally grounded and linguistically inclusive benchmarks in\nthe development and assessment of LLMs.",
      "authors": [
        "Zhong Ken Hew",
        "Jia Xin Low",
        "Sze Jue Yang",
        "Chee Seng Chan"
      ],
      "published": "2025-08-07T14:17:43+00:00",
      "updated": "2025-08-08T01:24:20+00:00",
      "arxiv_id": "2508.05429v2",
      "url": "http://arxiv.org/pdf/2508.05429v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Group Causal Policy Optimization for Post-Training Large Language Models",
      "abstract": "Recent advances in large language models (LLMs) have broadened their\napplicability across diverse tasks, yet specialized domains still require\ntargeted post training. Among existing methods, Group Relative Policy\nOptimization (GRPO) stands out for its efficiency, leveraging groupwise\nrelative rewards while avoiding costly value function learning. However, GRPO\ntreats candidate responses as independent, overlooking semantic interactions\nsuch as complementarity and contradiction. To address this challenge, we first\nintroduce a Structural Causal Model (SCM) that reveals hidden dependencies\namong candidate responses induced by conditioning on a final integrated output\nforming a collider structure. Then, our causal analysis leads to two insights:\n(1) projecting responses onto a causally informed subspace improves prediction\nquality, and (2) this projection yields a better baseline than query only\nconditioning. Building on these insights, we propose Group Causal Policy\nOptimization (GCPO), which integrates causal structure into optimization\nthrough two key components: a causally informed reward adjustment and a novel\nKL regularization term that aligns the policy with a causally projected\nreference distribution. Comprehensive experimental evaluations demonstrate that\nGCPO consistently surpasses existing methods, including GRPO across multiple\nreasoning benchmarks.",
      "authors": [
        "Ziyin Gu",
        "Jingyao Wang",
        "Ran Zuo",
        "Chuxiong Sun",
        "Zeen Song",
        "Changwen Zheng",
        "Wenwen Qiang"
      ],
      "published": "2025-08-07T14:17:28+00:00",
      "updated": "2025-08-07T14:17:28+00:00",
      "arxiv_id": "2508.05428v1",
      "url": "http://arxiv.org/pdf/2508.05428v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation",
      "abstract": "Large language models (LLMs) are beginning to reshape how chemists plan and\nrun reactions in organic synthesis. Trained on millions of reported\ntransformations, these text-based models can propose synthetic routes, forecast\nreaction outcomes and even instruct robots that execute experiments without\nhuman supervision. Here we survey the milestones that turned LLMs from\nspeculative tools into practical lab partners. We show how coupling LLMs with\ngraph neural networks, quantum calculations and real-time spectroscopy shrinks\ndiscovery cycles and supports greener, data-driven chemistry. We discuss\nlimitations, including biased datasets, opaque reasoning and the need for\nsafety gates that prevent unintentional hazards. Finally, we outline community\ninitiatives open benchmarks, federated learning and explainable interfaces that\naim to democratize access while keeping humans firmly in control. These\nadvances chart a path towards rapid, reliable and inclusive molecular\ninnovation powered by artificial intelligence and automation.",
      "authors": [
        "Kartar Kumar Lohana Tharwani",
        "Rajesh Kumar",
        "Sumita",
        "Numan Ahmed",
        "Yong Tang"
      ],
      "published": "2025-08-07T14:17:23+00:00",
      "updated": "2025-08-07T14:17:23+00:00",
      "arxiv_id": "2508.05427v1",
      "url": "http://arxiv.org/pdf/2508.05427v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "LLM-based Multi-Agent Copilot for Quantum Sensor",
      "abstract": "Large language models (LLM) exhibit broad utility but face limitations in\nquantum sensor development, stemming from interdisciplinary knowledge barriers\nand involving complex optimization processes. Here we present QCopilot, an\nLLM-based multi-agent framework integrating external knowledge access, active\nlearning, and uncertainty quantification for quantum sensor design and\ndiagnosis. Comprising commercial LLMs with few-shot prompt engineering and\nvector knowledge base, QCopilot employs specialized agents to adaptively select\noptimization methods, automate modeling analysis, and independently perform\nproblem diagnosis. Applying QCopilot to atom cooling experiments, we generated\n10${}^{\\rm{8}}$ sub-$\\rm{\\mu}$K atoms without any human intervention within a\nfew hours, representing $\\sim$100$\\times$ speedup over manual experimentation.\nNotably, by continuously accumulating prior knowledge and enabling dynamic\nmodeling, QCopilot can autonomously identify anomalous parameters in\nmulti-parameter experimental settings. Our work reduces barriers to large-scale\nquantum sensor deployment and readily extends to other quantum information\nsystems.",
      "authors": [
        "Rong Sha",
        "Binglin Wang",
        "Jun Yang",
        "Xiaoxiao Ma",
        "Chengkun Wu",
        "Liang Yan",
        "Chao Zhou",
        "Jixun Liu",
        "Guochao Wang",
        "Shuhua Yan",
        "Lingxiao Zhu"
      ],
      "published": "2025-08-07T14:14:08+00:00",
      "updated": "2025-08-07T14:14:08+00:00",
      "arxiv_id": "2508.05421v1",
      "url": "http://arxiv.org/pdf/2508.05421v1",
      "categories": [
        "quant-ph",
        "cs.AI",
        "physics.atom-ph"
      ],
      "primary_category": "quant-ph"
    },
    {
      "title": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms",
      "abstract": "Modern RL-based post-training for large language models (LLMs) co-locate\ntrajectory sampling and policy optimisation on the same GPU cluster, forcing\nthe system to switch between inference and training workloads. This serial\ncontext switching violates the single-program-multiple-data (SPMD) assumption\nunderlying today's distributed training systems. We present Echo, the RL system\nthat cleanly decouples these two phases across heterogeneous \"inference\" and\n\"training\" swarms while preserving statistical efficiency. Echo introduces two\nlightweight synchronization protocols: a sequential pull mode that refreshes\nsampler weights on every API call for minimal bias, and an asynchronous\npush-pull mode that streams version-tagged rollouts through a replay buffer to\nmaximise hardware utilisation. Training three representative RL workloads with\nQwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster,\nEcho matches a fully co-located Verl baseline in convergence speed and final\nreward while off-loading trajectory generation to commodity edge hardware.\nThese promising results demonstrate that large-scale RL for LLMs could achieve\ndatacentre-grade performance using decentralised, heterogeneous resources.",
      "authors": [
        "Jie Xiao",
        "Shaoduo Gan",
        "Changyuan Fan",
        "Qingnan Ren",
        "Alfred Long",
        "Yuchen Zhang",
        "Rymon Yu",
        "Eric Yang",
        "Lynn Ai"
      ],
      "published": "2025-08-07T13:37:04+00:00",
      "updated": "2025-08-07T13:37:04+00:00",
      "arxiv_id": "2508.05387v1",
      "url": "http://arxiv.org/pdf/2508.05387v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "CT-GRAPH: Hierarchical Graph Attention Network for Anatomy-Guided CT Report Generation",
      "abstract": "As medical imaging is central to diagnostic processes, automating the\ngeneration of radiology reports has become increasingly relevant to assist\nradiologists with their heavy workloads. Most current methods rely solely on\nglobal image features, failing to capture fine-grained organ relationships\ncrucial for accurate reporting. To this end, we propose CT-GRAPH, a\nhierarchical graph attention network that explicitly models radiological\nknowledge by structuring anatomical regions into a graph, linking fine-grained\norgan features to coarser anatomical systems and a global patient context. Our\nmethod leverages pretrained 3D medical feature encoders to obtain global and\norgan-level features by utilizing anatomical masks. These features are further\nrefined within the graph and then integrated into a large language model to\ngenerate detailed medical reports. We evaluate our approach for the task of\nreport generation on the large-scale chest CT dataset CT-RATE. We provide an\nin-depth analysis of pretrained feature encoders for CT report generation and\nshow that our method achieves a substantial improvement of absolute 7.9\\% in F1\nscore over current state-of-the-art methods. The code is publicly available at\nhttps://github.com/hakal104/CT-GRAPH.",
      "authors": [
        "Hamza Kalisch",
        "Fabian HÃ¶rst",
        "Jens Kleesiek",
        "Ken Herrmann",
        "Constantin Seibold"
      ],
      "published": "2025-08-07T13:18:03+00:00",
      "updated": "2025-08-07T13:18:03+00:00",
      "arxiv_id": "2508.05375v1",
      "url": "http://arxiv.org/pdf/2508.05375v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Simulating LLM training workloads for heterogeneous compute and network infrastructure",
      "abstract": "The growing demand for large-scale GPU clusters in distributed model training\npresents a significant barrier to innovation, particularly in model\noptimization, performance tuning, and system-level enhancements. To address\nthis challenge, LLM training simulators are employed to estimate training time\nand guide design decisions. However, the state-of-the-art LLM training\nsimulators assume homogeneous compute and network infrastructure. In practice,\ndevice heterogeneity is inevitable due to resource sharing in cloud\nenvironments, frequent shifts in device generations, and inherent intra-chip\ninterconnect heterogeneity. To address the gap between state-of-the-art and\npractical requirements, we propose the design of a heterogeneity-aware\ndistributed LLM simulator capable of predicting training time while enabling\nabstractions to specify custom configurations for device groups and\ndevice-to-parallelism mapping. We present the design requirements and\nchallenges in building a heterogeneity-aware distributed ML training simulator,\nand design components such as non-uniform workload partitioning. Our initial\nsimulation results demonstrate the impact of heterogeneity on the model\ncomputation and communication time.",
      "authors": [
        "Sumit Kumar",
        "Arjun Temura",
        "Naman Sharma",
        "Ramanjeet Singh",
        "Meet Dadhania",
        "Praveen Tammana",
        "Satananda Burla",
        "Abed Mohammad Kamaluddin",
        "Rinku Shah"
      ],
      "published": "2025-08-07T13:15:59+00:00",
      "updated": "2025-08-07T13:15:59+00:00",
      "arxiv_id": "2508.05370v1",
      "url": "http://arxiv.org/pdf/2508.05370v1",
      "categories": [
        "cs.DC"
      ],
      "primary_category": "cs.DC"
    },
    {
      "title": "Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025",
      "abstract": "Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim\nto enable autonomous search processes where Large Language Models (LLMs)\niteratively refine outputs. However, applying these systems to domain-specific\nprofessional search, such as biomedical research, presents challenges, as\nautomated systems may reduce user involvement and misalign with expert\ninformation needs. Professional search tasks often demand high levels of user\nexpertise and transparency. The BioASQ CLEF 2025 challenge, using\nexpert-formulated questions, can serve as a platform to study these issues. We\nexplored the performance of current reasoning and nonreasoning LLMs like\nGemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our\nmethodology was a self-feedback mechanism where LLMs generated, evaluated, and\nthen refined their outputs for query expansion and for multiple answer types\n(yes/no, factoid, list, ideal). We investigated whether this iterative\nself-correction improves performance and if reasoning models are more capable\nof generating useful feedback. Preliminary results indicate varied performance\nfor the self-feedback strategy across models and tasks. This work offers\ninsights into LLM self-correction and informs future work on comparing the\neffectiveness of LLM-generated feedback with direct human expert input in these\nsearch systems.",
      "authors": [
        "Samy Ateia",
        "Udo Kruschwitz"
      ],
      "published": "2025-08-07T13:13:19+00:00",
      "updated": "2025-08-07T13:13:19+00:00",
      "arxiv_id": "2508.05366v1",
      "url": "http://arxiv.org/pdf/2508.05366v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making",
      "abstract": "Recent advancements in large language models (LLMs) have extended their\ncapabilities from basic text processing to complex reasoning tasks, including\nlegal interpretation, argumentation, and strategic interaction. However,\nempirical understanding of LLM behavior in open-ended, multi-agent settings\nespecially those involving deliberation over legal and ethical dilemmas remains\nlimited. We introduce NomicLaw, a structured multi-agent simulation where LLMs\nengage in collaborative law-making, responding to complex legal vignettes by\nproposing rules, justifying them, and voting on peer proposals. We\nquantitatively measure trust and reciprocity via voting patterns and\nqualitatively assess how agents use strategic language to justify proposals and\ninfluence outcomes. Experiments involving homogeneous and heterogeneous LLM\ngroups demonstrate how agents spontaneously form alliances, betray trust, and\nadapt their rhetoric to shape collective decisions. Our results highlight the\nlatent social reasoning and persuasive capabilities of ten open-source LLMs and\nprovide insights into the design of future AI systems capable of autonomous\nnegotiation, coordination and drafting legislation in legal settings.",
      "authors": [
        "Asutosh Hota",
        "Jussi P. P. Jokinen"
      ],
      "published": "2025-08-07T12:49:44+00:00",
      "updated": "2025-08-07T12:49:44+00:00",
      "arxiv_id": "2508.05344v1",
      "url": "http://arxiv.org/pdf/2508.05344v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control",
      "abstract": "Teaching robots dexterous skills from human videos remains challenging due to\nthe reliance on low-level trajectory imitation, which fails to generalize\nacross object types, spatial layouts, and manipulator configurations. We\npropose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables\ndual-arm robotic systems to perform task-level reasoning and execution directly\nfrom RGB and Depth human demonstrations. GF-VLA first extracts\nShannon-information-based cues to identify hands and objects with the highest\ntask relevance, then encodes these cues into temporally ordered scene graphs\nthat capture both hand-object and object-object interactions. These graphs are\nfused with a language-conditioned transformer that generates hierarchical\nbehavior trees and interpretable Cartesian motion commands. To improve\nexecution efficiency in bimanual settings, we further introduce a cross-hand\nselection policy that infers optimal gripper assignment without explicit\ngeometric reasoning. We evaluate GF-VLA on four structured dual-arm block\nassembly tasks involving symbolic shape construction and spatial\ngeneralization. Experimental results show that the information-theoretic scene\nrepresentation achieves over 95 percent graph accuracy and 93 percent subtask\nsegmentation, supporting the LLM planner in generating reliable and\nhuman-readable task policies. When executed by the dual-arm robot, these\npolicies yield 94 percent grasp success, 89 percent placement accuracy, and 90\npercent overall task success across stacking, letter-building, and geometric\nreconfiguration scenarios, demonstrating strong generalization and robustness\nacross diverse spatial and semantic variations.",
      "authors": [
        "Shunlei Li",
        "Longsen Gao",
        "Jin Wang",
        "Chang Che",
        "Xi Xiao",
        "Jiuwen Cao",
        "Yingbai Hu",
        "Hamid Reza Karimi"
      ],
      "published": "2025-08-07T12:48:09+00:00",
      "updated": "2025-08-07T12:48:09+00:00",
      "arxiv_id": "2508.05342v1",
      "url": "http://arxiv.org/pdf/2508.05342v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "The Term 'Agent' Has Been Diluted Beyond Utility and Requires Redefinition",
      "abstract": "The term 'agent' in artificial intelligence has long carried multiple\ninterpretations across different subfields. Recent developments in AI\ncapabilities, particularly in large language model systems, have amplified this\nambiguity, creating significant challenges in research communication, system\nevaluation and reproducibility, and policy development. This paper argues that\nthe term 'agent' requires redefinition. Drawing from historical analysis and\ncontemporary usage patterns, we propose a framework that defines clear minimum\nrequirements for a system to be considered an agent while characterizing\nsystems along a multidimensional spectrum of environmental interaction,\nlearning and adaptation, autonomy, goal complexity, and temporal coherence.\nThis approach provides precise vocabulary for system description while\npreserving the term's historically multifaceted nature. After examining\npotential counterarguments and implementation challenges, we provide specific\nrecommendations for moving forward as a field, including suggestions for\nterminology standardization and framework adoption. The proposed approach\noffers practical tools for improving research clarity and reproducibility while\nsupporting more effective policy development.",
      "authors": [
        "Brinnae Bent"
      ],
      "published": "2025-08-07T12:40:25+00:00",
      "updated": "2025-08-07T12:40:25+00:00",
      "arxiv_id": "2508.05338v1",
      "url": "http://arxiv.org/pdf/2508.05338v1",
      "categories": [
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "mKG-RAG: Multimodal Knowledge Graph-Enhanced RAG for Visual Question Answering",
      "abstract": "Recently, Retrieval-Augmented Generation (RAG) has been proposed to expand\ninternal knowledge of Multimodal Large Language Models (MLLMs) by incorporating\nexternal knowledge databases into the generation process, which is widely used\nfor knowledge-based Visual Question Answering (VQA) tasks. Despite impressive\nadvancements, vanilla RAG-based VQA methods that rely on unstructured documents\nand overlook the structural relationships among knowledge elements frequently\nintroduce irrelevant or misleading content, reducing answer accuracy and\nreliability. To overcome these challenges, a promising solution is to integrate\nmultimodal knowledge graphs (KGs) into RAG-based VQA frameworks to enhance the\ngeneration by introducing structured multimodal knowledge. Therefore, in this\npaper, we propose a novel multimodal knowledge-augmented generation framework\n(mKG-RAG) based on multimodal KGs for knowledge-intensive VQA tasks.\nSpecifically, our approach leverages MLLM-powered keyword extraction and\nvision-text matching to distill semantically consistent and modality-aligned\nentities/relationships from multimodal documents, constructing high-quality\nmultimodal KGs as structured knowledge representations. In addition, a\ndual-stage retrieval strategy equipped with a question-aware multimodal\nretriever is introduced to improve retrieval efficiency while refining\nprecision. Comprehensive experiments demonstrate that our approach\nsignificantly outperforms existing methods, setting a new state-of-the-art for\nknowledge-based VQA.",
      "authors": [
        "Xu Yuan",
        "Liangbo Ning",
        "Wenqi Fan",
        "Qing Li"
      ],
      "published": "2025-08-07T12:22:50+00:00",
      "updated": "2025-08-07T12:22:50+00:00",
      "arxiv_id": "2508.05318v1",
      "url": "http://arxiv.org/pdf/2508.05318v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents",
      "abstract": "We propose a hybrid architecture that integrates decision tree-based symbolic\nreasoning with the generative capabilities of large language models (LLMs)\nwithin a coordinated multi-agent framework. Unlike prior approaches that\nloosely couple symbolic and neural modules, our design embeds decision trees\nand random forests as callable oracles within a unified reasoning system.\nTree-based modules enable interpretable rule inference and causal logic, while\nLLM agents handle abductive reasoning, generalization, and interactive\nplanning. A central orchestrator maintains belief state consistency and\nmediates communication across agents and external tools, enabling reasoning\nover both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On\n\\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through\nlogic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in\nmultistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it\nboosts abstraction accuracy by +6.0\\% through integration of symbolic oracles.\nApplications in clinical decision support and scientific discovery show how the\nsystem encodes domain rules symbolically while leveraging LLMs for contextual\ninference and hypothesis generation. This architecture offers a robust,\ninterpretable, and extensible solution for general-purpose neuro-symbolic\nreasoning.",
      "authors": [
        "Andrew Kiruluta"
      ],
      "published": "2025-08-07T12:11:53+00:00",
      "updated": "2025-08-07T12:11:53+00:00",
      "arxiv_id": "2508.05311v1",
      "url": "http://arxiv.org/pdf/2508.05311v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens",
      "abstract": "The recently proposed Large Concept Model (LCM) generates text by predicting\na sequence of sentence-level embeddings and training with either mean-squared\nerror or diffusion objectives. We present SONAR-LLM, a decoder-only transformer\nthat \"thinks\" in the same continuous SONAR embedding space, yet is supervised\nthrough token-level cross-entropy propagated via the frozen SONAR decoder. This\nhybrid objective retains the semantic abstraction of LCM while eliminating its\ndiffusion sampler and restoring a likelihood-based training signal. Across\nmodel sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive\ngeneration quality. We report scaling trends, ablations, benchmark results, and\nrelease the complete training code and all pretrained checkpoints to foster\nreproducibility and future research.",
      "authors": [
        "Nikita Dragunov",
        "Temurbek Rahmatullaev",
        "Elizaveta Goncharova",
        "Andrey Kuznetsov",
        "Anton Razzhigaev"
      ],
      "published": "2025-08-07T12:03:44+00:00",
      "updated": "2025-08-07T12:03:44+00:00",
      "arxiv_id": "2508.05305v1",
      "url": "http://arxiv.org/pdf/2508.05305v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test",
      "abstract": "The Drawing Projection Test (DPT) is an essential tool in art therapy,\nallowing psychologists to assess participants' mental states through their\nsketches. Specifically, through sketches with the theme of \"a person picking an\napple from a tree (PPAT)\", it can be revealed whether the participants are in\nmental states such as depression. Compared with scales, the DPT can enrich\npsychologists' understanding of an individual's mental state. However, the\ninterpretation of the PPAT is laborious and depends on the experience of the\npsychologists. To address this issue, we propose an effective identification\nmethod to support psychologists in conducting a large-scale automatic DPT.\nUnlike traditional sketch recognition, DPT more focus on the overall evaluation\nof the sketches, such as color usage and space utilization. Moreover, PPAT\nimposes a time limit and prohibits verbal reminders, resulting in low drawing\naccuracy and a lack of detailed depiction. To address these challenges, we\npropose the following efforts: (1) Providing an experimental environment for\nautomated analysis of PPAT sketches for depression assessment; (2) Offering a\nVisual-Semantic depression assessment based on LLM (VS-LLM) method; (3)\nExperimental results demonstrate that our method improves by 17.6% compared to\nthe psychologist assessment method. We anticipate that this work will\ncontribute to the research in mental state assessment based on PPAT sketches'\nelements recognition. Our datasets and codes are available at\nhttps://github.com/wmeiqi/VS-LLM.",
      "authors": [
        "Meiqi Wu",
        "Yaxuan Kang",
        "Xuchen Li",
        "Shiyu Hu",
        "Xiaotang Chen",
        "Yunfeng Kang",
        "Weiqiang Wang",
        "Kaiqi Huang"
      ],
      "published": "2025-08-07T11:59:50+00:00",
      "updated": "2025-08-07T11:59:50+00:00",
      "arxiv_id": "2508.05299v1",
      "url": "http://arxiv.org/pdf/2508.05299v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming",
      "abstract": "We present GhostShell, a novel approach that leverages Large Language Models\n(LLMs) to enable streaming and concurrent behavioral programming for embodied\nsystems. In contrast to conventional methods that rely on pre-scheduled action\nsequences or behavior trees, GhostShell drives embodied systems to act\non-the-fly by issuing function calls incrementally as tokens are streamed from\nthe LLM. GhostShell features a streaming XML function token parser, a dynamic\nfunction interface mapper, and a multi-channel scheduler that orchestrates\nintra-channel synchronous and inter-channel asynchronous function calls,\nthereby coordinating serial-parallel embodied actions across multiple robotic\ncomponents as directed by the LLM. We evaluate GhostShell on our robot\nprototype COCO through comprehensive grounded experiments across 34 real-world\ninteraction tasks and multiple LLMs. The results demonstrate that our approach\nachieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4\nSonnet and up to 66X faster response times compared to LLM native function\ncalling APIs. GhostShell also proves effective in long-horizon multimodal\ntasks, demonstrating strong robustness and generalization.",
      "authors": [
        "Jian Gong",
        "Youwei Huang",
        "Bo Yuan",
        "Ming Zhu",
        "Juncheng Zhan",
        "Jinke Wang",
        "Hang Shu",
        "Mingyue Xiong",
        "Yanjun Ye",
        "Yufan Zu",
        "Yang Zhou",
        "Yihan Ding",
        "Xuannian Chen",
        "Xingyu Lu",
        "Runjie Ban",
        "Bingchao Huang",
        "Fusen Liu"
      ],
      "published": "2025-08-07T11:55:46+00:00",
      "updated": "2025-08-07T11:55:46+00:00",
      "arxiv_id": "2508.05298v1",
      "url": "http://arxiv.org/pdf/2508.05298v1",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming",
      "abstract": "We present GhostShell, a novel approach that leverages Large Language Models\n(LLMs) to enable streaming and concurrent behavioral programming for embodied\nsystems. In contrast to conventional methods that rely on pre-scheduled action\nsequences or behavior trees, GhostShell drives embodied systems to act\non-the-fly by issuing function calls incrementally as tokens are streamed from\nthe LLM. GhostShell features a streaming XML function token parser, a dynamic\nfunction interface mapper, and a multi-channel scheduler that orchestrates\nintra-channel synchronous and inter-channel asynchronous function calls,\nthereby coordinating serial-parallel embodied actions across multiple robotic\ncomponents under LLM guidance. We evaluate GhostShell on our robotic prototype\nCOCO through comprehensive grounded experiments across 34 real-world\ninteraction tasks and multiple LLM backends. The results demonstrate that our\napproach achieves a state-of-the-art Behavioral Correctness Metric of 0.85 with\nClaude-4-Sonnet, and up to 66X faster response times compared to native LLM\nfunction calling APIs. GhostShell also proves effective in long-horizon\nmultimodal tasks, exhibiting strong robustness and generalization capabilities.",
      "authors": [
        "Jian Gong",
        "Youwei Huang",
        "Bo Yuan",
        "Ming Zhu",
        "Zhou Liao",
        "Jianhang Liang",
        "Juncheng Zhan",
        "Jinke Wang",
        "Hang Shu",
        "Mingyue Xiong",
        "Yanjun Ye",
        "Yufan Zu",
        "Yang Zhou",
        "Yihan Ding",
        "Xuannian Chen",
        "Xingyu Lu",
        "Runjie Ban",
        "Bingchao Huang",
        "Fusen Liu"
      ],
      "published": "2025-08-07T11:55:46+00:00",
      "updated": "2025-08-08T04:36:40+00:00",
      "arxiv_id": "2508.05298v2",
      "url": "http://arxiv.org/pdf/2508.05298v2",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction",
      "abstract": "Foundation models, including large language models (LLMs) and vision-language\nmodels (VLMs), have recently enabled novel approaches to robot autonomy and\nhuman-robot interfaces. In parallel, vision-language-action models (VLAs) or\nlarge behavior models (BLMs) are increasing the dexterity and capabilities of\nrobotic systems. This survey paper focuses on those words advancing towards\nagentic applications and architectures. This includes initial efforts exploring\nGPT-style interfaces to tooling, as well as more complex system where AI agents\nare coordinators, planners, perception actors, or generalist interfaces. Such\nagentic architectures allow robots to reason over natural language\ninstructions, invoke APIs, plan task sequences, or assist in operations and\ndiagnostics. In addition to peer-reviewed research, due to the fast-evolving\nnature of the field, we highlight and include community-driven projects, ROS\npackages, and industrial frameworks that show emerging trends. We propose a\ntaxonomy for classifying model integration approaches and present a comparative\nanalysis of the role that agents play in different solutions in today's\nliterature.",
      "authors": [
        "Sahar Salimpour",
        "Lei Fu",
        "Farhad Keramat",
        "Leonardo Militano",
        "Giovanni Toffetti",
        "Harry Edelman",
        "Jorge PeÃ±a Queralta"
      ],
      "published": "2025-08-07T11:48:03+00:00",
      "updated": "2025-08-07T11:48:03+00:00",
      "arxiv_id": "2508.05294v1",
      "url": "http://arxiv.org/pdf/2508.05294v1",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders",
      "abstract": "Conversational recommender systems (CRS) based on Large Language Models\n(LLMs) need to constantly be aligned to the user preferences to provide\nsatisfying and context-relevant item recommendations. The traditional\nsupervised fine-tuning cannot capture the implicit feedback signal, e.g., dwell\ntime, sentiment polarity, or engagement patterns. In this paper, we share a\nfine-tuning solution using human feedback reinforcement learning (RLHF) to\nmaximize implied user feedback (IUF) in a multi-turn recommendation context. We\nspecify a reward model $R_{\\phi}$ learnt on weakly-labelled engagement\ninformation and maximize user-centric utility by optimizing the foundational\nLLM M_{\\theta} through a proximal policy optimization (PPO) approach. The\narchitecture models conversational state transitions $s_t \\to a_t \\to s_{t\n+1}$, where the action $a_t$ is associated with LLM-generated item suggestions\nonly on condition of conversation history in the past. The evaluation across\nsynthetic and real-world datasets (e.g.REDIAL, OpenDialKG) demonstrates that\nour RLHF-fine-tuned models can perform better in terms of top-$k$\nrecommendation accuracy, coherence, and user satisfaction compared to\n(arrow-zero-cmwrquca-teja-falset ensuite 2Round group-deca States penalty give\nup This paper shows that implicit signal alignment can be efficient in\nachieving scalable and user-adaptive design of CRS.",
      "authors": [
        "Zhongheng Yang",
        "Aijia Sun",
        "Yushang Zhao",
        "Yinuo Yang",
        "Dannier Li",
        "Chengrui Zhou"
      ],
      "published": "2025-08-07T11:36:55+00:00",
      "updated": "2025-08-07T11:36:55+00:00",
      "arxiv_id": "2508.05289v1",
      "url": "http://arxiv.org/pdf/2508.05289v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue",
      "abstract": "Meta-reviewing is a pivotal stage in the peer-review process, serving as the\nfinal step in determining whether a paper is recommended for acceptance. Prior\nresearch on meta-reviewing has treated this as a summarization problem over\nreview reports. However, complementary to this perspective, meta-reviewing is a\ndecision-making process that requires weighing reviewer arguments and placing\nthem within a broader context. Prior research has demonstrated that\ndecision-makers can be effectively assisted in such scenarios via dialogue\nagents. In line with this framing, we explore the practical challenges for\nrealizing dialog agents that can effectively assist meta-reviewers. Concretely,\nwe first address the issue of data scarcity for training dialogue agents by\ngenerating synthetic data using Large Language Models (LLMs) based on a\nself-refinement strategy to improve the relevance of these dialogues to expert\ndomains. Our experiments demonstrate that this method produces higher-quality\nsynthetic data and can serve as a valuable resource towards training\nmeta-reviewing assistants. Subsequently, we utilize this data to train dialogue\nagents tailored for meta-reviewing and find that these agents outperform\n\\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our\nagents in real-world meta-reviewing scenarios and confirm their effectiveness\nin enhancing the efficiency of meta-reviewing.\\footnote{Code and Data:\nhttps://github.com/UKPLab/arxiv2025-meta-review-as-dialog",
      "authors": [
        "Sukannya Purkayastha",
        "Nils Dycke",
        "Anne Lauscher",
        "Iryna Gurevych"
      ],
      "published": "2025-08-07T11:27:43+00:00",
      "updated": "2025-08-07T11:27:43+00:00",
      "arxiv_id": "2508.05283v1",
      "url": "http://arxiv.org/pdf/2508.05283v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs",
      "abstract": "Chain-of-Thought (CoT) prompting has significantly advanced the reasoning\ncapabilities of Large Language Models (LLMs), yet the reliability of these\nreasoning chains remains a critical challenge. A widely held \"cascading\nfailure\" hypothesis suggests that errors are most detrimental when they occur\nearly in the reasoning process. This paper challenges that assumption through\nsystematic error-injection experiments, revealing a counter-intuitive\nphenomenon we term \"Late-Stage Fragility\": errors introduced in the later\nstages of a CoT chain are significantly more likely to corrupt the final answer\nthan identical errors made at the beginning. To address this specific\nvulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought\n(ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive\nVerification Manager (AVM) operates first, followed by the Multi-Perspective\nSelf-Correction Engine (MSCE). The AVM leverages a Positional Impact Score\nfunction I(k) that assigns different weights based on the position within the\nreasoning chains, addressing the Late-Stage Fragility issue by identifying and\nprioritizing high-risk, late-stage steps. Once these critical steps are\nidentified, the MSCE applies robust, dual-path correction specifically to the\nfailure parts. Extensive experiments on benchmarks such as GSM8K and MATH\ndemonstrate that ASCoT achieves outstanding accuracy, outperforming strong\nbaselines, including standard CoT. Our work underscores the importance of\ndiagnosing specific failure modes in LLM reasoning and advocates for a shift\nfrom uniform verification strategies to adaptive, vulnerability-aware\ncorrection mechanisms.",
      "authors": [
        "Dongxu Zhang",
        "Ning Yang",
        "Jihua Zhu",
        "Jinnan Yang",
        "Miao Xin",
        "Baoliang Tian"
      ],
      "published": "2025-08-07T11:26:40+00:00",
      "updated": "2025-08-07T11:26:40+00:00",
      "arxiv_id": "2508.05282v1",
      "url": "http://arxiv.org/pdf/2508.05282v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding",
      "abstract": "Understanding dynamic outdoor environments requires capturing complex object\ninteractions and their evolution over time. LiDAR-based 4D point clouds provide\nprecise spatial geometry and rich temporal cues, making them ideal for\nrepresenting real-world scenes. However, despite their potential, 4D LiDAR\nremains underexplored in the context of Multimodal Large Language Models\n(MLLMs) due to the absence of high-quality, modality-specific annotations and\nthe lack of MLLM architectures capable of processing its high-dimensional\ncomposition. To address these challenges, we introduce B4DL, a new benchmark\nspecifically designed for training and evaluating MLLMs on 4D LiDAR\nunderstanding. In addition, we propose a scalable data generation pipeline and\nan MLLM model that, for the first time, directly processes raw 4D LiDAR by\nbridging it with language understanding. Combined with our dataset and\nbenchmark, our model offers a unified solution for spatio-temporal reasoning in\ndynamic outdoor environments. We provide rendered 4D LiDAR videos, generated\ndataset, and inference outputs on diverse scenarios at:\nhttps://mmb4dl.github.io/mmb4dl/",
      "authors": [
        "Changho Choi",
        "Youngwoo Shin",
        "Gyojin Han",
        "Dong-Jae Lee",
        "Junmo Kim"
      ],
      "published": "2025-08-07T11:11:56+00:00",
      "updated": "2025-08-07T11:11:56+00:00",
      "arxiv_id": "2508.05269v1",
      "url": "http://arxiv.org/pdf/2508.05269v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "An Explainable Natural Language Framework for Identifying and Notifying Target Audiences In Enterprise Communication",
      "abstract": "In large-scale maintenance organizations, identifying subject matter experts\nand managing communications across complex entities relationships poses\nsignificant challenges -- including information overload and longer response\ntimes -- that traditional communication approaches fail to address effectively.\nWe propose a novel framework that combines RDF graph databases with LLMs to\nprocess natural language queries for precise audience targeting, while\nproviding transparent reasoning through a planning-orchestration architecture.\nOur solution enables communication owners to formulate intuitive queries\ncombining concepts such as equipment, manufacturers, maintenance engineers, and\nfacilities, delivering explainable results that maintain trust in the system\nwhile improving communication efficiency across the organization.",
      "authors": [
        "VÃ­tor N. LourenÃ§o",
        "Mohnish Dubey",
        "Yunfei Bai",
        "Audrey Depeige",
        "Vivek Jain"
      ],
      "published": "2025-08-07T11:02:40+00:00",
      "updated": "2025-08-07T11:02:40+00:00",
      "arxiv_id": "2508.05267v1",
      "url": "http://arxiv.org/pdf/2508.05267v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Understanding and Mitigating Errors of LLM-Generated RTL Code",
      "abstract": "Despite the promising potential of large language model (LLM) based\nregister-transfer-level (RTL) code generation, the overall success rate remains\nunsatisfactory. Errors arise from various factors, with limited understanding\nof specific failure causes hindering improvement. To address this, we conduct a\ncomprehensive error analysis and manual categorization. Our findings reveal\nthat most errors stem not from LLM reasoning limitations, but from insufficient\nRTL programming knowledge, poor understanding of circuit concepts, ambiguous\ndesign descriptions, or misinterpretation of complex multimodal inputs.\nLeveraging in-context learning, we propose targeted error correction\ntechniques. Specifically, we construct a domain-specific knowledge base and\nemploy retrieval-augmented generation (RAG) to supply necessary RTL knowledge.\nTo mitigate ambiguity errors, we introduce design description rules and\nimplement a rule-checking mechanism. For multimodal misinterpretation, we\nintegrate external tools to convert inputs into LLM-compatible meta-formats.\nFor remaining errors, we adopt an iterative debugging loop (simulation-error\nlocalization-correction). Integrating these techniques into an LLM-based\nframework significantly improves performance. We incorporate these error\ncorrection techniques into a foundational LLM-based RTL code generation\nframework, resulting in significantly improved performance. Experimental\nresults show that our enhanced framework achieves 91.0\\% accuracy on the\nVerilogEval benchmark, surpassing the baseline code generation approach by\n32.7\\%, demonstrating the effectiveness of our methods.",
      "authors": [
        "Jiazheng Zhang",
        "Cheng Liu",
        "Huawei Li"
      ],
      "published": "2025-08-07T11:02:32+00:00",
      "updated": "2025-08-07T11:02:32+00:00",
      "arxiv_id": "2508.05266v1",
      "url": "http://arxiv.org/pdf/2508.05266v1",
      "categories": [
        "cs.AR",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AR"
    },
    {
      "title": "MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs",
      "abstract": "The Mixture-of-Experts (MoE) architecture has become a predominant paradigm\nfor scaling large language models (LLMs). Despite offering strong performance\nand computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and\nKimi-K2-Instruct present serious challenges due to substantial memory\nrequirements in deployment. While recent works have explored MoE compression to\naddress this issue, existing methods often suffer from considerable accuracy\ndrops (e.g., 7-14% relatively) even at modest compression rates. This paper\nintroduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model\ncompression while incurring minimal accuracy drops. Specifically, each up/gate\nmatrix in an expert is decomposed via a rank decomposition as W = AB, where\nmatrix A is unique to each expert. The relatively larger matrix B is further\nre-parameterized as a linear combination of basis matrices {Bi} shared across\nall experts within a given MoE layer. The factorization is learned by\nminimizing the reconstruction error relative to the original weight matrices.\nExperiments demonstrate that MoBE achieves notably lower accuracy drops\ncompared to prior works. For instance, MoBE can reduce the parameter counts of\nQwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by\n24%-30% with only 1%-2% accuracy drop (about 2% drops when measured\nrelatively).",
      "authors": [
        "Xiaodong Chen",
        "Mingming Ha",
        "Zhenzhong Lan",
        "Jing Zhang",
        "Jianguo Li"
      ],
      "published": "2025-08-07T10:48:24+00:00",
      "updated": "2025-08-07T10:48:24+00:00",
      "arxiv_id": "2508.05257v1",
      "url": "http://arxiv.org/pdf/2508.05257v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL",
      "abstract": "Code large language models (LLMs) have become indispensable tools for\nbuilding efficient and automated coding pipelines. Existing models are\ntypically post-trained using reinforcement learning (RL) from general-purpose\nLLMs using \"human instruction-final answer\" pairs, where the instructions are\nusually from manual annotations. However, collecting high-quality coding\ninstructions is both labor-intensive and difficult to scale. On the other hand,\ncode snippets are abundantly available from various sources. This imbalance\npresents a major bottleneck in instruction-based post-training. We propose\nCodeBoost, a post-training framework that enhances code LLMs purely from code\nsnippets, without relying on human-annotated instructions. CodeBoost introduces\nthe following key components: (1) maximum-clique curation, which selects a\nrepresentative and diverse training corpus from code; (2) bi-directional\nprediction, which enables the model to learn from both forward and backward\nprediction objectives; (3) error-aware prediction, which incorporates learning\nsignals from both correct and incorrect outputs; (4) heterogeneous\naugmentation, which diversifies the training distribution to enrich code\nsemantics; and (5) heterogeneous rewarding, which guides model learning through\nmultiple reward types including format correctness and execution feedback from\nboth successes and failures. Extensive experiments across several code LLMs and\nbenchmarks verify that CodeBoost consistently improves performance,\ndemonstrating its effectiveness as a scalable and effective training pipeline.",
      "authors": [
        "Sijie Wang",
        "Quanjiang Guo",
        "Kai Zhao",
        "Yawei Zhang",
        "Xin Li",
        "Xiang Li",
        "Siqi Li",
        "Rui She",
        "Shangshu Yu",
        "Wee Peng Tay"
      ],
      "published": "2025-08-07T10:31:24+00:00",
      "updated": "2025-08-07T10:31:24+00:00",
      "arxiv_id": "2508.05242v1",
      "url": "http://arxiv.org/pdf/2508.05242v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Pruning Large Language Models by Identifying and Preserving Functional Networks",
      "abstract": "Structured pruning is one of the representative techniques for compressing\nlarge language models (LLMs) to reduce GPU memory consumption and accelerate\ninference speed. It offers significant practical value in improving the\nefficiency of LLMs in real-world applications. Current structured pruning\nmethods typically rely on assessment of the importance of the structure units\nand pruning the units with less importance. Most of them overlooks the\ninteraction and collaboration among artificial neurons that are crucial for the\nfunctionalities of LLMs, leading to a disruption in the macro functional\narchitecture of LLMs and consequently a pruning performance degradation.\nInspired by the inherent similarities between artificial neural networks and\nfunctional neural networks in the human brain, we alleviate this challenge and\npropose to prune LLMs by identifying and preserving functional networks within\nLLMs in this study. To achieve this, we treat an LLM as a digital brain and\ndecompose the LLM into functional networks, analogous to identifying functional\nbrain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving\nthe key neurons within these functional networks. Experimental results\ndemonstrate that the proposed method can successfully identify and locate\nfunctional networks and key neurons in LLMs, enabling efficient model pruning.\nOur code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.",
      "authors": [
        "Yiheng Liu",
        "Junhao Ning",
        "Sichen Xia",
        "Xiaohui Gao",
        "Ning Qiang",
        "Bao Ge",
        "Junwei Han",
        "Xintao Hu"
      ],
      "published": "2025-08-07T10:27:01+00:00",
      "updated": "2025-08-07T10:27:01+00:00",
      "arxiv_id": "2508.05239v1",
      "url": "http://arxiv.org/pdf/2508.05239v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models",
      "abstract": "Level 3 automated driving systems allows drivers to engage in secondary tasks\nwhile diminishing their perception of risk. In the event of an emergency\nnecessitating driver intervention, the system will alert the driver with a\nlimited window for reaction and imposing a substantial cognitive burden. To\naddress this challenge, this study employs a Large Language Model (LLM) to\nassist drivers in maintaining an appropriate attention on road conditions\nthrough a \"humanized\" persuasive advice. Our tool leverages the road conditions\nencountered by Level 3 systems as triggers, proactively steering driver\nbehavior via both visual and auditory routes. Empirical study indicates that\nour tool is effective in sustaining driver attention with reduced cognitive\nload and coordinating secondary tasks with takeover behavior. Our work provides\ninsights into the potential of using LLMs to support drivers during multi-task\nautomated driving.",
      "authors": [
        "Wei Xiang",
        "Muchen Li",
        "Jie Yan",
        "Manling Zheng",
        "Hanfei Zhu",
        "Mengyun Jiang",
        "Lingyun Sun"
      ],
      "published": "2025-08-07T10:26:28+00:00",
      "updated": "2025-08-07T10:26:28+00:00",
      "arxiv_id": "2508.05238v1",
      "url": "http://arxiv.org/pdf/2508.05238v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation",
      "abstract": "The surge in rich multimodal content on social media platforms has greatly\nadvanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)\nfurther accelerating progress in this field. Current approaches primarily\nleverage the knowledge and reasoning capabilities of parameter-heavy\n(Multimodal) LLMs for sentiment classification, overlooking autonomous\nmultimodal sentiment reasoning generation in resource-constrained environments.\nTherefore, we focus on the Resource-Limited Joint Multimodal Sentiment\nReasoning and Classification task, JMSRC, which simultaneously performs\nmultimodal sentiment reasoning chain generation and sentiment classification\nonly with a lightweight model. We propose a Multimodal Chain-of-Thought\nReasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a\n\"Teacher-Assistant-Student\" distillation paradigm to address deployment\nconstraints in resource-limited environments. We first leverage a\nhigh-performance Multimodal Large Language Model (MLLM) to generate the initial\nreasoning dataset and train a medium-sized assistant model with a multi-task\nlearning mechanism. A lightweight student model is jointly trained to perform\nefficient multimodal sentiment reasoning generation and classification.\nExtensive experiments on four datasets demonstrate that MulCoT-RD with only 3B\nparameters achieves strong performance on JMSRC, while exhibiting robust\ngeneralization and enhanced interpretability.",
      "authors": [
        "Haonan Shangguan",
        "Xiaocui Yang",
        "Shi Feng",
        "Daling Wang",
        "Yifei Zhang",
        "Ge Yu"
      ],
      "published": "2025-08-07T10:23:14+00:00",
      "updated": "2025-08-07T10:23:14+00:00",
      "arxiv_id": "2508.05234v1",
      "url": "http://arxiv.org/pdf/2508.05234v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs",
      "abstract": "Traditional parameter-efficient fine-tuning (PEFT) methods such as LoRA are\ntightly coupled with the base model architecture, which constrains their\napplicability across heterogeneous pretrained large language models (LLMs). To\naddress this limitation, we introduce Cross-LoRA, a data-free framework for\ntransferring LoRA modules between diverse base models without requiring\nadditional training data. Cross-LoRA consists of two key components: (a)\nLoRA-Align, which performs subspace alignment between source and target base\nmodels through rank-truncated singular value decomposition (SVD) and\nFrobenius-optimal linear transformation, ensuring compatibility under dimension\nmismatch; and (b) LoRA-Shift, which applies the aligned subspaces to project\nsource LoRA weight updates into the target model parameter space. Both\ncomponents are data-free, training-free, and enable lightweight adaptation on a\ncommodity GPU in 20 minutes. Experiments on ARCs, OBOA and HellaSwag show that\nCross-LoRA achieves relative gains of up to 5.26% over base models. Across\nother commonsense reasoning benchmarks, Cross-LoRA maintains performance\ncomparable to that of directly trained LoRA adapters.",
      "authors": [
        "Feifan Xia",
        "Mingyang Liao",
        "Yuyang Fang",
        "Defang Li",
        "Yantong Xie",
        "Weikang Li",
        "Yang Li",
        "Deguo Xia",
        "Jizhou Huang"
      ],
      "published": "2025-08-07T10:21:08+00:00",
      "updated": "2025-08-07T10:21:08+00:00",
      "arxiv_id": "2508.05232v1",
      "url": "http://arxiv.org/pdf/2508.05232v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images",
      "abstract": "Spectral information has long been recognized as a critical cue in remote\nsensing observations. Although numerous vision-language models have been\ndeveloped for pixel-level interpretation, spectral information remains\nunderutilized, resulting in suboptimal performance, particularly in\nmultispectral scenarios. To address this limitation, we construct a\nvision-language instruction-following dataset named SPIE, which encodes\nspectral priors of land-cover objects into textual attributes recognizable by\nlarge language models (LLMs), based on classical spectral index computations.\nLeveraging this dataset, we propose SPEX, a multimodal LLM designed for\ninstruction-driven land cover extraction. To this end, we introduce several\ncarefully designed components and training strategies, including multiscale\nfeature aggregation, token context condensation, and multispectral visual\npre-training, to achieve precise and flexible pixel-level interpretation. To\nthe best of our knowledge, SPEX is the first multimodal vision-language model\ndedicated to land cover extraction in spectral remote sensing imagery.\nExtensive experiments on five public multispectral datasets demonstrate that\nSPEX consistently outperforms existing state-of-the-art methods in extracting\ntypical land cover categories such as vegetation, buildings, and water bodies.\nMoreover, SPEX is capable of generating textual explanations for its\npredictions, thereby enhancing interpretability and user-friendliness. Code\nwill be released at: https://github.com/MiliLab/SPEX.",
      "authors": [
        "Dongchen Si",
        "Di Wang",
        "Erzhong Gao",
        "Xiaolei Qin",
        "Liu Zhao",
        "Jing Zhang",
        "Minqiang Xu",
        "Jianbo Zhan",
        "Jianshe Wang",
        "Lin Liu",
        "Bo Du",
        "Liangpei Zhang"
      ],
      "published": "2025-08-07T09:37:45+00:00",
      "updated": "2025-08-07T09:37:45+00:00",
      "arxiv_id": "2508.05202v1",
      "url": "http://arxiv.org/pdf/2508.05202v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in finance",
      "abstract": "Hallucination remains a critical challenge for deploying Large Language\nModels (LLMs) in finance. Accurate extraction and precise calculation from\ntabular data are essential for reliable financial analysis, since even minor\nnumerical errors can undermine decision-making and regulatory compliance.\nFinancial applications have unique requirements, often relying on\ncontext-dependent, numerical, and proprietary tabular data that existing\nhallucination benchmarks rarely capture. In this study, we develop a rigorous\nand scalable framework for evaluating intrinsic hallucinations in financial\nLLMs, conceptualized as a context-aware masked span prediction task over\nreal-world financial documents. Our main contributions are: (1) a novel,\nautomated dataset creation paradigm using a masking strategy; (2) a new\nhallucination evaluation dataset derived from S&P 500 annual reports; and (3) a\ncomprehensive evaluation of intrinsic hallucination patterns in\nstate-of-the-art LLMs on financial tabular data. Our work provides a robust\nmethodology for in-house LLM evaluation and serves as a critical step toward\nbuilding more trustworthy and reliable financial Generative AI systems.",
      "authors": [
        "Mengao Zhang",
        "Jiayu Fu",
        "Tanya Warrier",
        "Yuwen Wang",
        "Tianhui Tan",
        "Ke-wei Huang"
      ],
      "published": "2025-08-07T09:37:14+00:00",
      "updated": "2025-08-07T09:37:14+00:00",
      "arxiv_id": "2508.05201v1",
      "url": "http://arxiv.org/pdf/2508.05201v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0",
      "abstract": "We introduce **EvoGraph**, a framework that enables software systems to\nevolve their own source code, build pipelines, documentation, and tickets.\nEvoGraph represents every artefact in a typed directed graph, applies learned\nmutation operators driven by specialized small language models (SLMs), and\nselects survivors with a multi-objective fitness. On three benchmarks, EvoGraph\nfixes 83% of known security vulnerabilities, translates COBOL to Java with 93%\nfunctional equivalence (test verified), and maintains documentation freshness\nwithin two minutes. Experiments show a 40% latency reduction and a sevenfold\ndrop in feature lead time compared with strong baselines. We extend our\napproach to **evoGraph**, leveraging language-specific SLMs for modernizing\n.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%\nsemantic equivalence across languages while reducing computational costs by 90%\ncompared to large language models. EvoGraph's design responds to empirical\nfailure modes in legacy modernization, such as implicit contracts, performance\npreservation, and integration evolution. Our results suggest a practical path\ntoward Software 3.0, where systems adapt continuously yet remain under\nmeasurable control.",
      "authors": [
        "Igor Costa",
        "Christopher Baran"
      ],
      "published": "2025-08-07T09:36:30+00:00",
      "updated": "2025-08-07T09:36:30+00:00",
      "arxiv_id": "2508.05199v1",
      "url": "http://arxiv.org/pdf/2508.05199v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "D.2.2; D.2.7; I.2.2"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering",
      "abstract": "Retrieval-Augmented Generation (RAG) has been introduced to mitigate\nhallucinations in Multimodal Large Language Models (MLLMs) by incorporating\nexternal knowledge into the generation process, and it has become a widely\nadopted approach for knowledge-intensive Visual Question Answering (VQA).\nHowever, existing RAG methods typically retrieve from either text or images in\nisolation, limiting their ability to address complex queries that require\nmulti-hop reasoning or up-to-date factual knowledge. To address this\nlimitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for\nKnowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to\nidentify the query's subject domain for domain-specific reasoning, along with a\nsearch router that dynamically selects optimal retrieval strategies. By\norchestrating both text and image search agents in a hybrid setup, our system\nsupports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle\ncomplex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM\nChallenge at KDD Cup 2025, where it significantly enhances the reasoning\nperformance of base models under challenging scenarios. Our framework achieves\nsubstantial improvements in both answer accuracy and knowledge overlap scores,\noutperforming baselines by 5.06% on the single-source task, 6.35% on the\nmulti-source task, and 5.03% on the multi-turn task.",
      "authors": [
        "Zhuohang Jiang",
        "Pangjing Wu",
        "Xu Yuan",
        "Wenqi Fan",
        "Qing Li"
      ],
      "published": "2025-08-07T09:32:49+00:00",
      "updated": "2025-08-07T09:32:49+00:00",
      "arxiv_id": "2508.05197v1",
      "url": "http://arxiv.org/pdf/2508.05197v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning",
      "abstract": "In recent years, large language models (LLMs) have made significant progress\nin code intelligence, yet systematically evaluating their code understanding\nand reasoning abilities remains challenging. Mainstream benchmarks such as\nHumanEval and MBPP primarily assess functional correctness, while reasoning\nbenchmarks like CRUXEVAL are limited to single-function, low-complexity\nscenarios. As a result, advanced models achieve nearly saturated scores,\nlimiting their discriminative power. To address this, we present\nSTEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex\nmulti-function understanding and fine-grained execution reasoning. SX-Bench\nfeatures tasks involving collaboration among multiple sub-functions (e.g.,\nchained calls, nested loops), shifting evaluation towards overall control and\ndata flow modeling. It defines \"computation steps\" as the minimal execution\nunit and requires models to predict the total number of steps in reasoning\ntasks, thereby assessing a model's in-depth understanding of dynamic execution\nbeyond simple I/O matching. Evaluation on over 20 mainstream models (including\n14 reasoning-enhanced models) demonstrates that SX-Bench is highly\ndiscriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent\naccuracy on Hard-Reasoning tasks, much lower than its saturated scores on\nprevious benchmarks, thereby revealing bottlenecks in complex and fine-grained\nreasoning. We also release an automated pipeline combining program synthesis,\nsymbolic execution, and LLM-aided validation for efficient benchmark generation\nand quality assurance. SX-Bench advances code evaluation from \"single-function\nverification\" to \"multi-function dynamic reasoning,\" providing a key tool for\nthe in-depth assessment of advanced code intelligence models.",
      "authors": [
        "Kaiwen Yan",
        "Yuhang Chang",
        "Zirui Guo",
        "Yaling Mou",
        "Jiang Ming",
        "Jingwei Sun"
      ],
      "published": "2025-08-07T09:28:43+00:00",
      "updated": "2025-08-07T09:28:43+00:00",
      "arxiv_id": "2508.05193v1",
      "url": "http://arxiv.org/pdf/2508.05193v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "AI-assisted JSON Schema Creation and Mapping",
      "abstract": "Model-Driven Engineering (MDE) places models at the core of system and data\nengineering processes. In the context of research data, these models are\ntypically expressed as schemas that define the structure and semantics of\ndatasets. However, many domains still lack standardized models, and creating\nthem remains a significant barrier, especially for non-experts. We present a\nhybrid approach that combines large language models (LLMs) with deterministic\ntechniques to enable JSON Schema creation, modification, and schema mapping\nbased on natural language inputs by the user. These capabilities are integrated\ninto the open-source tool MetaConfigurator, which already provides visual model\nediting, validation, code generation, and form generation from models. For data\nintegration, we generate schema mappings from heterogeneous JSON, CSV, XML, and\nYAML data using LLMs, while ensuring scalability and reliability through\ndeterministic execution of generated mapping rules. The applicability of our\nwork is demonstrated in an application example in the field of chemistry. By\ncombining natural language interaction with deterministic safeguards, this work\nsignificantly lowers the barrier to structured data modeling and data\nintegration for non-experts.",
      "authors": [
        "Felix Neubauer",
        "JÃ¼rgen Pleiss",
        "Benjamin Uekermann"
      ],
      "published": "2025-08-07T09:27:10+00:00",
      "updated": "2025-08-07T09:27:10+00:00",
      "arxiv_id": "2508.05192v1",
      "url": "http://arxiv.org/pdf/2508.05192v1",
      "categories": [
        "cs.SE",
        "H.2.3; I.2.6; D.2.2"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination",
      "abstract": "Timely and effective incident response is key to managing the growing\nfrequency of cyberattacks. However, identifying the right response actions for\ncomplex systems is a major technical challenge. A promising approach to\nmitigate this challenge is to use the security knowledge embedded in large\nlanguage models (LLMs) to assist security operators during incident handling.\nRecent research has demonstrated the potential of this approach, but current\nmethods are mainly based on prompt engineering of frontier LLMs, which is\ncostly and prone to hallucinations. We address these limitations by presenting\na novel way to use an LLM for incident response planning with reduced\nhallucination. Our method includes three steps: fine-tuning, information\nretrieval, and lookahead planning. We prove that our method generates response\nplans with a bounded probability of hallucination and that this probability can\nbe made arbitrarily small at the expense of increased planning time under\ncertain assumptions. Moreover, we show that our method is lightweight and can\nrun on commodity hardware. We evaluate our method on logs from incidents\nreported in the literature. The experimental results show that our method a)\nachieves up to 22% shorter recovery times than frontier LLMs and b) generalizes\nto a broad range of incident types and response actions.",
      "authors": [
        "Kim Hammar",
        "Tansu Alpcan",
        "Emil C. Lupu"
      ],
      "published": "2025-08-07T09:23:25+00:00",
      "updated": "2025-08-07T09:23:25+00:00",
      "arxiv_id": "2508.05188v1",
      "url": "http://arxiv.org/pdf/2508.05188v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering",
      "abstract": "This paper presents the contributions of the ATLANTIS team to SemEval-2025\nTask 3, focusing on detecting hallucinated text spans in question answering\nsystems. Large Language Models (LLMs) have significantly advanced Natural\nLanguage Generation (NLG) but remain susceptible to hallucinations, generating\nincorrect or misleading content. To address this, we explored methods both with\nand without external context, utilizing few-shot prompting with a LLM,\ntoken-level classification or LLM fine-tuned on synthetic data. Notably, our\napproaches achieved top rankings in Spanish and competitive placements in\nEnglish and German. This work highlights the importance of integrating relevant\ncontext to mitigate hallucinations and demonstrate the potential of fine-tuned\nmodels and prompt engineering.",
      "authors": [
        "Catherine Kobus",
        "FranÃ§ois Lancelot",
        "Marion-CÃ©cile Martin",
        "Nawal Ould Amer"
      ],
      "published": "2025-08-07T09:15:15+00:00",
      "updated": "2025-08-07T09:15:15+00:00",
      "arxiv_id": "2508.05179v1",
      "url": "http://arxiv.org/pdf/2508.05179v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Posterior-GRPO: Rewarding Reasoning Processes in Code Generation",
      "abstract": "Reinforcement learning (RL) has significantly advanced code generation for\nlarge language models (LLMs). However, current paradigms rely on outcome-based\nrewards from test cases, neglecting the quality of the intermediate reasoning\nprocess. While supervising the reasoning process directly is a promising\ndirection, it is highly susceptible to reward hacking, where the policy model\nlearns to exploit the reasoning reward signal without improving final outcomes.\nTo address this, we introduce a unified framework that can effectively\nincorporate the quality of the reasoning process during RL. First, to enable\nreasoning evaluation, we develop LCB-RB, a benchmark comprising preference\npairs of superior and inferior reasoning processes. Second, to accurately score\nreasoning quality, we introduce an Optimized-Degraded based (OD-based) method\nfor reward model training. This method generates high-quality preference pairs\nby systematically optimizing and degrading initial reasoning paths along\ncurated dimensions of reasoning quality, such as factual accuracy, logical\nrigor, and coherence. A 7B parameter reward model with this method achieves\nstate-of-the-art (SOTA) performance on LCB-RB and generalizes well to other\nbenchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method\nthat conditions process-based rewards on task success. By selectively applying\nrewards to the reasoning processes of only successful outcomes, P-GRPO\neffectively mitigates reward hacking and aligns the model's internal reasoning\nwith final code correctness. A 7B parameter model with P-GRPO achieves superior\nperformance across diverse code generation tasks, outperforming outcome-only\nbaselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further\ndemonstrate the generalizability of our approach by extending it to\nmathematical tasks. Our models, dataset, and code are publicly available.",
      "authors": [
        "Lishui Fan",
        "Yu Zhang",
        "Mouxiang Chen",
        "Zhongxin Liu"
      ],
      "published": "2025-08-07T09:04:10+00:00",
      "updated": "2025-08-07T09:04:10+00:00",
      "arxiv_id": "2508.05170v1",
      "url": "http://arxiv.org/pdf/2508.05170v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems",
      "abstract": "Multimodal Large Language Models (MLLMs) are becoming integral to autonomous\ndriving (AD) systems due to their strong vision-language reasoning\ncapabilities. However, MLLMs are vulnerable to adversarial attacks,\nparticularly adversarial patch attacks, which can pose serious threats in\nreal-world scenarios. Existing patch-based attack methods are primarily\ndesigned for object detection models and perform poorly when transferred to\nMLLM-based systems due to the latter's complex architectures and reasoning\nabilities. To address these limitations, we propose PhysPatch, a physically\nrealizable and transferable adversarial patch framework tailored for MLLM-based\nAD systems. PhysPatch jointly optimizes patch location, shape, and content to\nenhance attack effectiveness and real-world applicability. It introduces a\nsemantic-based mask initialization strategy for realistic placement, an\nSVD-based local alignment loss with patch-guided crop-resize to improve\ntransferability, and a potential field-based mask refinement method. Extensive\nexperiments across open-source, commercial, and reasoning-capable MLLMs\ndemonstrate that PhysPatch significantly outperforms prior methods in steering\nMLLM-based AD systems toward target-aligned perception and planning outputs.\nMoreover, PhysPatch consistently places adversarial patches in physically\nfeasible regions of AD scenes, ensuring strong real-world applicability and\ndeployability.",
      "authors": [
        "Qi Guo",
        "Xiaojun Jia",
        "Shanmin Pang",
        "Simeng Qin",
        "Lin Wang",
        "Ju Jia",
        "Yang Liu",
        "Qing Guo"
      ],
      "published": "2025-08-07T08:54:54+00:00",
      "updated": "2025-08-07T08:54:54+00:00",
      "arxiv_id": "2508.05167v1",
      "url": "http://arxiv.org/pdf/2508.05167v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models",
      "abstract": "Aligning LLMs with user preferences is crucial for real-world use but often\nrequires costly fine-tuning or expensive inference, forcing trade-offs between\nalignment quality and computational cost. Existing inference-time methods\ntypically ignore this balance, focusing solely on the optimized policy's\nperformance. We propose HIA (Heuristic-Guided Inference-time Alignment), a\ntuning-free, black-box-compatible approach that uses a lightweight prompt\noptimizer, heuristic reward models, and two-stage filtering to reduce inference\ncalls while preserving alignment quality. On real-world prompt datasets,\nHelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and\ngreedy search baselines in multi-objective, goal-conditioned tasks under the\nsame inference budget. We also find that HIA is effective under low-inference\nbudgets with as little as one or two response queries, offering a practical\nsolution for scalable, personalized LLM deployment.",
      "authors": [
        "Mason Nakamura",
        "Saaduddin Mahmud",
        "Kyle H. Wray",
        "Hamed Zamani",
        "Shlomo Zilberstein"
      ],
      "published": "2025-08-07T08:54:27+00:00",
      "updated": "2025-08-07T08:54:27+00:00",
      "arxiv_id": "2508.05165v1",
      "url": "http://arxiv.org/pdf/2508.05165v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "I.2.7; I.2.6; I.2.8"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Tool Graph Retriever: Exploring Dependency Graph-based Tool Retrieval for Large Language Models",
      "abstract": "With the remarkable advancement of AI agents, the number of their equipped\ntools is increasing rapidly. However, integrating all tool information into the\nlimited model context becomes impractical, highlighting the need for efficient\ntool retrieval methods. In this regard, dominant methods primarily rely on\nsemantic similarities between tool descriptions and user queries to retrieve\nrelevant tools. However, they often consider each tool independently,\noverlooking dependencies between tools, which may lead to the omission of\nprerequisite tools for successful task execution. To deal with this defect, in\nthis paper, we propose Tool Graph Retriever (TGR), which exploits the\ndependencies among tools to learn better tool representations for retrieval.\nFirst, we construct a dataset termed TDI300K to train a discriminator for\nidentifying tool dependencies. Then, we represent all candidate tools as a tool\ndependency graph and use graph convolution to integrate the dependencies into\ntheir representations. Finally, these updated tool representations are employed\nfor online retrieval. Experimental results on several commonly used datasets\nshow that our TGR can bring a performance improvement to existing dominant\nmethods, achieving SOTA performance. Moreover, in-depth analyses also verify\nthe importance of tool dependencies and the effectiveness of our TGR.",
      "authors": [
        "Linfeng Gao",
        "Yaoxiang Wang",
        "Minlong Peng",
        "Jialong Tang",
        "Yuzhe Shang",
        "Mingming Sun",
        "Jinsong Su"
      ],
      "published": "2025-08-07T08:36:26+00:00",
      "updated": "2025-08-07T08:36:26+00:00",
      "arxiv_id": "2508.05152v1",
      "url": "http://arxiv.org/pdf/2508.05152v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Speech LLMs in Low-Resource Scenarios: Data Volume Requirements and the Impact of Pretraining on High-Resource Languages",
      "abstract": "Large language models (LLMs) have demonstrated potential in handling spoken\ninputs for high-resource languages, reaching state-of-the-art performance in\nvarious tasks. However, their applicability is still less explored in\nlow-resource settings. This work investigates the use of Speech LLMs for\nlow-resource Automatic Speech Recognition using the SLAM-ASR framework, where a\ntrainable lightweight projector connects a speech encoder and a LLM. Firstly,\nwe assess training data volume requirements to match Whisper-only performance,\nre-emphasizing the challenges of limited data. Secondly, we show that\nleveraging mono- or multilingual projectors pretrained on high-resource\nlanguages reduces the impact of data scarcity, especially with small training\nsets. Using multilingual LLMs (EuroLLM, Salamandra) with\nwhisper-large-v3-turbo, we evaluate performance on several public benchmarks,\nproviding insights for future research on optimizing Speech LLMs for\nlow-resource languages and multilinguality.",
      "authors": [
        "Seraphina Fong",
        "Marco Matassoni",
        "Alessio Brutti"
      ],
      "published": "2025-08-07T08:33:42+00:00",
      "updated": "2025-08-07T08:33:42+00:00",
      "arxiv_id": "2508.05149v1",
      "url": "http://arxiv.org/pdf/2508.05149v1",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "eess.AS"
    },
    {
      "title": "Towards Assessing Medical Ethics from Knowledge to Practice",
      "abstract": "The integration of large language models into healthcare necessitates a\nrigorous evaluation of their ethical reasoning, an area current benchmarks\noften overlook. We introduce PrinciplismQA, a comprehensive benchmark with\n3,648 questions designed to systematically assess LLMs' alignment with core\nmedical ethics. Grounded in Principlism, our benchmark features a high-quality\ndataset. This includes multiple-choice questions curated from authoritative\ntextbooks and open-ended questions sourced from authoritative medical ethics\ncase study literature, all validated by medical experts. Our experiments reveal\na significant gap between models' ethical knowledge and their practical\napplication, especially in dynamically applying ethical principles to\nreal-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence,\noften over-emphasizing other principles. Frontier closed-source models, driven\nby strong general capabilities, currently lead the benchmark. Notably, medical\ndomain fine-tuning can enhance models' overall ethical competence, but further\nprogress requires better alignment with medical ethical knowledge.\nPrinciplismQA offers a scalable framework to diagnose these specific ethical\nweaknesses, paving the way for more balanced and responsible medical AI.",
      "authors": [
        "Chang Hong",
        "Minghao Wu",
        "Qingying Xiao",
        "Yuchi Wang",
        "Xiang Wan",
        "Guangjun Yu",
        "Benyou Wang",
        "Yan Hu"
      ],
      "published": "2025-08-07T08:10:14+00:00",
      "updated": "2025-08-07T08:10:14+00:00",
      "arxiv_id": "2508.05132v1",
      "url": "http://arxiv.org/pdf/2508.05132v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning",
      "abstract": "With the rapid and continuous increase in academic publications, identifying\nhigh-quality research has become an increasingly pressing challenge. While\nrecent methods leveraging Large Language Models (LLMs) for automated paper\nevaluation have shown great promise, they are often constrained by outdated\ndomain knowledge and limited reasoning capabilities. In this work, we present\nPaperEval, a novel LLM-based framework for automated paper evaluation that\naddresses these limitations through two key components: 1) a domain-aware paper\nretrieval module that retrieves relevant concurrent work to support\ncontextualized assessments of novelty and contributions, and 2) a latent\nreasoning mechanism that enables deep understanding of complex motivations and\nmethodologies, along with comprehensive comparison against concurrently related\nwork, to support more accurate and reliable evaluation. To guide the reasoning\nprocess, we introduce a progressive ranking optimization strategy that\nencourages the LLM to iteratively refine its predictions with an emphasis on\nrelative comparison. Experiments on two datasets demonstrate that PaperEval\nconsistently outperforms existing methods in both academic impact and paper\nquality evaluation. In addition, we deploy PaperEval in a real-world paper\nrecommendation system for filtering high-quality papers, which has gained\nstrong engagement on social media -- amassing over 8,000 subscribers and\nattracting over 10,000 views for many filtered high-quality papers --\ndemonstrating the practical effectiveness of PaperEval.",
      "authors": [
        "Wuqiang Zheng",
        "Yiyan Xu",
        "Xinyu Lin",
        "Chongming Gao",
        "Wenjie Wang",
        "Fuli Feng"
      ],
      "published": "2025-08-07T08:08:13+00:00",
      "updated": "2025-08-07T08:08:13+00:00",
      "arxiv_id": "2508.05129v1",
      "url": "http://arxiv.org/pdf/2508.05129v1",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Attention Basin: Why Contextual Position Matters in Large Language Models",
      "abstract": "The performance of Large Language Models (LLMs) is significantly sensitive to\nthe contextual position of information in the input. To investigate the\nmechanism behind this positional bias, our extensive experiments reveal a\nconsistent phenomenon we term the attention basin: when presented with a\nsequence of structured items (e.g., retrieved documents or few-shot examples),\nmodels systematically assign higher attention to the items at the beginning and\nend of the sequence, while neglecting those in the middle. Crucially, our\nanalysis further reveals that allocating higher attention to critical\ninformation is key to enhancing model performance. Based on these insights, we\nintroduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i)\nestimates a model's intrinsic positional attention preferences using a small\ncalibration set, and (ii) reorders retrieved documents or few-shot examples to\nalign the most salient content with these high-attention positions. AttnRank is\na model-agnostic, training-free, and plug-and-play method with minimal\ncomputational overhead. Experiments on multi-hop QA and few-shot in-context\nlearning tasks demonstrate that AttnRank achieves substantial improvements\nacross 10 large language models of varying architectures and scales, without\nmodifying model parameters or training procedures.",
      "authors": [
        "Zihao Yi",
        "Delong Zeng",
        "Zhenqing Ling",
        "Haohao Luo",
        "Zhe Xu",
        "Wei Liu",
        "Jian Luan",
        "Wanxia Cao",
        "Ying Shen"
      ],
      "published": "2025-08-07T08:08:08+00:00",
      "updated": "2025-08-07T08:08:08+00:00",
      "arxiv_id": "2508.05128v1",
      "url": "http://arxiv.org/pdf/2508.05128v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Exploring Superior Function Calls via Reinforcement Learning",
      "abstract": "Function calling capabilities are crucial for deploying Large Language Models\nin real-world applications, yet current training approaches fail to develop\nrobust reasoning strategies. Supervised fine-tuning produces models that rely\non superficial pattern matching, while standard reinforcement learning methods\nstruggle with the complex action space of structured function calls. We present\na novel reinforcement learning framework designed to enhance group relative\npolicy optimization through strategic entropy based exploration specifically\ntailored for function calling tasks. Our approach addresses three critical\nchallenges in function calling: insufficient exploration during policy\nlearning, lack of structured reasoning in chain-of-thought generation, and\ninadequate verification of parameter extraction. Our two-stage data preparation\npipeline ensures high-quality training samples through iterative LLM evaluation\nand abstract syntax tree validation. Extensive experiments on the Berkeley\nFunction Calling Leaderboard demonstrate that this framework achieves\nstate-of-the-art performance among open-source models with 86.02\\% overall\naccuracy, outperforming standard GRPO by up to 6\\% on complex multi-function\nscenarios. Notably, our method shows particularly strong improvements on\ncode-pretrained models, suggesting that structured language generation\ncapabilities provide an advantageous starting point for reinforcement learning\nin function calling tasks. We will release all the code, models and dataset to\nbenefit the community.",
      "authors": [
        "Bingguang Hao",
        "Maolin Wang",
        "Zengzhuang Xu",
        "Yicheng Chen",
        "Cunyin Peng",
        "Jinjie GU",
        "Chenyi Zhuang"
      ],
      "published": "2025-08-07T07:51:38+00:00",
      "updated": "2025-08-07T07:51:38+00:00",
      "arxiv_id": "2508.05118v1",
      "url": "http://arxiv.org/pdf/2508.05118v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Exploring Superior Function Calls via Reinforcement Learning",
      "abstract": "Function calling capabilities are crucial for deploying Large Language Models\nin real-world applications, yet current training approaches fail to develop\nrobust reasoning strategies. Supervised fine-tuning produces models that rely\non superficial pattern matching, while standard reinforcement learning methods\nstruggle with the complex action space of structured function calls. We present\na novel reinforcement learning framework designed to enhance group relative\npolicy optimization through strategic entropy based exploration specifically\ntailored for function calling tasks. Our approach addresses three critical\nchallenges in function calling: insufficient exploration during policy\nlearning, lack of structured reasoning in chain-of-thought generation, and\ninadequate verification of parameter extraction. Our two-stage data preparation\npipeline ensures high-quality training samples through iterative LLM evaluation\nand abstract syntax tree validation. Extensive experiments on the Berkeley\nFunction Calling Leaderboard demonstrate that this framework achieves\nstate-of-the-art performance among open-source models with 86.02\\% overall\naccuracy, outperforming standard GRPO by up to 6\\% on complex multi-function\nscenarios. Notably, our method shows particularly strong improvements on\ncode-pretrained models, suggesting that structured language generation\ncapabilities provide an advantageous starting point for reinforcement learning\nin function calling tasks. We will release all the code, models and dataset to\nbenefit the community.",
      "authors": [
        "Bingguang Hao",
        "Maolin Wang",
        "Zengzhuang Xu",
        "Yicheng Chen",
        "Cunyin Peng",
        "Jinjie GU",
        "Chenyi Zhuang"
      ],
      "published": "2025-08-07T07:51:38+00:00",
      "updated": "2025-08-08T01:50:45+00:00",
      "arxiv_id": "2508.05118v2",
      "url": "http://arxiv.org/pdf/2508.05118v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures",
      "abstract": "Generative AI is no longer a peripheral tool in higher education. It is\nrapidly evolving into a general-purpose infrastructure that reshapes how\nknowledge is generated, mediated, and validated. This paper presents findings\nfrom a controlled experiment evaluating a Socratic AI Tutor, a large language\nmodel designed to scaffold student research question development through\nstructured dialogue grounded in constructivist theory. Conducted with 65\npre-service teacher students in Germany, the study compares interaction with\nthe Socratic Tutor to engagement with an uninstructed AI chatbot. Students\nusing the Socratic Tutor reported significantly greater support for critical,\nindependent, and reflective thinking, suggesting that dialogic AI can stimulate\nmetacognitive engagement and challenging recent narratives of de-skilling due\nto generative AI usage. These findings serve as a proof of concept for a\nbroader pedagogical shift: the use of multi-agent systems (MAS) composed of\nspecialised AI agents. To conceptualise this, we introduce the notion of\norchestrated MAS, modular, pedagogically aligned agent constellations, curated\nby educators, that support diverse learning trajectories through differentiated\nroles and coordinated interaction. To anchor this shift, we propose an adapted\noffer-and-use model, in which students appropriate instructional offers from\nthese agents. Beyond technical feasibility, we examine system-level\nimplications for higher education institutions and students, including funding\nnecessities, changes to faculty roles, curriculars, competencies and assessment\npractices. We conclude with a comparative cost-effectiveness analysis\nhighlighting the scalability of such systems. In sum, this study contributes\nboth empirical evidence and a conceptual roadmap for hybrid learning ecosystems\nthat embed human-AI co-agency and pedagogical alignment.",
      "authors": [
        "Peer-Benedikt Degen",
        "Igor Asanov"
      ],
      "published": "2025-08-07T07:49:03+00:00",
      "updated": "2025-08-07T07:49:03+00:00",
      "arxiv_id": "2508.05116v1",
      "url": "http://arxiv.org/pdf/2508.05116v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search",
      "abstract": "Analog circuit design is a time-consuming, experience-driven task in chip\ndevelopment. Despite advances in AI, developing universal, fast, and stable\ngate sizing methods for analog circuits remains a significant challenge. Recent\napproaches combine Large Language Models (LLMs) with heuristic search\ntechniques to enhance generalizability, but they often depend on large model\nsizes and lack portability across different technology nodes. To overcome these\nlimitations, we propose EasySize, the first lightweight gate sizing framework\nbased on a finetuned Qwen3-8B model, designed for universal applicability\nacross process nodes, design specifications, and circuit topologies. EasySize\nexploits the varying Ease of Attainability (EOA) of performance metrics to\ndynamically construct task-specific loss functions, enabling efficient\nheuristic search through global Differential Evolution (DE) and local Particle\nSwarm Optimization (PSO) within a feedback-enhanced flow. Although finetuned\nsolely on 350nm node data, EasySize achieves strong performance on 5\noperational amplifier (Op-Amp) netlists across 180nm, 45nm, and 22nm technology\nnodes without additional targeted training, and outperforms AutoCkt, a\nwidely-used Reinforcement Learning based sizing framework, on 86.67\\% of tasks\nwith more than 96.67\\% of simulation resources reduction. We argue that\nEasySize can significantly reduce the reliance on human expertise and\ncomputational resources in gate sizing, thereby accelerating and simplifying\nthe analog circuit design process. EasySize will be open-sourced at a later\ndate.",
      "authors": [
        "Xinyue Wu",
        "Fan Hu",
        "Shaik Jani Babu",
        "Yi Zhao",
        "Xinfei Guo"
      ],
      "published": "2025-08-07T07:47:07+00:00",
      "updated": "2025-08-07T07:47:07+00:00",
      "arxiv_id": "2508.05113v1",
      "url": "http://arxiv.org/pdf/2508.05113v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation",
      "abstract": "With the rapid advancement of large language models (LLMs),\nretrieval-augmented generation (RAG) has emerged as a critical approach to\nsupplement the inherent knowledge limitations of LLMs. However, due to the\ntypically large volume of retrieved information, RAG tends to operate with long\ncontext lengths. From the perspective of entropy engineering, we identify\nunconstrained entropy growth and attention dilution due to long retrieval\ncontext as significant factors affecting RAG performance. In this paper, we\npropose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves\nthe adaptability of RAG systems to varying context lengths through the\nprinciple of entropy invariance. By leveraging balanced context entropy to\nreformulate attention dynamics, BEE-RAG separates attention sensitivity from\ncontext length, ensuring a stable entropy level. Building upon this, we\nintroduce a zero-shot inference strategy for multi-importance estimation and a\nparameter-efficient adaptive fine-tuning mechanism to obtain the optimal\nbalancing factor for different settings. Extensive experiments across multiple\nRAG tasks demonstrate the effectiveness of BEE-RAG.",
      "authors": [
        "Yuhao Wang",
        "Ruiyang Ren",
        "Yucheng Wang",
        "Jing Liu",
        "Wayne Xin Zhao",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "published": "2025-08-07T07:37:25+00:00",
      "updated": "2025-08-07T07:37:25+00:00",
      "arxiv_id": "2508.05100v1",
      "url": "http://arxiv.org/pdf/2508.05100v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning",
      "abstract": "Precise, correct feedback is crucial for effectively training large language\nmodels (LLMs) in code reinforcement learning. However, synthesizing\nhigh-quality test cases remains a profoundly challenging and unsolved problem.\nIn this work, we present Klear-CodeTest, a comprehensive test case synthesis\nframework featuring rigorous verification to ensure quality and reliability of\ntest cases. Our approach achieves broad coverage of programming problems via a\nnovel Generator-Validation (G-V) framework, ensuring correctness through a\nconsistency validation mechanism that verifies outputs against gold solutions.\nThe proposed G-V framework generates comprehensive test cases including both\nregular and corner cases, enhancing test coverage and discriminative power for\nsolution correctness assessment in code reinforcement learning. In addition, we\ndesign a multi-layered security sandbox system optimized for online\nverification platforms, guaranteeing safe and reliable code execution. Through\ncomprehensive experiments, we demonstrate the effectiveness of our curated\ndataset, showing significant improvements in model performance and training\nstability. The source codes, curated dataset and sandbox system are available\nat: https://github.com/Kwai-Klear/CodeTest.",
      "authors": [
        "Jia Fu",
        "Xinyu Yang",
        "Hongzhi Zhang",
        "Yahui Liu",
        "Jingyuan Zhang",
        "Qi Wang",
        "Fuzheng Zhang",
        "Guorui Zhou"
      ],
      "published": "2025-08-07T07:36:01+00:00",
      "updated": "2025-08-07T07:36:01+00:00",
      "arxiv_id": "2508.05710v1",
      "url": "http://arxiv.org/pdf/2508.05710v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation",
      "abstract": "User feedback is critical for refining recommendation systems, yet explicit\nfeedback (e.g., likes or dislikes) remains scarce in practice. As a more\nfeasible alternative, inferring user preferences from massive implicit feedback\nhas shown great potential (e.g., a user quickly skipping a recommended video\nusually indicates disinterest). Unfortunately, implicit feedback is often\nnoisy: a user might skip a video due to accidental clicks or other reasons,\nrather than disliking it. Such noise can easily misjudge user interests,\nthereby undermining recommendation performance. To address this issue, we\npropose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which\nleverages contextual guidance from relevant user groups, enabling robust and\nin-depth interpretation of implicit feedback for individual users.\nSpecifically, G-UBS operates via two key agents. First, the User Group Manager\n(UGM) effectively clusters users to generate group profiles utilizing a\n``summarize-cluster-reflect\" workflow based on LLMs. Second, the User Feedback\nModeler (UFM) employs an innovative group-aware reinforcement learning\napproach, where each user is guided by the associated group profiles during the\nreinforcement learning process, allowing UFM to robustly and deeply examine the\nreasons behind implicit feedback. To assess our G-UBS paradigm, we have\nconstructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To\nthe best of our knowledge, this is the first multi-modal benchmark for implicit\nfeedback evaluation in video recommendation, encompassing 15k users, 25k\nvideos, and 933k interaction records with implicit feedback. Extensive\nexperiments on IF-VR demonstrate that G-UBS significantly outperforms\nmainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a\nplay rate > 30% and 14.9% higher reasoning accuracy on IF-VR.",
      "authors": [
        "Boyu Chen",
        "Siran Chen",
        "Zhengrong Yue",
        "Kainan Yan",
        "Chenyun Yu",
        "Beibei Kong",
        "Cheng Lei",
        "Chengxiang Zhuo",
        "Zang Li",
        "Yali Wang"
      ],
      "published": "2025-08-07T07:26:08+00:00",
      "updated": "2025-08-07T07:26:08+00:00",
      "arxiv_id": "2508.05709v1",
      "url": "http://arxiv.org/pdf/2508.05709v1",
      "categories": [
        "cs.IR",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering",
      "abstract": "Jailbreak attacks against multimodal large language Models (MLLMs) are a\nsignificant research focus. Current research predominantly focuses on\nmaximizing attack success rate (ASR), often overlooking whether the generated\nresponses actually fulfill the attacker's malicious intent. This oversight\nfrequently leads to low-quality outputs that bypass safety filters but lack\nsubstantial harmful content. To address this gap, we propose JPS,\n\\underline{J}ailbreak MLLMs with collaborative visual \\underline{P}erturbation\nand textual \\underline{S}teering, which achieves jailbreaks via corporation of\nvisual image and textually steering prompt. Specifically, JPS utilizes\ntarget-guided adversarial image perturbations for effective safety bypass,\ncomplemented by \"steering prompt\" optimized via a multi-agent system to\nspecifically guide LLM responses fulfilling the attackers' intent. These visual\nand textual components undergo iterative co-optimization for enhanced\nperformance. To evaluate the quality of attack outcomes, we propose the\nMalicious Intent Fulfillment Rate (MIFR) metric, assessed using a\nReasoning-LLM-based evaluator. Our experiments show JPS sets a new\nstate-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with\nanalyses confirming its efficacy. Codes are available at\n\\href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}.\n\\color{warningcolor}{Warning: This paper contains potentially sensitive\ncontents.}",
      "authors": [
        "Renmiao Chen",
        "Shiyao Cui",
        "Xuancheng Huang",
        "Chengwei Pan",
        "Victor Shea-Jay Huang",
        "QingLin Zhang",
        "Xuan Ouyang",
        "Zhexin Zhang",
        "Hongning Wang",
        "Minlie Huang"
      ],
      "published": "2025-08-07T07:14:01+00:00",
      "updated": "2025-08-07T07:14:01+00:00",
      "arxiv_id": "2508.05087v1",
      "url": "http://arxiv.org/pdf/2508.05087v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "I.2.7; K.4.1; K.6.5"
      ],
      "primary_category": "cs.MM"
    },
    {
      "title": "MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models",
      "abstract": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly improved medical AI, enabling it to unify the understanding of\nvisual and textual information. However, as medical knowledge continues to\nevolve, it is critical to allow these models to efficiently update outdated or\nincorrect information without retraining from scratch. Although textual\nknowledge editing has been widely studied, there is still a lack of systematic\nbenchmarks for multimodal medical knowledge editing involving image and text\nmodalities. To fill this gap, we present MedMKEB, the first comprehensive\nbenchmark designed to evaluate the reliability, generality, locality,\nportability, and robustness of knowledge editing in medical multimodal large\nlanguage models. MedMKEB is built on a high-quality medical visual\nquestion-answering dataset and enriched with carefully constructed editing\ntasks, including counterfactual correction, semantic generalization, knowledge\ntransfer, and adversarial robustness. We incorporate human expert validation to\nensure the accuracy and reliability of the benchmark. Extensive single editing\nand sequential editing experiments on state-of-the-art general and medical\nMLLMs demonstrate the limitations of existing knowledge-based editing\napproaches in medicine, highlighting the need to develop specialized editing\nstrategies. MedMKEB will serve as a standard benchmark to promote the\ndevelopment of trustworthy and efficient medical knowledge editing algorithms.",
      "authors": [
        "Dexuan Xu",
        "Jieyi Wang",
        "Zhongyan Chai",
        "Yongzhi Cao",
        "Hanpin Wang",
        "Huamin Zhang",
        "Yu Huang"
      ],
      "published": "2025-08-07T07:09:26+00:00",
      "updated": "2025-08-07T07:09:26+00:00",
      "arxiv_id": "2508.05083v1",
      "url": "http://arxiv.org/pdf/2508.05083v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning",
      "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large\nLanguage Models (LLMs). In practice, LLMs are often required to handle a\ndiverse set of tasks from multiple domains, a scenario naturally addressed by\nmulti-task learning (MTL). Within this MTL context, a prevailing trend involves\nLoRA variants with multiple adapters or heads, which advocate for structural\ndiversity to capture task-specific knowledge. Our findings present a direct\nchallenge to this paradigm. We first show that a simplified multi-head\narchitecture with high inter-head similarity substantially outperforms complex\nmulti-adapter and multi-head systems. This leads us to question the\nmulti-component paradigm itself, and we further demonstrate that a standard\nsingle-adapter LoRA, with a sufficiently increased rank, also achieves highly\ncompetitive performance. These results lead us to a new hypothesis: effective\nMTL generalization hinges on learning robust shared representations, not\nisolating task-specific features. To validate this, we propose Align-LoRA,\nwhich incorporates an explicit loss to align task representations within the\nshared adapter space. Experiments confirm that Align-LoRA significantly\nsurpasses all baselines, establishing a simpler yet more effective paradigm for\nadapting LLMs to multiple tasks. The code is available at\nhttps://github.com/jinda-liu/Align-LoRA.",
      "authors": [
        "Jinda Liu",
        "Bo Cheng",
        "Yi Chang",
        "Yuan Wu"
      ],
      "published": "2025-08-07T07:02:55+00:00",
      "updated": "2025-08-07T07:02:55+00:00",
      "arxiv_id": "2508.05078v1",
      "url": "http://arxiv.org/pdf/2508.05078v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding",
      "abstract": "Gaussian Splatting has rapidly emerged as a transformative technique for\nreal-time 3D scene representation, offering a highly efficient and expressive\nalternative to Neural Radiance Fields (NeRF). Its ability to render complex\nscenes with high fidelity has enabled progress across domains such as scene\nreconstruction, robotics, and interactive content creation. More recently, the\nintegration of Large Language Models (LLMs) and language embeddings into\nGaussian Splatting pipelines has opened new possibilities for text-conditioned\ngeneration, editing, and semantic scene understanding. Despite these advances,\na comprehensive overview of this emerging intersection has been lacking. This\nsurvey presents a structured review of current research efforts that combine\nlanguage guidance with 3D Gaussian Splatting, detailing theoretical\nfoundations, integration strategies, and real-world use cases. We highlight key\nlimitations such as computational bottlenecks, generalizability, and the\nscarcity of semantically annotated 3D Gaussian data and outline open challenges\nand future directions for advancing language-guided 3D scene understanding\nusing Gaussian Splatting.",
      "authors": [
        "Mahmoud Chick Zaouali",
        "Todd Charter",
        "Yehor Karpichev",
        "Brandon Haworth",
        "Homayoun Najjjaran"
      ],
      "published": "2025-08-07T06:33:08+00:00",
      "updated": "2025-08-07T06:33:08+00:00",
      "arxiv_id": "2508.05064v1",
      "url": "http://arxiv.org/pdf/2508.05064v1",
      "categories": [
        "cs.GR",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.GR"
    },
    {
      "title": "Finding Needles in Images: Can Multimodal LLMs Locate Fine Details?",
      "abstract": "While Multi-modal Large Language Models (MLLMs) have shown impressive\ncapabilities in document understanding tasks, their ability to locate and\nreason about fine-grained details within complex documents remains\nunderstudied. Consider searching a restaurant menu for a specific nutritional\ndetail or identifying a disclaimer in a lengthy newspaper article tasks that\ndemand careful attention to small but significant details within a broader\nnarrative, akin to Finding Needles in Images (NiM). To address this gap, we\nintroduce NiM, a carefully curated benchmark spanning diverse real-world\ndocuments including newspapers, menus, and lecture images, specifically\ndesigned to evaluate MLLMs' capability in these intricate tasks. Building on\nthis, we further propose Spot-IT, a simple yet effective approach that enhances\nMLLMs capability through intelligent patch selection and Gaussian attention,\nmotivated from how humans zoom and focus when searching documents. Our\nextensive experiments reveal both the capabilities and limitations of current\nMLLMs in handling fine-grained document understanding tasks, while\ndemonstrating the effectiveness of our approach. Spot-IT achieves significant\nimprovements over baseline methods, particularly in scenarios requiring precise\ndetail extraction from complex layouts.",
      "authors": [
        "Parth Thakkar",
        "Ankush Agarwal",
        "Prasad Kasu",
        "Pulkit Bansal",
        "Chaitanya Devaguptapu"
      ],
      "published": "2025-08-07T06:10:15+00:00",
      "updated": "2025-08-07T06:10:15+00:00",
      "arxiv_id": "2508.05053v1",
      "url": "http://arxiv.org/pdf/2508.05053v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Evaluation of LLMs in AMR Parsing",
      "abstract": "Meaning Representation (AMR) is a semantic formalism that encodes sentence\nmeaning as rooted, directed, acyclic graphs, where nodes represent concepts and\nedges denote semantic relations. Finetuning decoder only Large Language Models\n(LLMs) represent a promising novel straightfoward direction for AMR parsing.\nThis paper presents a comprehensive evaluation of finetuning four distinct LLM\narchitectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled\nusing the LDC2020T02 Gold AMR3.0 test set. Our results have shown that\nstraightfoward finetuning of decoder only LLMs can achieve comparable\nperformance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2\ndemonstrates competitive performance against SOTA AMR parsers given a\nstraightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full\nLDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching\nGraphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a\nconsistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5\nexcels in structural validity.",
      "authors": [
        "Shu Han Ho"
      ],
      "published": "2025-08-07T04:43:47+00:00",
      "updated": "2025-08-07T04:43:47+00:00",
      "arxiv_id": "2508.05028v1",
      "url": "http://arxiv.org/pdf/2508.05028v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Evaluation of LLMs in AMR Parsing",
      "abstract": "AMR (Abstract Meaning Representation) is a semantic formalism that encodes\nsentence meaning as rooted, directed, acyclic graphs, where nodes represent\nconcepts and edges denote semantic relations. Finetuning decoder only Large\nLanguage Models (LLMs) represent a promising novel straightfoward direction for\nAMR parsing. This paper presents a comprehensive evaluation of finetuning four\ndistinct LLM architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA\nDistilled using the LDC2020T02 Gold AMR3.0 test set. Our results have shown\nthat straightfoward finetuning of decoder only LLMs can achieve comparable\nperformance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2\ndemonstrates competitive performance against SOTA AMR parsers given a\nstraightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full\nLDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching\nGraphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a\nconsistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5\nexcels in structural validity.",
      "authors": [
        "Shu Han Ho"
      ],
      "published": "2025-08-07T04:43:47+00:00",
      "updated": "2025-08-08T02:47:19+00:00",
      "arxiv_id": "2508.05028v2",
      "url": "http://arxiv.org/pdf/2508.05028v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models",
      "abstract": "Large language models (LLMs) have shown strong reasoning capabilities when\nfine-tuned with reinforcement learning (RL). However, such methods require\nextensive data and compute, making them impractical for smaller models. Current\napproaches to curriculum learning or data selection are largely\nheuristic-driven or demand extensive computational resources, limiting their\nscalability and generalizability. We propose \\textbf{SPaRFT}, a self-paced\nlearning framework that enables efficient learning based on the capability of\nthe model being trained through optimizing which data to use and when. First,\nwe apply \\emph{cluster-based data reduction} to partition training data by\nsemantics and difficulty, extracting a compact yet diverse subset that reduces\nredundancy. Then, a \\emph{multi-armed bandit} treats data clusters as arms,\noptimized to allocate training samples based on model current performance.\nExperiments across multiple reasoning benchmarks show that SPaRFT achieves\ncomparable or better accuracy than state-of-the-art baselines while using up to\n\\(100\\times\\) fewer samples. Ablation studies and analyses further highlight\nthe importance of both data clustering and adaptive selection. Our results\ndemonstrate that carefully curated, performance-driven training curricula can\nunlock strong reasoning abilities in LLMs with minimal resources.",
      "authors": [
        "Dai Do",
        "Manh Nguyen",
        "Svetha Venkatesh",
        "Hung Le"
      ],
      "published": "2025-08-07T03:50:48+00:00",
      "updated": "2025-08-07T03:50:48+00:00",
      "arxiv_id": "2508.05015v1",
      "url": "http://arxiv.org/pdf/2508.05015v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines",
      "abstract": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion.",
      "authors": [
        "Ugur Cetintemel",
        "Shu Chen",
        "Alexander W. Lee",
        "Deepti Raghavan"
      ],
      "published": "2025-08-07T03:49:56+00:00",
      "updated": "2025-08-07T03:49:56+00:00",
      "arxiv_id": "2508.05012v1",
      "url": "http://arxiv.org/pdf/2508.05012v1",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.DB"
    },
    {
      "title": "Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses",
      "abstract": "We explore the application of large language models (LLMs) to empower domain\nexperts in integrating large, heterogeneous, and noisy urban spatial datasets.\nTraditional rule-based integration methods are unable to cover all edge cases,\nrequiring manual verification and repair. Machine learning approaches require\ncollecting and labeling of large numbers of task-specific samples. In this\nstudy, we investigate the potential of LLMs for spatial data integration. Our\nanalysis first considers how LLMs reason about environmental spatial\nrelationships mediated by human experience, such as between roads and\nsidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they\nstruggle to connect the macro-scale environment with the relevant computational\ngeometry tasks, often producing logically incoherent responses. But when\nprovided relevant features, thereby reducing dependence on spatial reasoning,\nLLMs are able to generate high-performing results. We then adapt a\nreview-and-refine method, which proves remarkably effective in correcting\nerroneous initial responses while preserving accurate responses. We discuss\npractical implications of employing LLMs for spatial data integration in\nreal-world contexts and outline future research directions, including\npost-training, multi-modal integration methods, and support for diverse data\nformats. Our findings position LLMs as a promising and flexible alternative to\ntraditional rule-based heuristics, advancing the capabilities of adaptive\nspatial data integration.",
      "authors": [
        "Bin Han",
        "Robert Wolfe",
        "Anat Caspi",
        "Bill Howe"
      ],
      "published": "2025-08-07T03:44:20+00:00",
      "updated": "2025-08-07T03:44:20+00:00",
      "arxiv_id": "2508.05009v1",
      "url": "http://arxiv.org/pdf/2508.05009v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic",
      "abstract": "We find ourselves in the midst of an explosion in artificial intelligence\nresearch, particularly with large language models (LLMs). These models have\ndiverse applications spanning finance, commonsense knowledge graphs, medicine,\nand visual analysis. In the world of Object-Oriented Programming(OOP), a robust\nbody of knowledge and methods has been developed for managing complex tasks\nthrough object-oriented thinking. However, the intersection of LLMs with OOP\nremains an underexplored territory. Empirically, we currently possess limited\nunderstanding of how LLMs can enhance the effectiveness of OOP learning and\ncode writing, as well as how we can evaluate such AI-powered tools. Our work\naims to address this gap by presenting a vision from the perspectives of key\nstakeholders involved in an OOP task: programmers, mariners, and experienced\nprogrammers. We identify critical junctures within typical coding workflows\nwhere the integration of LLMs can offer significant benefits. Furthermore, we\npropose ways to augment existing logical reasoning and code writing, ultimately\nenhancing the programming experience.",
      "authors": [
        "Gang Xu",
        "Airong Wang",
        "Yushan Pan"
      ],
      "published": "2025-08-07T03:38:17+00:00",
      "updated": "2025-08-07T03:38:17+00:00",
      "arxiv_id": "2508.05005v1",
      "url": "http://arxiv.org/pdf/2508.05005v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data",
      "abstract": "Self-evolving Large Language Models (LLMs) offer a scalable path toward\nsuper-intelligence by autonomously generating, refining, and learning from\ntheir own experiences. However, existing methods for training such models still\nrely heavily on vast human-curated tasks and labels, typically via fine-tuning\nor reinforcement learning, which poses a fundamental bottleneck to advancing AI\nsystems toward capabilities beyond human intelligence. To overcome this\nlimitation, we introduce R-Zero, a fully autonomous framework that generates\nits own training data from scratch. Starting from a single base LLM, R-Zero\ninitializes two independent models with distinct roles, a Challenger and a\nSolver. These models are optimized separately and co-evolve through\ninteraction: the Challenger is rewarded for proposing tasks near the edge of\nthe Solver capability, and the Solver is rewarded for solving increasingly\nchallenging tasks posed by the Challenger. This process yields a targeted,\nself-improving curriculum without any pre-existing tasks and labels.\nEmpirically, R-Zero substantially improves reasoning capability across\ndifferent backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on\nmath-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.",
      "authors": [
        "Chengsong Huang",
        "Wenhao Yu",
        "Xiaoyang Wang",
        "Hongming Zhang",
        "Zongxia Li",
        "Ruosen Li",
        "Jiaxin Huang",
        "Haitao Mi",
        "Dong Yu"
      ],
      "published": "2025-08-07T03:38:16+00:00",
      "updated": "2025-08-07T03:38:16+00:00",
      "arxiv_id": "2508.05004v1",
      "url": "http://arxiv.org/pdf/2508.05004v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health",
      "abstract": "Background: Understanding social determinants of health (SDoH) factors\ncontributing to suicide incidents is crucial for early intervention and\nprevention. However, data-driven approaches to this goal face challenges such\nas long-tailed factor distributions, analyzing pivotal stressors preceding\nsuicide incidents, and limited model explainability. Methods: We present a\nmulti-stage large language model framework to enhance SDoH factor extraction\nfrom unstructured text. Our approach was compared to other state-of-the-art\nlanguage models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning\nmodels (i.e., DeepSeek-R1). We also evaluated how the model's explanations help\npeople annotate SDoH factors more quickly and accurately. The analysis included\nboth automated comparisons and a pilot user study. Results: We show that our\nproposed framework demonstrated performance boosts in the overarching task of\nextracting SDoH factors and in the finer-grained tasks of retrieving relevant\ncontext. Additionally, we show that fine-tuning a smaller, task-specific model\nachieves comparable or better performance with reduced inference costs. The\nmulti-stage design not only enhances extraction but also provides intermediate\nexplanations, improving model explainability. Conclusions: Our approach\nimproves both the accuracy and transparency of extracting suicide-related SDoH\nfrom unstructured texts. These advancements have the potential to support early\nidentification of individuals at risk and inform more effective prevention\nstrategies.",
      "authors": [
        "Song Wang",
        "Yishu Wei",
        "Haotian Ma",
        "Max Lovitt",
        "Kelly Deng",
        "Yuan Meng",
        "Zihan Xu",
        "Jingze Zhang",
        "Yunyu Xiao",
        "Ying Ding",
        "Xuhai Xu",
        "Joydeep Ghosh",
        "Yifan Peng"
      ],
      "published": "2025-08-07T03:36:38+00:00",
      "updated": "2025-08-07T03:36:38+00:00",
      "arxiv_id": "2508.05003v1",
      "url": "http://arxiv.org/pdf/2508.05003v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge",
      "abstract": "Large Language Models (LLMs) such as ChatGPT have rendered visible the\nfragility of contemporary knowledge infrastructures by simulating coherence\nwhile bypassing traditional modes of citation, authority, and validation. This\npaper introduces the Situated Epistemic Infrastructures (SEI) framework as a\ndiagnostic tool for analyzing how knowledge becomes authoritative across hybrid\nhuman-machine systems under post-coherence conditions. Rather than relying on\nstable scholarly domains or bounded communities of practice, SEI traces how\ncredibility is mediated across institutional, computational, and temporal\narrangements. Integrating insights from infrastructure studies, platform\ntheory, and epistemology, the framework foregrounds coordination over\nclassification, emphasizing the need for anticipatory and adaptive models of\nepistemic stewardship. The paper contributes to debates on AI governance,\nknowledge production, and the ethical design of information systems by offering\na robust alternative to representationalist models of scholarly communication.",
      "authors": [
        "Matthew Kelly"
      ],
      "published": "2025-08-07T03:08:23+00:00",
      "updated": "2025-08-07T03:08:23+00:00",
      "arxiv_id": "2508.04995v1",
      "url": "http://arxiv.org/pdf/2508.04995v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.DL",
        "K.4.1; K.3; K.2"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Sentiment-Aware Stock Price Prediction with Transformer and LLM-Generated Formulaic Alpha",
      "abstract": "Traditionally, traders and quantitative analysts address alpha decay by\nmanually crafting formulaic alphas, mathematical expressions that identify\npatterns or signals in financial data, through domain expertise and\ntrial-and-error. This process is often time-consuming and difficult to scale.\nWith recent advances in large language models (LLMs), it is now possible to\nautomate the generation of such alphas by leveraging the reasoning capabilities\nof LLMs. This paper introduces a novel framework that integrates a prompt-based\nLLM with a Transformer model for stock price prediction. The LLM first\ngenerates diverse and adaptive alphas using structured inputs such as\nhistorical stock features (Close, Open, High, Low, Volume), technical\nindicators, sentiment scores of both target and related companies. These\nalphas, instead of being used directly for trading, are treated as high-level\nfeatures that capture complex dependencies within the financial data. To\nevaluate the effectiveness of these LLM-generated formulaic alphas, the alpha\nfeatures are then fed into prediction models such as Transformer, LSTM, TCN,\nSVR, and Random Forest to forecast future stock prices. Experimental results\ndemonstrate that the LLM-generated alphas significantly improve predictive\naccuracy. Moreover, the accompanying natural language reasoning provided by the\nLLM enhances the interpretability and transparency of the predictions,\nsupporting more informed financial decision-making.",
      "authors": [
        "Qizhao Chen",
        "Hiroaki Kawashima"
      ],
      "published": "2025-08-07T02:02:39+00:00",
      "updated": "2025-08-07T02:02:39+00:00",
      "arxiv_id": "2508.04975v1",
      "url": "http://arxiv.org/pdf/2508.04975v1",
      "categories": [
        "cs.CE"
      ],
      "primary_category": "cs.CE"
    },
    {
      "title": "A Metric for MLLM Alignment in Large-scale Recommendation",
      "abstract": "Multimodal recommendation has emerged as a critical technique in modern\nrecommender systems, leveraging content representations from advanced\nmultimodal large language models (MLLMs). To ensure these representations are\nwell-adapted, alignment with the recommender system is essential. However,\nevaluating the alignment of MLLMs for recommendation presents significant\nchallenges due to three key issues: (1) static benchmarks are inaccurate\nbecause of the dynamism in real-world applications, (2) evaluations with online\nsystem, while accurate, are prohibitively expensive at scale, and (3)\nconventional metrics fail to provide actionable insights when learned\nrepresentations underperform. To address these challenges, we propose the\nLeakage Impact Score (LIS), a novel metric for multimodal recommendation.\nRather than directly assessing MLLMs, LIS efficiently measures the upper bound\nof preference data. We also share practical insights on deploying MLLMs with\nLIS in real-world scenarios. Online A/B tests on both Content Feed and Display\nAds of Xiaohongshu's Explore Feed production demonstrate the effectiveness of\nour proposed method, showing significant improvements in user spent time and\nadvertiser value.",
      "authors": [
        "Yubin Zhang",
        "Yanhua Huang",
        "Haiming Xu",
        "Mingliang Qi",
        "Chang Wang",
        "Jiarui Jin",
        "Xiangyuan Ren",
        "Xiaodan Wang",
        "Ruiwen Xu"
      ],
      "published": "2025-08-07T01:21:51+00:00",
      "updated": "2025-08-07T01:21:51+00:00",
      "arxiv_id": "2508.04963v1",
      "url": "http://arxiv.org/pdf/2508.04963v1",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Semantic Reasoning Meets Numerical Precision: An LLM-Powered Multi-Agent System for Power Grid Control",
      "abstract": "The increasing penetration of Distributed Energy Resources (DERs), widespread\nadoption of Electric Vehicles (EVs), and the growing frequency of extreme\nweather events have significantly increased the complexity of power grid\nplanning, operation, and management. Traditional rule-based systems and\nnumerical optimization approaches often struggle with the scale, dynamics, and\nadaptability required by modern power networks. This paper introduces\nGrid-Agent, an autonomous, AI-driven framework that combines Large Language\nModels (LLMs) with multi-agent reinforcement learning to detect and remediate\ngrid violations in real time. Grid-Agent integrates semantic reasoning with\nnumerical precision through a modular agent architecture: a planning agent\ngenerates coordinated action sequences using numerical power flow solvers,\nwhile a validation agent evaluates system stability and action effectiveness\nvia sandboxed execution with safety rollbacks. To ensure scalability,\nGrid-Agent incorporates an adaptive multiscale network representation that\ndynamically selects optimal encoding schemes based on network size and\ncomplexity. The framework enables coordinated violation resolution through\noptimizing switch configurations, battery deployment, and load curtailment\nstrategies. Experimental results in standard IEEE and CIGRE test systems (IEEE\n69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation\nperformance. Additionally, the framework's built-in data collection and\nlearning capabilities enable continuous learning and adaptation to diverse\nnetwork topologies. The autonomous nature of the framework makes it\nparticularly suitable for modern smart grid applications requiring rapid\nresponse to dynamic operating conditions.",
      "authors": [
        "Yan Zhang"
      ],
      "published": "2025-08-07T01:10:28+00:00",
      "updated": "2025-08-07T01:10:28+00:00",
      "arxiv_id": "2508.05702v1",
      "url": "http://arxiv.org/pdf/2508.05702v1",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.MA"
    },
    {
      "title": "I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations",
      "abstract": "This paper introduces a comprehensive benchmark for evaluating how Large\nLanguage Models (LLMs) respond to linguistic shibboleths: subtle linguistic\nmarkers that can inadvertently reveal demographic attributes such as gender,\nsocial class, or regional background. Through carefully constructed interview\nsimulations using 100 validated question-response pairs, we demonstrate how\nLLMs systematically penalize certain linguistic patterns, particularly hedging\nlanguage, despite equivalent content quality. Our benchmark generates\ncontrolled linguistic variations that isolate specific phenomena while\nmaintaining semantic equivalence, which enables the precise measurement of\ndemographic bias in automated evaluation systems. We validate our approach\nalong multiple linguistic dimensions, showing that hedged responses receive\n25.6% lower ratings on average, and demonstrate the benchmark's effectiveness\nin identifying model-specific biases. This work establishes a foundational\nframework for detecting and measuring linguistic discrimination in AI systems,\nwith broad applications to fairness in automated decision-making contexts.",
      "authors": [
        "Julia Kharchenko",
        "Tanya Roosta",
        "Aman Chadha",
        "Chirag Shah"
      ],
      "published": "2025-08-06T23:51:03+00:00",
      "updated": "2025-08-06T23:51:03+00:00",
      "arxiv_id": "2508.04939v1",
      "url": "http://arxiv.org/pdf/2508.04939v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Root Cause Analysis Training for Healthcare Professionals With AI-Powered Virtual Simulation: A Proof-of-Concept",
      "abstract": "Root Cause Analysis (RCA) is a critical tool for investigating adverse events\nin healthcare and improving patient safety. However, existing RCA training\nprograms are often limited by high resource demands, leading to insufficient\ntraining and inconsistent implementation. To address this challenge, we present\nan AI-powered 3D simulation game that helps healthcare professionals develop\nRCA skills through interactive, immersive simulations. This approach offers a\ncost-effective, scalable, and accessible alternative to traditional training.\nThe prototype simulates an RCA investigation following a death in the ICU,\nwhere learners interview five virtual avatars representing ICU team members to\ninvestigate the incident and complete a written report. The system enables\nnatural, life-like interactions with avatars via large language models (LLMs),\nemotional text-to-speech, and AI-powered animations. An additional LLM\ncomponent provides formative and summative feedback to support continual\nimprovement. We conclude by outlining plans to empirically evaluate the\nsystem's efficacy.",
      "authors": [
        "Yuqi Hu",
        "Qiwen Xiong",
        "Zhenzhen Qin",
        "Brandon Watanabe",
        "Yujing Wang",
        "Mirjana Prpa",
        "Ilmi Yoon"
      ],
      "published": "2025-08-06T22:02:45+00:00",
      "updated": "2025-08-06T22:02:45+00:00",
      "arxiv_id": "2508.04904v1",
      "url": "http://arxiv.org/pdf/2508.04904v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory",
      "abstract": "Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems.",
      "authors": [
        "Jun Liu",
        "Zhenglun Kong",
        "Changdi Yang",
        "Fan Yang",
        "Tianqi Li",
        "Peiyan Dong",
        "Joannah Nanjekye",
        "Hao Tang",
        "Geng Yuan",
        "Wei Niu",
        "Wenbin Zhang",
        "Pu Zhao",
        "Xue Lin",
        "Dong Huang",
        "Yanzhi Wang"
      ],
      "published": "2025-08-06T21:59:34+00:00",
      "updated": "2025-08-06T21:59:34+00:00",
      "arxiv_id": "2508.04903v1",
      "url": "http://arxiv.org/pdf/2508.04903v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)",
      "abstract": "Large Language Models (LLMs) are increasingly integrated with\ngraph-structured data for tasks like node classification, a domain\ntraditionally dominated by Graph Neural Networks (GNNs). While this integration\nleverages rich relational information to improve task performance, their\nrobustness against adversarial attacks remains unexplored. We take the first\nstep to explore the vulnerabilities of graph-aware LLMs by leveraging existing\nadversarial attack methods tailored for graph-based models, including those for\npoisoning (training-time attacks) and evasion (test-time attacks), on two\nrepresentative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al.\n2024). Additionally, we discover a new attack surface for LLAGA where an\nattacker can inject malicious nodes as placeholders into the node sequence\ntemplate to severely degrade its performance. Our systematic analysis reveals\nthat certain design choices in graph encoding can enhance attack success, with\nspecific findings that: (1) the node sequence template in LLAGA increases its\nvulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater\nrobustness; and (3) both approaches remain susceptible to imperceptible feature\nperturbation attacks. Finally, we propose an end-to-end defense framework\nGALGUARD, that combines an LLM-based feature correction module to mitigate\nfeature-level perturbations and adapted GNN defenses to protect against\nstructural attacks.",
      "authors": [
        "Iyiola E. Olatunji",
        "Franziska Boenisch",
        "Jing Xu",
        "Adam Dziedzic"
      ],
      "published": "2025-08-06T21:38:52+00:00",
      "updated": "2025-08-06T21:38:52+00:00",
      "arxiv_id": "2508.04894v1",
      "url": "http://arxiv.org/pdf/2508.04894v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment",
      "abstract": "Large language models (LLMs) already excel at writing code in high-resource\nlanguages such as Python and JavaScript, yet stumble on low-resource languages\nthat remain essential to science and engineering. Besides the obvious shortage\nof pre-training data, post-training itself is a bottleneck: every new language\nseems to require new datasets, test harnesses, and reinforcement-learning (RL)\ninfrastructure.\n  We introduce Agnostics, a language-agnostic post-training pipeline that\neliminates this per-language engineering. The key idea is to judge code solely\nby its externally observable behavior, so a single verifier can test solutions\nwritten in any language. Concretely, we (i) use an LLM to rewrite existing\nunit-test datasets into an I/O format, (ii) supply a short configuration that\ntells the verifier how to compile and run a target language, and (iii) apply\nreinforcement learning with verifiable rewards (RLVR) in a robust code\nexecution environment.\n  Applied to five low-resource languages--Lua, Julia, R, OCaml, and\nFortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other\n16B-70B open-weight models; (2) scales cleanly to larger and diverse model\nfamilies (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for\n${\\le} 16$B parameter models, sets new state-of-the-art pass@1 results on\nMultiPL-E and a new multi-language version LiveCodeBench that we introduce.\n  We will release the language-agnostic training datasets (Ag-MBPP-X,\nAg-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use\nconfigurations, making RL post-training in any programming language as simple\nas editing a short YAML file.",
      "authors": [
        "Aleksander Boruch-Gruszecki",
        "Yangtian Zi",
        "Zixuan Wu",
        "Tejas Oberoi",
        "Carolyn Jane Anderson",
        "Joydeep Biswas",
        "Arjun Guha"
      ],
      "published": "2025-08-06T20:30:55+00:00",
      "updated": "2025-08-06T20:30:55+00:00",
      "arxiv_id": "2508.04865v1",
      "url": "http://arxiv.org/pdf/2508.04865v1",
      "categories": [
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos",
      "abstract": "Post-training quantization (PTQ) has become a crucial tool for reducing the\nmemory and compute costs of modern deep neural networks, including large\nlanguage models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as\nGPTQ-has emerged as a leading method due to its computational efficiency and\nstrong empirical performance. Despite its widespread adoption, however, OPTQ\nlacks rigorous quantitative theoretical guarantees. This paper presents the\nfirst quantitative error bounds for both deterministic and stochastic variants\nof OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ\nalgorithm. We analyze how OPTQ's iterative procedure induces quantization error\nand derive non-asymptotic 2-norm error bounds that depend explicitly on the\ncalibration data and a regularization parameter that OPTQ uses. Our analysis\nprovides theoretical justification for several practical design choices,\nincluding the widely used heuristic of ordering features by decreasing norm, as\nwell as guidance for selecting the regularization parameter. For the stochastic\nvariant, we establish stronger infinity-norm error bounds, which enable control\nover the required quantization alphabet and are particularly useful for\ndownstream layers and nonlinearities. Finally, we extend our analysis to\nQronos, providing new theoretical bounds, for both its deterministic and\nstochastic variants, that help explain its empirical advantages.",
      "authors": [
        "Haoyu Zhang",
        "Shihao Zhang",
        "Ian Colbert",
        "Rayan Saab"
      ],
      "published": "2025-08-06T20:00:40+00:00",
      "updated": "2025-08-06T20:00:40+00:00",
      "arxiv_id": "2508.04853v1",
      "url": "http://arxiv.org/pdf/2508.04853v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "cs.NA",
        "math.IT",
        "math.NA",
        "68T07, 68W25, 62M45, 68Q25"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning",
      "abstract": "Reinforcement learning (RL) has become a key technique for enhancing the\nreasoning abilities of large language models (LLMs), with policy-gradient\nalgorithms dominating the post-training stage because of their efficiency and\neffectiveness. However, most existing benchmarks evaluate large-language-model\nreasoning under idealized settings, overlooking performance in realistic,\nnon-ideal scenarios. We identify three representative non-ideal scenarios with\npractical relevance: summary inference, fine-grained noise suppression, and\ncontextual filtering. We introduce a new research direction guided by\nbrain-science findings that human reasoning remains reliable under imperfect\ninputs. We formally define and evaluate these challenging scenarios. We\nfine-tune three LLMs and a state-of-the-art large vision-language model (LVLM)\nusing RL with a representative policy-gradient algorithm and then test their\nperformance on eight public datasets. Our results reveal that while RL\nfine-tuning improves baseline reasoning under idealized settings, performance\ndeclines significantly across all three non-ideal scenarios, exposing critical\nlimitations in advanced reasoning capabilities. Although we propose a\nscenario-specific remediation method, our results suggest current methods leave\nthese reasoning deficits largely unresolved. This work highlights that the\nreasoning abilities of large models are often overstated and underscores the\nimportance of evaluating models under non-ideal scenarios. The code and data\nwill be released at XXXX.",
      "authors": [
        "Chang Tian",
        "Matthew B. Blaschko",
        "Mingzhe Xing",
        "Xiuxing Li",
        "Yinliang Yue",
        "Marie-Francine Moens"
      ],
      "published": "2025-08-06T19:51:29+00:00",
      "updated": "2025-08-06T19:51:29+00:00",
      "arxiv_id": "2508.04848v1",
      "url": "http://arxiv.org/pdf/2508.04848v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)",
      "abstract": "Autonomous web-based geographical information systems (AWebGIS) aim to\nperform geospatial operations from natural language input, providing intuitive,\nintelligent, and hands-free interaction. However, most current solutions rely\non cloud-based large language models (LLMs), which require continuous internet\naccess and raise users' privacy and scalability issues due to centralized\nserver processing. This study compares three approaches to enabling AWebGIS:\n(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)\na semi-automated offline method using classical machine learning classifiers\nsuch as support vector machine and random forest; and (3) a fully autonomous\noffline (client-side) method based on a fine-tuned small language model (SLM),\nspecifically T5-small model, executed in the client's web browser. The third\napproach, which leverages SLMs, achieved the highest accuracy among all\nmethods, with an exact matching accuracy of 0.93, Levenshtein similarity of\n0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L\nscores of 0.98. Crucially, this client-side computation strategy reduces the\nload on backend servers by offloading processing to the user's device,\neliminating the need for server-based inference. These results highlight the\nfeasibility of browser-executable models for AWebGIS solutions.",
      "authors": [
        "Mahdi Nazari Ashani",
        "Ali Asghar Alesheikh",
        "Saba Kazemi",
        "Kimya Kheirkhah",
        "Yasin Mohammadi",
        "Fatemeh Rezaie",
        "Amir Mahdi Manafi",
        "Hedieh Zarkesh"
      ],
      "published": "2025-08-06T19:50:29+00:00",
      "updated": "2025-08-06T19:50:29+00:00",
      "arxiv_id": "2508.04846v1",
      "url": "http://arxiv.org/pdf/2508.04846v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Charts-of-Thought: Enhancing LLM Visualization Literacy Through Structured Data Extraction",
      "abstract": "This paper evaluates the visualization literacy of modern Large Language\nModels (LLMs) and introduces a novel prompting technique called\nCharts-of-Thought. We tested three state-of-the-art LLMs (Claude-3.7-sonnet,\nGPT-4.5 preview, and Gemini-2.0-pro) on the Visualization Literacy Assessment\nTest (VLAT) using standard prompts and our structured approach. The\nCharts-of-Thought method guides LLMs through a systematic data extraction,\nverification, and analysis process before answering visualization questions.\nOur results show Claude-3.7-sonnet achieved a score of 50.17 using this method,\nfar exceeding the human baseline of 28.82. This approach improved performance\nacross all models, with score increases of 21.8% for GPT-4.5, 9.4% for\nGemini-2.0, and 13.5% for Claude-3.7 compared to standard prompting. The\nperformance gains were consistent across original and modified VLAT charts,\nwith Claude correctly answering 100% of questions for several chart types that\npreviously challenged LLMs. Our study reveals that modern multimodal LLMs can\nsurpass human performance on visualization literacy tasks when given the proper\nanalytical framework. These findings establish a new benchmark for LLM\nvisualization literacy and demonstrate the importance of structured prompting\nstrategies for complex visual interpretation tasks. Beyond improving LLM\nvisualization literacy, Charts-of-Thought could also enhance the accessibility\nof visualizations, potentially benefiting individuals with visual impairments\nor lower visualization literacy.",
      "authors": [
        "Amit Kumar Das",
        "Mohammad Tarun",
        "Klaus Mueller"
      ],
      "published": "2025-08-06T19:42:43+00:00",
      "updated": "2025-08-06T19:42:43+00:00",
      "arxiv_id": "2508.04842v1",
      "url": "http://arxiv.org/pdf/2508.04842v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History",
      "abstract": "Large language models require consistent behavioral patterns for safe\ndeployment, yet their personality-like traits remain poorly understood. We\npresent PERSIST (PERsonality Stability in Synthetic Text), a comprehensive\nevaluation framework testing 25+ open-source models (1B-671B parameters) across\n500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted\npersonality instruments, we systematically vary question order, paraphrasing,\npersonas, and reasoning modes. Our findings challenge fundamental deployment\nassumptions: (1) Even 400B+ models exhibit substantial response variability (SD\n> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up\nto 20%; (3) Interventions expected to stabilize behavior, such as\nchain-of-thought reasoning, detailed personas instruction, inclusion of\nconversation history, can paradoxically increase variability; (4) LLM-adapted\ninstruments show equal instability to human-centric versions, confirming\narchitectural rather than translational limitations. This persistent\ninstability across scales and mitigation strategies suggests current LLMs lack\nthe foundations for genuine behavioral consistency. For safety-critical\napplications requiring predictable behavior, these findings indicate that\npersonality-based alignment strategies may be fundamentally inadequate.",
      "authors": [
        "Tommaso Tosato",
        "Saskia Helbling",
        "Yorguin-Jose Mantilla-Ramos",
        "Mahmood Hegazy",
        "Alberto Tosato",
        "David John Lemay",
        "Irina Rish",
        "Guillaume Dumas"
      ],
      "published": "2025-08-06T19:11:33+00:00",
      "updated": "2025-08-06T19:11:33+00:00",
      "arxiv_id": "2508.04826v1",
      "url": "http://arxiv.org/pdf/2508.04826v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini",
      "abstract": "Logging is essential in software development, helping developers monitor\nsystem behavior and aiding in debugging applications. Given the ability of\nlarge language models (LLMs) to generate natural language and code, researchers\nare exploring their potential to generate log statements. However, prior work\nfocuses on evaluating logs introduced in code functions, leaving file-level log\ngeneration underexplored -- especially in machine learning (ML) applications,\nwhere comprehensive logging can enhance reliability. In this study, we evaluate\nthe capacity of GPT-4o mini as a case study to generate log statements for ML\nprojects at file level. We gathered a set of 171 ML repositories containing\n4,073 Python files with at least one log statement. We identified and removed\nthe original logs from the files, prompted the LLM to generate logs for them,\nand evaluated both the position of the logs and log level, variables, and text\nquality of the generated logs compared to human-written logs. In addition, we\nmanually analyzed a representative sample of generated logs to identify common\npatterns and challenges. We find that the LLM introduces logs in the same place\nas humans in 63.91% of cases, but at the cost of a high overlogging rate of\n82.66%. Furthermore, our manual analysis reveals challenges for file-level\nlogging, which shows overlogging at the beginning or end of a function,\ndifficulty logging within large code blocks, and misalignment with\nproject-specific logging conventions. While the LLM shows promise for\ngenerating logs for complete files, these limitations remain to be addressed\nfor practical implementation.",
      "authors": [
        "Mayra Sofia Ruiz Rodriguez",
        "SayedHassan Khatoonabadi",
        "Emad Shihab"
      ],
      "published": "2025-08-06T18:57:51+00:00",
      "updated": "2025-08-06T18:57:51+00:00",
      "arxiv_id": "2508.04820v1",
      "url": "http://arxiv.org/pdf/2508.04820v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM",
      "abstract": "In dialogue transcription pipelines, Large Language Models (LLMs) are\nfrequently employed in post-processing to improve grammar, punctuation, and\nreadability. We explore a complementary post-processing step: enriching\ntranscribed dialogues by adding metadata tags for speaker characteristics such\nas age, gender, and emotion. Some of the tags are global to the entire\ndialogue, while some are time-variant. Our approach couples frozen audio\nfoundation models, such as Whisper or WavLM, with a frozen LLAMA language model\nto infer these speaker attributes, without requiring task-specific fine-tuning\nof either model. Using lightweight, efficient connectors to bridge audio and\nlanguage representations, we achieve competitive performance on speaker\nprofiling tasks while preserving modularity and speed. Additionally, we\ndemonstrate that a frozen LLAMA model can compare x-vectors directly, achieving\nan Equal Error Rate of 8.8% in some scenarios.",
      "authors": [
        "Thomas Thebaud",
        "Yen-Ju Lu",
        "Matthew Wiesner",
        "Peter Viechnicki",
        "Najim Dehak"
      ],
      "published": "2025-08-06T18:14:04+00:00",
      "updated": "2025-08-06T18:14:04+00:00",
      "arxiv_id": "2508.04795v1",
      "url": "http://arxiv.org/pdf/2508.04795v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Evaluating the Impact of LLM-guided Reflection on Learning Outcomes with Interactive AI-Generated Educational Podcasts",
      "abstract": "This study examined whether embedding LLM-guided reflection prompts in an\ninteractive AI-generated podcast improved learning and user experience compared\nto a version without prompts. Thirty-six undergraduates participated, and while\nlearning outcomes were similar across conditions, reflection prompts reduced\nperceived attractiveness, highlighting a call for more research on reflective\ninteractivity design.",
      "authors": [
        "Vishnu Menon",
        "Andy Cherney",
        "Elizabeth B. Cloude",
        "Li Zhang",
        "Tiffany D. Do"
      ],
      "published": "2025-08-06T18:03:42+00:00",
      "updated": "2025-08-06T18:03:42+00:00",
      "arxiv_id": "2508.04787v1",
      "url": "http://arxiv.org/pdf/2508.04787v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data",
      "abstract": "LLM-powered conversational assistants are often deployed in a\none-size-fits-all manner, which fails to accommodate individual user\npreferences. Recently, LLM personalization -- tailoring models to align with\nspecific user preferences -- has gained increasing attention as a way to bridge\nthis gap. In this work, we specifically focus on a practical yet challenging\nsetting where only a small set of preference annotations can be collected per\nuser -- a problem we define as Personalized Preference Alignment with Limited\nData (PPALLI). To support research in this area, we introduce two datasets --\nDnD and ELIP -- and benchmark a variety of alignment techniques on them. We\nfurther propose FaST, a highly parameter-efficient approach that leverages\nhigh-level features automatically discovered from the data, achieving the best\noverall performance.",
      "authors": [
        "Thibaut Thonet",
        "GermÃ¡n Kruszewski",
        "Jos Rozen",
        "Pierre Erbacher",
        "Marc Dymetman"
      ],
      "published": "2025-08-06T17:58:26+00:00",
      "updated": "2025-08-06T17:58:26+00:00",
      "arxiv_id": "2508.04698v1",
      "url": "http://arxiv.org/pdf/2508.04698v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models",
      "abstract": "Misleading visualizations pose a significant challenge to accurate data\ninterpretation. While recent research has explored the use of Large Language\nModels (LLMs) for detecting such misinformation, practical tools that also\nsupport explanation and correction remain limited. We present MisVisFix, an\ninteractive dashboard that leverages both Claude and GPT models to support the\nfull workflow of detecting, explaining, and correcting misleading\nvisualizations. MisVisFix correctly identifies 96% of visualization issues and\naddresses all 74 known visualization misinformation types, classifying them as\nmajor, minor, or potential concerns. It provides detailed explanations,\nactionable suggestions, and automatically generates corrected charts. An\ninteractive chat interface allows users to ask about specific chart elements or\nrequest modifications. The dashboard adapts to newly emerging misinformation\nstrategies through targeted user interactions. User studies with visualization\nexperts and developers of fact-checking tools show that MisVisFix accurately\nidentifies issues and offers useful suggestions for improvement. By\ntransforming LLM-based detection into an accessible, interactive platform,\nMisVisFix advances visualization literacy and supports more trustworthy data\ncommunication.",
      "authors": [
        "Amit Kumar Das",
        "Klaus Mueller"
      ],
      "published": "2025-08-06T17:45:11+00:00",
      "updated": "2025-08-06T17:45:11+00:00",
      "arxiv_id": "2508.04679v1",
      "url": "http://arxiv.org/pdf/2508.04679v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay",
      "abstract": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe.",
      "authors": [
        "Yunan Zhang",
        "Shuoran Jiang",
        "Mengchen Zhao",
        "Yuefeng Li",
        "Yang Fan",
        "Xiangping Wu",
        "Qingcai Chen"
      ],
      "published": "2025-08-06T17:42:22+00:00",
      "updated": "2025-08-06T17:42:22+00:00",
      "arxiv_id": "2508.04676v1",
      "url": "http://arxiv.org/pdf/2508.04676v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management",
      "abstract": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale.",
      "authors": [
        "Mo Li",
        "L. H. Xu",
        "Qitai Tan",
        "Ting Cao",
        "Yunxin Liu"
      ],
      "published": "2025-08-06T17:32:58+00:00",
      "updated": "2025-08-06T17:32:58+00:00",
      "arxiv_id": "2508.04664v1",
      "url": "http://arxiv.org/pdf/2508.04664v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "X-SAM: From Segment Anything to Any Segmentation",
      "abstract": "Large Language Models (LLMs) demonstrate strong capabilities in broad\nknowledge representation, yet they are inherently deficient in pixel-level\nperceptual understanding. Although the Segment Anything Model (SAM) represents\na significant advancement in visual-prompt-driven image segmentation, it\nexhibits notable limitations in multi-mask prediction and category-specific\nsegmentation tasks, and it cannot integrate all segmentation tasks within a\nunified model architecture. To address these limitations, we present X-SAM, a\nstreamlined Multimodal Large Language Model (MLLM) framework that extends the\nsegmentation paradigm from \\textit{segment anything} to \\textit{any\nsegmentation}. Specifically, we introduce a novel unified framework that\nenables more advanced pixel-level perceptual comprehension for MLLMs.\nFurthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)\nsegmentation, which segments all instance objects with interactive visual\nprompts and empowers MLLMs with visual grounded, pixel-wise interpretative\ncapabilities. To enable effective training on diverse data sources, we present\na unified training strategy that supports co-training across multiple datasets.\nExperimental results demonstrate that X-SAM achieves state-of-the-art\nperformance on a wide range of image segmentation benchmarks, highlighting its\nefficiency for multimodal, pixel-level visual understanding. Code is available\nat https://github.com/wanghao9610/X-SAM.",
      "authors": [
        "Hao Wang",
        "Limeng Qiao",
        "Zequn Jie",
        "Zhijian Huang",
        "Chengjian Feng",
        "Qingfang Zheng",
        "Lin Ma",
        "Xiangyuan Lan",
        "Xiaodan Liang"
      ],
      "published": "2025-08-06T17:19:10+00:00",
      "updated": "2025-08-06T17:19:10+00:00",
      "arxiv_id": "2508.04655v1",
      "url": "http://arxiv.org/pdf/2508.04655v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "LLM Collaboration With Multi-Agent Reinforcement Learning",
      "abstract": "A large amount of work has been done in Multi-Agent Systems (MAS) for\nmodeling and solving problems with multiple interacting agents. However, most\nLLMs are pretrained independently and not specifically optimized for\ncoordination. Existing LLM fine-tuning frameworks rely on individual rewards,\nwhich require complex reward designs for each agent to encourage collaboration.\nTo address these challenges, we model LLM collaboration as a cooperative\nMulti-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,\nmulti-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),\nto solve it, building on current RL approaches for LLMs as well as MARL\ntechniques. Our experiments on LLM writing and coding collaboration demonstrate\nthat fine-tuning MAS with MAGRPO enables agents to generate high-quality\nresponses efficiently through effective cooperation. Our approach opens the\ndoor to using other MARL methods for LLMs and highlights the associated\nchallenges.",
      "authors": [
        "Shuo Liu",
        "Zeyu Liang",
        "Xueguang Lyu",
        "Christopher Amato"
      ],
      "published": "2025-08-06T17:18:25+00:00",
      "updated": "2025-08-06T17:18:25+00:00",
      "arxiv_id": "2508.04652v1",
      "url": "http://arxiv.org/pdf/2508.04652v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case",
      "abstract": "Collecting real-world data for rare high-risk scenarios, long-tailed driving\nevents, and complex interactions remains challenging, leading to poor\nperformance of existing autonomous driving systems in these critical\nsituations. In this paper, we propose RoboTron-Sim that improves real-world\ndriving in critical situations by utilizing simulated hard cases. First, we\ndevelop a simulated dataset called Hard-case Augmented Synthetic Scenarios\n(HASS), which covers 13 high-risk edge-case categories, as well as balanced\nenvironmental conditions such as day/night and sunny/rainy. Second, we\nintroduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder\n(I2E Encoder) to enable multimodal large language models to effectively learn\nreal-world challenging driving skills from HASS, via adapting to environmental\ndeviations and hardware differences between real-world and simulated scenarios.\nExtensive experiments on nuScenes show that RoboTron-Sim improves driving\nperformance in challenging scenarios by around 50%, achieving state-of-the-art\nresults in real-world open-loop planning. Qualitative results further\ndemonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk\ndriving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/",
      "authors": [
        "Baihui Xiao",
        "Chengjian Feng",
        "Zhijian Huang",
        "Feng yan",
        "Yujie Zhong",
        "Lin Ma"
      ],
      "published": "2025-08-06T17:07:25+00:00",
      "updated": "2025-08-06T17:07:25+00:00",
      "arxiv_id": "2508.04642v1",
      "url": "http://arxiv.org/pdf/2508.04642v1",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "VirtLab: An AI-Powered System for Flexible, Customizable, and Large-scale Team Simulations",
      "abstract": "Simulating how team members collaborate within complex environments using\nAgentic AI is a promising approach to explore hypotheses grounded in social\nscience theories and study team behaviors. We introduce VirtLab, a\nuser-friendly, customizable, multi-agent, and scalable team simulation system\nthat enables testing teams with LLM-based agents in spatial and temporal\nsettings. This system addresses the current frameworks' design and technical\nlimitations that do not consider flexible simulation scenarios and spatial\nsettings. VirtLab contains a simulation engine and a web interface that enables\nboth technical and non-technical users to formulate, run, and analyze team\nsimulations without programming. We demonstrate the system's utility by\ncomparing ground truth data with simulated scenarios.",
      "authors": [
        "Mohammed Almutairi",
        "Charles Chiang",
        "Haoze Guo",
        "Matthew Belcher",
        "Nandini Banerjee",
        "Maria Milkowski",
        "Svitlana Volkova",
        "Daniel Nguyen",
        "Tim Weninger",
        "Michael Yankoski",
        "Trenton W. Ford",
        "Diego Gomez-Zara"
      ],
      "published": "2025-08-06T17:02:01+00:00",
      "updated": "2025-08-06T17:02:01+00:00",
      "arxiv_id": "2508.04634v1",
      "url": "http://arxiv.org/pdf/2508.04634v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research.",
      "authors": [
        "Xu Guo",
        "Tianyi Liang",
        "Tong Jian",
        "Xiaogui Yang",
        "Ling-I Wu",
        "Chenhui Li",
        "Zhihui Lu",
        "Qipeng Guo",
        "Kai Chen"
      ],
      "published": "2025-08-06T17:00:54+00:00",
      "updated": "2025-08-07T11:30:20+00:00",
      "arxiv_id": "2508.04632v2",
      "url": "http://arxiv.org/pdf/2508.04632v2",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis",
      "abstract": "Large Language Models (LLMs) are expected to produce safe, helpful, and\nhonest content during interaction with human users, but they frequently fail to\nalign with such values when given flawed instructions, e.g., missing context,\nambiguous directives, or inappropriate tone, leaving substantial room for\nimprovement along multiple dimensions. A cost-effective yet high-impact way is\nto pre-align instructions before the model begins decoding. Existing approaches\neither rely on prohibitive test-time search costs or end-to-end model rewrite,\nwhich is powered by a customized training corpus with unclear objectives. In\nthis work, we demonstrate that the goal of efficient and effective preference\nalignment can be achieved by P-Aligner, a lightweight module generating\ninstructions that preserve the original intents while being expressed in a more\nhuman-preferred form. P-Aligner is trained on UltraPrompt, a new dataset\nsynthesized via a proposed principle-guided pipeline using Monte-Carlo Tree\nSearch, which systematically explores the space of candidate instructions that\nare closely tied to human preference. Experiments across different methods show\nthat P-Aligner generally outperforms strong baselines across various models and\nbenchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo\nand Gemma-2-SimPO, respectively. Further analyses validate its effectiveness\nand efficiency through multiple perspectives, including data quality, search\nstrategies, iterative deployment, and time overhead.",
      "authors": [
        "Feifan Song",
        "Bofei Gao",
        "Yifan Song",
        "Yi Liu",
        "Weimin Xiong",
        "Yuyang Song",
        "Tianyu Liu",
        "Guoyin Wang",
        "Houfeng Wang"
      ],
      "published": "2025-08-06T16:51:38+00:00",
      "updated": "2025-08-06T16:51:38+00:00",
      "arxiv_id": "2508.04626v1",
      "url": "http://arxiv.org/pdf/2508.04626v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging",
      "abstract": "We present FinMMR, a novel bilingual multimodal benchmark tailored to\nevaluate the reasoning capabilities of multimodal large language models (MLLMs)\nin financial numerical reasoning tasks. Compared to existing benchmarks, our\nwork introduces three significant advancements. (1) Multimodality: We\nmeticulously transform existing financial reasoning benchmarks, and construct\nnovel questions from the latest Chinese financial research reports. FinMMR\ncomprises 4.3K questions and 8.7K images spanning 14 categories, including\ntables, bar charts, and ownership structure charts. (2) Comprehensiveness:\nFinMMR encompasses 14 financial subdomains, including corporate finance,\nbanking, and industry analysis, significantly exceeding existing benchmarks in\nfinancial domain knowledge breadth. (3) Challenge: Models are required to\nperform multi-step precise numerical reasoning by integrating financial\nknowledge with the understanding of complex financial images and text. The\nbest-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe\nthat FinMMR will drive advancements in enhancing the reasoning capabilities of\nMLLMs in real-world scenarios.",
      "authors": [
        "Zichen Tang",
        "Haihong E",
        "Jiacheng Liu",
        "Zhongjun Yang",
        "Rongjin Li",
        "Zihua Rong",
        "Haoyang He",
        "Zhuodi Hao",
        "Xinyang Hu",
        "Kun Ji",
        "Ziyan Ma",
        "Mengyuan Ji",
        "Jun Zhang",
        "Chenghao Ma",
        "Qianhe Zheng",
        "Yang Liu",
        "Yiling Huang",
        "Xinyi Hu",
        "Qing Huang",
        "Zijian Xie",
        "Shiyao Peng"
      ],
      "published": "2025-08-06T16:51:09+00:00",
      "updated": "2025-08-06T16:51:09+00:00",
      "arxiv_id": "2508.04625v1",
      "url": "http://arxiv.org/pdf/2508.04625v1",
      "categories": [
        "cs.CV",
        "cs.CE"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search",
      "abstract": "The advent of Large Language Models (LLMs) is transforming search engines\ninto conversational AI search products, primarily using Retrieval-Augmented\nGeneration (RAG) on web corpora. However, this paradigm has significant\nindustrial limitations. Traditional RAG approaches struggle with real-time\nneeds and structured queries that require accessing dynamically generated\ncontent like ticket availability or inventory. Limited to indexing static\npages, search engines cannot perform the interactive queries needed for such\ntime-sensitive data. Academic research has focused on optimizing RAG for static\ncontent, overlooking complex intents and the need for dynamic sources like\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\nframework that combines RAG with agentic tool-use to access both static content\nand dynamic, real-time information. TURA has three key components: an\nIntent-Aware Retrieval module to decompose queries and retrieve information\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\noptimal parallel execution, and a lightweight Distilled Agent Executor for\nefficient tool calling. TURA is the first architecture to systematically bridge\nthe gap between static RAG and dynamic information sources for a world-class AI\nsearch product. Serving tens of millions of users, it leverages an agentic\nframework to deliver robust, real-time answers while meeting the low-latency\ndemands of a large-scale industrial system.",
      "authors": [
        "Zhejun Zhao",
        "Yuehu Dong",
        "Alley Liu",
        "Lixue Zheng",
        "Pingsheng Liu",
        "Dongdong Shen",
        "Long Xia",
        "Jiashu Zhao",
        "Dawei Yin"
      ],
      "published": "2025-08-06T16:24:17+00:00",
      "updated": "2025-08-06T16:24:17+00:00",
      "arxiv_id": "2508.04604v1",
      "url": "http://arxiv.org/pdf/2508.04604v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning",
      "abstract": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
      "authors": [
        "Magauiya Zhussip",
        "Dmitriy Shopkhoev",
        "Ammar Ali",
        "Stamatios Lefkimmiatis"
      ],
      "published": "2025-08-06T16:06:43+00:00",
      "updated": "2025-08-06T16:06:43+00:00",
      "arxiv_id": "2508.04581v1",
      "url": "http://arxiv.org/pdf/2508.04581v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges",
      "abstract": "Reasoning is a critical capability of multimodal large language models\n(MLLMs) for solving complex multimodal tasks, and judging the correctness of\nreasoning steps is crucial for improving this capability. Recently, MLLM-based\nprocess judges (MPJs) have been widely used to assess the correctness of\nreasoning steps in multimodal tasks. Therefore, evaluating MPJs is important\nfor identifying their limitations and guiding future improvements. However,\nexisting benchmarks for MPJs mainly focus on tasks such as step correctness\nclassification and reasoning process search, while overlooking a key aspect:\nwhether the confidence scores produced by MPJs at the step level are reliable.\nTo address this gap, we propose ConfProBench, the first comprehensive benchmark\ndesigned to systematically evaluate the reliability of step-level confidence\nscores generated by MPJs. Our benchmark constructs three types of adversarially\nperturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and\nImage Perturbation, to test the robustness of MPJ confidence under\nperturbations. In addition, we introduce three novel evaluation metrics:\nConfidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and\nConfidence Calibration Score (CCS), which evaluate robustness, sensitivity, and\ncalibration, respectively. We evaluate 14 state-of-the-art MLLMs, including\nboth proprietary and open-source models. Experiments reveal limitations in\ncurrent MPJs' confidence performance and offer competitive baselines to support\nfuture research.",
      "authors": [
        "Yue Zhou",
        "Yi Chang",
        "Yuan Wu"
      ],
      "published": "2025-08-06T16:00:19+00:00",
      "updated": "2025-08-06T16:00:19+00:00",
      "arxiv_id": "2508.04576v1",
      "url": "http://arxiv.org/pdf/2508.04576v1",
      "categories": [
        "cs.AI",
        "I.2.6; I.2.7; D.2.8"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset",
      "abstract": "Fostering students' abilities for knowledge integration and transfer in\ncomplex problem-solving scenarios is a core objective of modern education, and\ninterdisciplinary STEM is a key pathway to achieve this, yet it requires expert\nguidance that is difficult to scale. While LLMs offer potential in this regard,\ntheir true capability for guided instruction remains unclear due to the lack of\nan effective evaluation benchmark. To address this, we introduce SID, the first\nbenchmark designed to systematically evaluate the higher-order guidance\ncapabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our\ncontributions include a large-scale dataset of 10,000 dialogue turns across 48\ncomplex STEM projects, a novel annotation schema for capturing deep pedagogical\nfeatures, and a new suite of evaluation metrics (e.g., X-SRG). Baseline\nexperiments confirm that even state-of-the-art LLMs struggle to execute\neffective guided dialogues that lead students to achieve knowledge integration\nand transfer. This highlights the critical value of our benchmark in driving\nthe development of more pedagogically-aware LLMs.",
      "authors": [
        "Mei Jiang",
        "Houping Yue",
        "Bingdong Li",
        "Hao Hao",
        "Ying Qian",
        "Bo Jiang",
        "Aimin Zhou"
      ],
      "published": "2025-08-06T15:49:26+00:00",
      "updated": "2025-08-06T15:49:26+00:00",
      "arxiv_id": "2508.04563v1",
      "url": "http://arxiv.org/pdf/2508.04563v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning",
      "abstract": "Depression is a widespread mental disorder that affects millions worldwide.\nWhile automated depression assessment shows promise, most studies rely on\nlimited or non-clinically validated data, and often prioritize complex model\ndesign over real-world effectiveness. In this paper, we aim to unveil the\nlandscape of clinical depression assessment. We introduce C-MIND, a clinical\nneuropsychiatric multimodal diagnosis dataset collected over two years from\nreal hospital visits. Each participant completes three structured psychiatric\ntasks and receives a final diagnosis from expert clinicians, with informative\naudio, video, transcript, and functional near-infrared spectroscopy (fNIRS)\nsignals recorded. Using C-MIND, we first analyze behavioral signatures relevant\nto diagnosis. We train a range of classical models to quantify how different\ntasks and modalities contribute to diagnostic performance, and dissect the\neffectiveness of their combinations. We then explore whether LLMs can perform\npsychiatric reasoning like clinicians and identify their clear limitations in\nrealistic clinical settings. In response, we propose to guide the reasoning\nprocess with clinical expertise and consistently improves LLM diagnostic\nperformance by up to 10% in Macro-F1 score. We aim to build an infrastructure\nfor clinical depression assessment from both data and algorithmic perspectives,\nenabling C-MIND to facilitate grounded and reliable research for mental\nhealthcare.",
      "authors": [
        "Zhuang Chen",
        "Guanqun Bi",
        "Wen Zhang",
        "Jiawei Hu",
        "Aoyun Wang",
        "Xiyao Xiao",
        "Kun Feng",
        "Minlie Huang"
      ],
      "published": "2025-08-06T15:13:24+00:00",
      "updated": "2025-08-06T15:13:24+00:00",
      "arxiv_id": "2508.04531v1",
      "url": "http://arxiv.org/pdf/2508.04531v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Balancing Stylization and Truth via Disentangled Representation Steering",
      "abstract": "Generating stylized large language model (LLM) responses via representation\nediting is a promising way for fine-grained output control. However, there\nexists an inherent trade-off: imposing a distinctive style often degrades\ntruthfulness. Existing representation editing methods, by naively injecting\nstyle signals, overlook this collateral impact and frequently contaminate the\nmodel's core truthfulness representations, resulting in reduced answer\ncorrectness. We term this phenomenon stylization-induced truthfulness collapse.\nWe attribute this issue to latent coupling between style and truth directions\nin certain key attention heads, and propose StyliTruth, a mechanism that\npreserves stylization while keeping truthfulness intact. StyliTruth separates\nthe style-relevant and truth-relevant subspaces in the model's representation\nspace via an orthogonal deflation process. This decomposition enables\nindependent control of style and truth in their own subspaces, minimizing\ninterference. By designing adaptive, token-level steering vectors within each\nsubspace, we dynamically and precisely control the generation process to\nmaintain both stylistic fidelity and truthfulness. We validate our method on\nmultiple styles and languages. Extensive experiments and analyses show that\nStyliTruth significantly reduces stylization-induced truthfulness collapse and\noutperforms existing inference-time intervention methods in balancing style\nadherence with truthfulness.",
      "authors": [
        "Chenglei Shen",
        "Zhongxiang Sun",
        "Teng Shi",
        "Xiao Zhang",
        "Jun Xu"
      ],
      "published": "2025-08-06T15:12:05+00:00",
      "updated": "2025-08-07T06:14:50+00:00",
      "arxiv_id": "2508.04530v2",
      "url": "http://arxiv.org/pdf/2508.04530v2",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning Framework for Explainable Deepfake Detection",
      "abstract": "The rapid advancement of AI-generation models has enabled the creation of\nhyperrealistic imagery, posing ethical risks through widespread misinformation.\nCurrent deepfake detection methods, categorized as face specific detectors or\ngeneral AI-generated detectors, lack transparency by framing detection as a\nclassification task without explaining decisions. While several LLM-based\napproaches offer explainability, they suffer from coarse-grained analyses and\ndependency on labor-intensive annotations. This paper introduces RAIDX\n(Retrieval-Augmented Image Deepfake Detection and Explainability), a novel\ndeepfake detection framework integrating Retrieval-Augmented Generation (RAG)\nand Group Relative Policy Optimization (GRPO) to enhance detection accuracy and\ndecision explainability. Specifically, RAIDX leverages RAG to incorporate\nexternal knowledge for improved detection accuracy and employs GRPO to\nautonomously generate fine-grained textual explanations and saliency maps,\neliminating the need for extensive manual annotations. Experiments on multiple\nbenchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and\nproviding interpretable rationales in both textual descriptions and saliency\nmaps, achieving state-of-the-art detection performance while advancing\ntransparency in deepfake identification. RAIDX represents the first unified\nframework to synergize RAG and GRPO, addressing critical gaps in accuracy and\nexplainability. Our code and models will be publicly available.",
      "authors": [
        "Tianxiao Li",
        "Zhenglin Huang",
        "Haiquan Wen",
        "Yiwei He",
        "Shuchang Lyu",
        "Baoyuan Wu",
        "Guangliang Cheng"
      ],
      "published": "2025-08-06T15:08:16+00:00",
      "updated": "2025-08-06T15:08:16+00:00",
      "arxiv_id": "2508.04524v1",
      "url": "http://arxiv.org/pdf/2508.04524v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Causal Reflection with Language Models",
      "abstract": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments.",
      "authors": [
        "Abi Aryan",
        "Zac Liu"
      ],
      "published": "2025-08-06T14:44:23+00:00",
      "updated": "2025-08-06T14:44:23+00:00",
      "arxiv_id": "2508.04495v1",
      "url": "http://arxiv.org/pdf/2508.04495v1",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use",
      "abstract": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain.",
      "authors": [
        "Xueyu Hu",
        "Tao Xiong",
        "Biao Yi",
        "Zishu Wei",
        "Ruixuan Xiao",
        "Yurun Chen",
        "Jiasheng Ye",
        "Meiling Tao",
        "Xiangxin Zhou",
        "Ziyu Zhao",
        "Yuhuai Li",
        "Shengze Xu",
        "Shenzhi Wang",
        "Xinchen Xu",
        "Shuofei Qiao",
        "Zhaokai Wang",
        "Kun Kuang",
        "Tieyong Zeng",
        "Liang Wang",
        "Jiwei Li",
        "Yuchen Eleanor Jiang",
        "Wangchunshu Zhou",
        "Guoyin Wang",
        "Keting Yin",
        "Zhou Zhao",
        "Hongxia Yang",
        "Fan Wu",
        "Shengyu Zhang",
        "Fei Wu"
      ],
      "published": "2025-08-06T14:33:45+00:00",
      "updated": "2025-08-06T14:33:45+00:00",
      "arxiv_id": "2508.04482v1",
      "url": "http://arxiv.org/pdf/2508.04482v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large Language Models",
      "abstract": "Recent advances in large language models (LLMs) have unlocked powerful\nreasoning and decision-making capabilities. However, their inherent dependence\non static parametric memory fundamentally limits their adaptability, factual\naccuracy, and interpretability in knowledge-intensive scenarios. Knowledge\ngraphs (KGs), as structured repositories of explicit relational knowledge,\noffer a promising approach for augmenting LLMs with external, interpretable\nmemory. Nevertheless, most existing methods that combine LLMs with KGs treat\nreasoning and knowledge updating as separate processes, resulting in suboptimal\nutilization of new information and hindering real-time updates. In this work,\nwe propose TRAIL: a novel, unified framework for Thinking, Reasoning, And\nIncremental Learning that couples joint inference and dynamic KG refinement\nwith large language models. TRAIL enables LLM agents to iteratively explore,\nupdate, and refine knowledge graphs during the reasoning process, employing a\nconfidence-driven mechanism for the generation, validation, and pruning of new\nfacts. This plug-and-play architecture facilitates seamless integration with\nvarious LLMs, supporting continual adaptation without the need for retraining.\nExtensive experiments on multiple benchmarks demonstrate that TRAIL outperforms\nexisting KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More\nimportantly, these results represent a significant step toward developing\nadaptive, memory-augmented language models capable of continual learning and\nreliable, transparent reasoning.",
      "authors": [
        "Xinkui Zhao",
        "Haode Li",
        "Yifan Zhang",
        "Guanjie Cheng",
        "Yueshen Xu"
      ],
      "published": "2025-08-06T14:25:05+00:00",
      "updated": "2025-08-06T14:25:05+00:00",
      "arxiv_id": "2508.04474v1",
      "url": "http://arxiv.org/pdf/2508.04474v1",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference",
      "abstract": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
      "authors": [
        "Enyu Zhou",
        "Kai Sheng",
        "Hao Chen",
        "Xin He"
      ],
      "published": "2025-08-06T14:02:10+00:00",
      "updated": "2025-08-06T14:02:10+00:00",
      "arxiv_id": "2508.04462v1",
      "url": "http://arxiv.org/pdf/2508.04462v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "From \"Aha Moments\" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control",
      "abstract": "Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex\nreasoning by spontaneously exhibiting cognitive behaviors such as step-by-step\nreasoning, reflection, and backtracking, commonly referred to as \"Aha Moments\".\nHowever, such emergent behaviors remain unregulated and uncontrolled, often\nresulting in overthinking, where the model continues generating redundant\nreasoning content even after reaching reliable conclusions. This leads to\nexcessive computational costs and increased latency, limiting the practical\ndeployment of LRMs. The root cause lies in the absence of intrinsic regulatory\nmechanisms, as current models are unable to monitor and adaptively manage their\nreasoning process to determine when to continue, backtrack, or terminate. To\naddress this issue, we propose the Meta-cognitive Reasoning Framework (MERA),\nwhich explicitly decouples the thinking process into distinct reasoning and\ncontrol components, thereby enabling the independent optimization of control\nstrategies. Specifically, MERA incorporates a takeover-based data construction\nmechanism that identifies critical decision points during reasoning and\ndelegates the creation of control signals to auxiliary LLMs, thereby enabling\nthe construction of high-quality reasoning-control data. Additionally, a\nstructured reasoning-control separation is implemented via supervised\nfine-tuning, enabling the model to generate explicit traces and acquire initial\nmeta-cognitive control capabilities. Finally, MERA employs Control-Segment\nPolicy Optimization (CSPO), which combines segment-wise Group Relative Policy\nOptimization (GRPO) with a control-masking mechanism to optimize control\nbehavior learning while minimizing interference from irrelevant content.\nExperiments on various reasoning benchmarks demonstrate that models trained\nwith MERA enhance both reasoning efficiency and accuracy.",
      "authors": [
        "Rui Ha",
        "Chaozhuo Li",
        "Rui Pu",
        "Sen Su"
      ],
      "published": "2025-08-06T13:59:17+00:00",
      "updated": "2025-08-06T13:59:17+00:00",
      "arxiv_id": "2508.04460v1",
      "url": "http://arxiv.org/pdf/2508.04460v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Automatic LLM Red Teaming",
      "abstract": "Red teaming is critical for identifying vulnerabilities and building trust in\ncurrent LLMs. However, current automated methods for Large Language Models\n(LLMs) rely on brittle prompt templates or single-turn attacks, failing to\ncapture the complex, interactive nature of real-world adversarial dialogues. We\npropose a novel paradigm: training an AI to strategically `break' another AI.\nBy formalizing red teaming as a Markov Decision Process (MDP) and employing a\nhierarchical Reinforcement Learning (RL) framework, we effectively address the\ninherent sparse reward and long-horizon challenges. Our generative agent learns\ncoherent, multi-turn attack strategies through a fine-grained, token-level harm\nreward, enabling it to uncover subtle vulnerabilities missed by existing\nbaselines. This approach sets a new state-of-the-art, fundamentally reframing\nLLM red teaming as a dynamic, trajectory-based process (rather than a one-step\ntest) essential for robust AI deployment.",
      "authors": [
        "Roman Belaire",
        "Arunesh Sinha",
        "Pradeep Varakantham"
      ],
      "published": "2025-08-06T13:52:00+00:00",
      "updated": "2025-08-06T13:52:00+00:00",
      "arxiv_id": "2508.04451v1",
      "url": "http://arxiv.org/pdf/2508.04451v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection",
      "abstract": "Modern software relies on a multitude of automated testing and quality\nassurance tools to prevent errors, bugs and potential vulnerabilities. This\nstudy sets out to provide a head-to-head, quantitative and qualitative\nevaluation of six automated approaches: three industry-standard rule-based\nstatic code-analysis tools (SonarQube, CodeQL and Snyk Code) and three\nstate-of-the-art large language models hosted on the GitHub Models platform\n(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten\nreal-world C# projects that embed 63 vulnerabilities across common categories\nsuch as SQL injection, hard-coded secrets and outdated dependencies, we measure\nclassical detection accuracy (precision, recall, F-score), analysis latency,\nand the developer effort required to vet true positives. The language-based\nscanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their\nstatic counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'\nadvantage originates from superior recall, confirming an ability to reason\nacross broader code contexts. However, this benefit comes with substantial\ntrade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language\nmodels mislocate issues at line-or-column granularity due to tokenisation\nartefacts. Overall, language models successfully rival traditional static\nanalysers in finding real vulnerabilities. Still, their noisier output and\nimprecise localisation limit their standalone use in safety-critical audits. We\ntherefore recommend a hybrid pipeline: employ language models early in\ndevelopment for broad, context-aware triage, while reserving deterministic\nrule-based scanners for high-assurance verification. The open benchmark and\nJSON-based result harness released with this paper lay a foundation for\nreproducible, practitioner-centric research into next-generation automated code\nsecurity.",
      "authors": [
        "Damian Gnieciak",
        "Tomasz Szandala"
      ],
      "published": "2025-08-06T13:48:38+00:00",
      "updated": "2025-08-06T13:48:38+00:00",
      "arxiv_id": "2508.04448v1",
      "url": "http://arxiv.org/pdf/2508.04448v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle",
      "abstract": "Reinforcement learning (RL)-based dynamic treatment regimes (DTRs) hold\npromise for automating complex clinical decision-making, yet their practical\ndeployment remains hindered by the intensive engineering required to inject\nclinical knowledge and ensure patient safety. Recent advancements in large\nlanguage models (LLMs) suggest a complementary approach, where implicit prior\nknowledge and clinical heuristics are naturally embedded through linguistic\nprompts without requiring environment-specific training. In this study, we\nrigorously evaluate open-source LLMs as dynamic insulin dosing agents in an in\nsilico Type 1 diabetes simulator, comparing their zero-shot inference\nperformance against small neural network-based RL agents (SRAs) explicitly\ntrained for the task. Our results indicate that carefully designed zero-shot\nprompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or\nsuperior clinical performance relative to extensively trained SRAs,\nparticularly in stable patient cohorts. However, LLMs exhibit notable\nlimitations, such as overly aggressive insulin dosing when prompted with\nchain-of-thought (CoT) reasoning, highlighting critical failure modes including\narithmetic hallucination, temporal misinterpretation, and inconsistent clinical\nlogic. Incorporating explicit reasoning about latent clinical states (e.g.,\nmeals) yielded minimal performance gains, underscoring the current model's\nlimitations in capturing complex, hidden physiological dynamics solely through\ntextual inference. Our findings advocate for cautious yet optimistic\nintegration of LLMs into clinical workflows, emphasising the necessity of\ntargeted prompt engineering, careful validation, and potentially hybrid\napproaches that combine linguistic reasoning with structured physiological\nmodelling to achieve safe, robust, and clinically effective decision-support\nsystems.",
      "authors": [
        "Zhiyao Luo",
        "Tingting Zhu"
      ],
      "published": "2025-08-06T13:46:02+00:00",
      "updated": "2025-08-06T13:46:02+00:00",
      "arxiv_id": "2508.04755v1",
      "url": "http://arxiv.org/pdf/2508.04755v1",
      "categories": [
        "cs.LG",
        "cs.CE"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion",
      "abstract": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models.",
      "authors": [
        "Yutong Wu",
        "Di Huang",
        "Ruosi Wan",
        "Yue Peng",
        "Shijie Shang",
        "Chenrui Cao",
        "Lei Qi",
        "Rui Zhang",
        "Zidong Du",
        "Jie Yan",
        "Xing Hu"
      ],
      "published": "2025-08-06T13:28:22+00:00",
      "updated": "2025-08-06T13:28:22+00:00",
      "arxiv_id": "2508.04440v1",
      "url": "http://arxiv.org/pdf/2508.04440v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices",
      "abstract": "High-quality, multi-turn instructional dialogues between novices and experts\nare essential for developing AI systems that support teaching, learning, and\ndecision-making. These dialogues often involve scaffolding -- the process by\nwhich an expert supports a novice's thinking through questions, feedback, and\nstep-by-step guidance. However, such data are scarce due to privacy concerns in\nrecording and the vulnerability inherent in help-seeking. We present\nSimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding\ndialogues. Using teaching development coaching as an example domain,\nSimInstruct simulates novice instructors via LLMs, varying their teaching\nchallenges and LLM's persona traits, while human experts provide multi-turn\nfeedback, reasoning, and instructional support. This design enables the\ncreation of realistic, pedagogically rich dialogues without requiring real\nnovice participants. Our results reveal that persona traits, such as\nextroversion and introversion, meaningfully influence how experts engage.\nCompared to real mentoring recordings, SimInstruct dialogues demonstrate\ncomparable pedagogical relevance and cognitive depth. Experts also reported the\nprocess as engaging and reflective, improving both data quality and their own\nprofessional insight. We further fine-tuned a LLaMA model to be an expert model\nusing the augmented dataset, which outperformed GPT-4o in instructional\nquality. Our analysis highlights GPT-4o's limitations in weak reflective\nquestioning, overuse of generic praise, a condescending tone, and a tendency to\noverwhelm novices with excessive suggestions.",
      "authors": [
        "Si Chen",
        "Izzy Molnar",
        "Ting Hua",
        "Peiyu Li",
        "Le Huy Khiem",
        "G. Alex Ambrose",
        "Jim Lang",
        "Ronald Metoyer",
        "Nitesh V. Chawla"
      ],
      "published": "2025-08-06T13:16:10+00:00",
      "updated": "2025-08-06T13:16:10+00:00",
      "arxiv_id": "2508.04428v1",
      "url": "http://arxiv.org/pdf/2508.04428v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Evaluating, Synthesizing, and Enhancing for Customer Support Conversation",
      "abstract": "Effective customer support requires not only accurate problem solving but\nalso structured and empathetic communication aligned with professional\nstandards. However, existing dialogue datasets often lack strategic guidance,\nand real-world service data is difficult to access and annotate. To address\nthis, we introduce the task of Customer Support Conversation (CSC), aimed at\ntraining customer service agents to respond using well-defined support\nstrategies. We propose a structured CSC framework grounded in COPC guidelines,\ndefining five conversational stages and twelve strategies to guide high-quality\ninteractions. Based on this, we construct CSConv, an evaluation dataset of\n1,855 real-world customer-agent conversations rewritten using LLMs to reflect\ndeliberate strategy use, and annotated accordingly. Additionally, we develop a\nrole-playing approach that simulates strategy-rich conversations using\nLLM-powered roles aligned with the CSC framework, resulting in the training\ndataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS\nsignificantly improves their ability to generate high-quality, strategy-aligned\nresponses on CSConv. Human evaluations further confirm gains in problem\nresolution. All code and data will be made publicly available at\nhttps://github.com/aliyun/qwen-dianjin.",
      "authors": [
        "Jie Zhu",
        "Huaixia Dou",
        "Junhui Li",
        "Lifan Guo",
        "Feng Chen",
        "Chi Zhang",
        "Fang Kong"
      ],
      "published": "2025-08-06T13:11:17+00:00",
      "updated": "2025-08-06T13:11:17+00:00",
      "arxiv_id": "2508.04423v1",
      "url": "http://arxiv.org/pdf/2508.04423v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning",
      "abstract": "The video reasoning ability of multimodal large language models (MLLMs) is\ncrucial for downstream tasks like video question answering and temporal\ngrounding. While recent approaches have explored text-based chain-of-thought\n(CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal\ninteraction and increased hallucination, especially with longer videos or\nreasoning chains. To address these challenges, we propose Video Intelligence\nvia Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning\nframework. With a visual toolbox, the model can densely sample new video frames\non demand and generate multimodal CoT for precise long video reasoning. We\nobserve that temporal grounding and question answering are mutually beneficial\nfor video understanding tasks. Therefore, we construct two high-quality\nmulti-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and\nMTVR-RL-110k for reinforcement learning. Moreover, we propose a\nDifficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to\nmitigate difficulty imbalance in multi-task reinforcement learning. Extensive\nexperiments on 11 challenging video understanding benchmarks demonstrate the\nadvanced reasoning ability of VITAL, outperforming existing methods in video\nquestion answering and temporal grounding tasks, especially in long video\nscenarios. All code, data and model weight will be made publicly available.",
      "authors": [
        "Haoji Zhang",
        "Xin Gu",
        "Jiawen Li",
        "Chixiang Ma",
        "Sule Bai",
        "Chubin Zhang",
        "Bowen Zhang",
        "Zhichao Zhou",
        "Dongliang He",
        "Yansong Tang"
      ],
      "published": "2025-08-06T13:03:21+00:00",
      "updated": "2025-08-06T13:03:21+00:00",
      "arxiv_id": "2508.04416v1",
      "url": "http://arxiv.org/pdf/2508.04416v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents",
      "abstract": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.",
      "authors": [
        "Thassilo M. Schiepanski",
        "Nicholas PiÃ«l"
      ],
      "published": "2025-08-06T12:56:54+00:00",
      "updated": "2025-08-06T12:56:54+00:00",
      "arxiv_id": "2508.04412v1",
      "url": "http://arxiv.org/pdf/2508.04412v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design",
      "abstract": "Large Language Models (LLMs) demonstrate exceptional performance but entail\nsignificant memory and computational costs, restricting their practical\ndeployment. While existing INT4/INT8 quantization reduces these costs, they\noften degrade accuracy or lack optimal efficiency. INT6 quantization offers a\nsuperior trade-off between model accuracy and inference efficiency, but lacks\nhardware support in modern GPUs, forcing emulation via higher-precision\narithmetic units that limit acceleration.\n  In this paper, we propose FlexQ, a novel post-training INT6 quantization\nframework combining algorithmic innovation with system-level optimizations.\nFlexQ employs uniform 6-bit weight quantization across all layers, with\nadaptive retention of 8-bit activations in layers identified through layer-wise\nsensitivity analysis. To maximize hardware efficiency, we develop a specialized\nhigh-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8\nrepresentations via Binary Tensor Core (BTC) equivalents, effectively bypassing\nthe lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ\nmaintains near-FP16 accuracy, with perplexity increases of no more than 0.05.\nThe proposed kernel achieves an average 1.39$\\times$ speedup over ABQ-LLM on\nLLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\\times$ inference\nacceleration and 1.21$\\times$ memory savings over SmoothQuant. Code is released\nat https://github.com/FlyFoxPlayer/FlexQ.",
      "authors": [
        "Hao Zhang",
        "Aining Jia",
        "Weifeng Bu",
        "Yushu Cai",
        "Kai Sheng",
        "Hao Chen",
        "Xin He"
      ],
      "published": "2025-08-06T12:47:05+00:00",
      "updated": "2025-08-06T12:47:05+00:00",
      "arxiv_id": "2508.04405v1",
      "url": "http://arxiv.org/pdf/2508.04405v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Why are LLMs' abilities emergent?",
      "abstract": "The remarkable success of Large Language Models (LLMs) in generative tasks\nhas raised fundamental questions about the nature of their acquired\ncapabilities, which often appear to emerge unexpectedly without explicit\ntraining. This paper examines the emergent properties of Deep Neural Networks\n(DNNs) through both theoretical analysis and empirical observation, addressing\nthe epistemological challenge of \"creation without understanding\" that\ncharacterises contemporary AI development. We explore how the neural approach's\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\ncomputational paradigms, creating systems whose macro-level behaviours cannot\nbe analytically derived from micro-level neuron activities. Through analysis of\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\nI demonstrate that emergent abilities arise from the complex dynamics of highly\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\ninvestigation reveals that current debates over metrics, pre-training loss\nthresholds, and in-context learning miss the fundamental ontological nature of\nemergence in DNNs. I argue that these systems exhibit genuine emergent\nproperties analogous to those found in other complex natural phenomena, where\nsystemic capabilities emerge from cooperative interactions among simple\ncomponents without being reducible to their individual behaviours. The paper\nconcludes that understanding LLM capabilities requires recognising DNNs as a\nnew domain of complex dynamical systems governed by universal principles of\nemergence, similar to those operating in physics, chemistry, and biology. This\nperspective shifts the focus from purely phenomenological definitions of\nemergence to understanding the internal dynamic transformations that enable\nthese systems to acquire capabilities that transcend their individual\ncomponents.",
      "authors": [
        "VladimÃ­r HavlÃ­k"
      ],
      "published": "2025-08-06T12:43:04+00:00",
      "updated": "2025-08-06T12:43:04+00:00",
      "arxiv_id": "2508.04401v1",
      "url": "http://arxiv.org/pdf/2508.04401v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky",
      "abstract": "This study evaluates advanced natural language processing (NLP) techniques to\nenhance crash data quality by mining crash narratives, using secondary crash\nidentification in Kentucky as a case study. Drawing from 16,656 manually\nreviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we\ncompare three model classes: zero-shot open-source large language models (LLMs)\n(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers\n(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic\nregression as baseline. Models were calibrated on 2015-2021 data and tested on\n1,771 narratives from 2022. Fine-tuned transformers achieved superior\nperformance, with RoBERTa yielding the highest F1-score (0.90) and accuracy\n(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139\nminutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs\nexcelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred\nhigh computational costs (up to 723 minutes for DeepSeek-R1:70B), while\nfine-tuned models processed the test set in seconds after brief training.\nFurther analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can\nrival larger counterparts in performance while reducing runtime, suggesting\nopportunities for optimized deployments. Results highlight trade-offs between\naccuracy, efficiency, and data requirements, with fine-tuned transformer models\nbalancing precision and recall effectively on Kentucky data. Practical\ndeployment considerations emphasize privacy-preserving local deployment,\nensemble approaches for improved accuracy, and incremental processing for\nscalability, providing a replicable scheme for enhancing crash-data quality\nwith advanced NLP.",
      "authors": [
        "Xu Zhang",
        "Mei Chen"
      ],
      "published": "2025-08-06T12:41:18+00:00",
      "updated": "2025-08-06T12:41:18+00:00",
      "arxiv_id": "2508.04399v1",
      "url": "http://arxiv.org/pdf/2508.04399v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning",
      "abstract": "Graphical user interface visual grounding (GUI-VG), a core capability for GUI\nagents, has primarily relied on supervised fine-tuning (SFT) of multimodal\nlarge language models (MLLMs), which demands extensive data curation and\nsignificant training costs. However, as MLLMs continue to advance and even\ncover GUI domains during pretraining, the necessity of exhaustive SFT\npost-training becomes increasingly questionable. Meanwhile, recent successes of\nrule-based reinforcement fine-tuning (RFT) suggest a more efficient\nalternative. Despite this promise, the optimal manner of applying RFT for\nGUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a\nreinforcement learning-based GUI-VG method built on a systematic empirical\nstudy and a novel stabilization technique. We find that naive application of\nRFT underperforms the SFT baseline, motivating a deeper exploration. First, we\ndecompose RFT into its core components and analyze the optimal formulation of\neach. Second, we propose a novel Adversarial KL Factor that dynamically\nstabilizes training to mitigate reward over-optimization. Third, we further\nexplore the training configurations of RFT to enhance effectiveness. Extensive\nexperiments show that GuirlVG, with only 5.2K training samples, outperforms SFT\nmethods trained on over 10M samples, achieving a 7.7% improvement on\nScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on\nScreenSpotV2.",
      "authors": [
        "Weitai Kang",
        "Bin Lei",
        "Gaowen Liu",
        "Caiwen Ding",
        "Yan Yan"
      ],
      "published": "2025-08-06T12:35:24+00:00",
      "updated": "2025-08-06T12:35:24+00:00",
      "arxiv_id": "2508.04389v1",
      "url": "http://arxiv.org/pdf/2508.04389v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding",
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in vision-language tasks, yet they still face challenges when\nprocessing long-duration video inputs. The limitation arises from MLLMs'\ncontext limit and training costs, necessitating sparse frame sampling before\nfeeding videos into MLLMs. Existing video MLLMs adopt training-free uniform\nsampling or keyframe search, which may miss critical events or be constrained\nby the pre-trained models' event understanding capabilities. Meanwhile,\nbuilding a training-based method remains challenging due to the unsupervised\nand non-differentiable nature of sparse frame sampling. To address these\nproblems, we propose Temporal Sampling Policy Optimization (TSPO), advancing\nMLLMs' long-form video-language understanding via reinforcement learning.\nSpecifically, we first propose a trainable event-aware temporal agent, which\ncaptures event-query correlation for performing probabilistic keyframe\nselection. Then, we propose the TSPO reinforcement learning paradigm, which\nmodels keyframe selection and language generation as a joint decision-making\nprocess, enabling end-to-end group relative optimization with efficient\nrule-based rewards. Furthermore, for the TSPO's training, we propose a long\nvideo training data construction pipeline with comprehensive temporal data and\nvideo Needle-in-a-Haystack data. Finally, we incorporate rule-based answering\naccuracy and temporal locating reward mechanisms to optimize the temporal\nsampling policy. Comprehensive experiments show that our TSPO achieves\nstate-of-the-art performance across multiple long video understanding\nbenchmarks, and shows transferable ability across different cutting-edge\nVideo-MLLMs. Our code is available at https://github.com/Hui-design/TSPO",
      "authors": [
        "Canhui Tang",
        "Zifan Han",
        "Hongbo Sun",
        "Sanping Zhou",
        "Xuchong Zhang",
        "Xin Wei",
        "Ye Yuan",
        "Jinglin Xu",
        "Hao Sun"
      ],
      "published": "2025-08-06T12:03:36+00:00",
      "updated": "2025-08-07T15:24:17+00:00",
      "arxiv_id": "2508.04369v2",
      "url": "http://arxiv.org/pdf/2508.04369v2",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "LUST: A Multi-Modal Framework with Hierarchical LLM-based Scoring for Learned Thematic Significance Tracking in Multimedia Content",
      "abstract": "This paper introduces the Learned User Significance Tracker (LUST), a\nframework designed to analyze video content and quantify the thematic relevance\nof its segments in relation to a user-provided textual description of\nsignificance. LUST leverages a multi-modal analytical pipeline, integrating\nvisual cues from video frames with textual information extracted via Automatic\nSpeech Recognition (ASR) from the audio track. The core innovation lies in a\nhierarchical, two-stage relevance scoring mechanism employing Large Language\nModels (LLMs). An initial \"direct relevance\" score, $S_{d,i}$, assesses\nindividual segments based on immediate visual and auditory content against the\ntheme. This is followed by a \"contextual relevance\" score, $S_{c,i}$, that\nrefines the assessment by incorporating the temporal progression of preceding\nthematic scores, allowing the model to understand evolving narratives. The LUST\nframework aims to provide a nuanced, temporally-aware measure of user-defined\nsignificance, outputting an annotated video with visualized relevance scores\nand comprehensive analytical logs.",
      "authors": [
        "Anderson de Lima Luiz"
      ],
      "published": "2025-08-06T11:48:51+00:00",
      "updated": "2025-08-06T11:48:51+00:00",
      "arxiv_id": "2508.04353v1",
      "url": "http://arxiv.org/pdf/2508.04353v1",
      "categories": [
        "cs.MM",
        "cs.AI",
        "68T07"
      ],
      "primary_category": "cs.MM"
    },
    {
      "title": "Chain of Questions: Guiding Multimodal Curiosity in Language Models",
      "abstract": "Reasoning capabilities in large language models (LLMs) have substantially\nadvanced through methods such as chain-of-thought and explicit step-by-step\nexplanations. However, these improvements have not yet fully transitioned to\nmultimodal contexts, where models must proactively decide which sensory\nmodalities such as vision, audio, or spatial perception to engage when\ninteracting with complex real-world environments. In this paper, we introduce\nthe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach\nthat encourages multimodal language models to dynamically generate targeted\nquestions regarding their surroundings. These generated questions guide the\nmodel to selectively activate relevant modalities, thereby gathering critical\ninformation necessary for accurate reasoning and response generation. We\nevaluate our framework on a novel multimodal benchmark dataset, assembled by\nintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results\ndemonstrate that our CoQ method improves a foundation model's ability to\neffectively identify and integrate pertinent sensory information. This leads to\nimproved accuracy, interpretability, and alignment of the reasoning process\nwith diverse multimodal tasks.",
      "authors": [
        "Nima Iji",
        "Kia Dashtipour"
      ],
      "published": "2025-08-06T11:42:54+00:00",
      "updated": "2025-08-06T11:42:54+00:00",
      "arxiv_id": "2508.04350v1",
      "url": "http://arxiv.org/pdf/2508.04350v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy",
      "abstract": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models.",
      "authors": [
        "Hongze Tan",
        "Jianfei Pan"
      ],
      "published": "2025-08-06T11:42:47+00:00",
      "updated": "2025-08-06T11:42:47+00:00",
      "arxiv_id": "2508.04349v1",
      "url": "http://arxiv.org/pdf/2508.04349v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models",
      "abstract": "Large language models often fail at logical reasoning when semantic\nheuristics conflict with decisive evidence - a phenomenon we term cognitive\ntraps. To address this fundamental limitation, we introduce the Deliberative\nReasoning Network (DRN), a novel paradigm that reframes logical reasoning from\nprobability maximization to uncertainty minimization. Instead of asking \"Which\nanswer is most likely?\", DRN asks \"Which hypothesis has the most internally\nconsistent evidence?\". DRN achieves intrinsic interpretability by explicitly\ntracking belief states and quantifying epistemic uncertainty for competing\nhypotheses through an iterative evidence synthesis process. We validate our\napproach through two complementary architectures - a bespoke discriminative\nmodel that embodies the core uncertainty minimization principle, and a\nlightweight verification module that enhances existing generative LLMs.\nEvaluated on LCR-1000, our new adversarial reasoning benchmark designed to\nexpose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over\nstandard baselines. When integrated as a parameter-efficient verifier with\nMistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most\nchallenging problems. Critically, DRN demonstrates strong zero-shot\ngeneralization, improving TruthfulQA performance by 23.6% without additional\ntraining, indicating that uncertainty-driven deliberation learns transferable\nreasoning principles. We position DRN as a foundational, verifiable System 2\nreasoning component for building more trustworthy AI systems.",
      "authors": [
        "Anran Xu",
        "Jincheng Wang",
        "Baigen Cai",
        "Tao Wen"
      ],
      "published": "2025-08-06T11:33:35+00:00",
      "updated": "2025-08-06T11:33:35+00:00",
      "arxiv_id": "2508.04339v1",
      "url": "http://arxiv.org/pdf/2508.04339v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Modelling and Classifying the Components of a Literature Review",
      "abstract": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels.",
      "authors": [
        "Francisco BolaÃ±os",
        "Angelo Salatino",
        "Francesco Osborne",
        "Enrico Motta"
      ],
      "published": "2025-08-06T11:30:07+00:00",
      "updated": "2025-08-06T11:30:07+00:00",
      "arxiv_id": "2508.04337v1",
      "url": "http://arxiv.org/pdf/2508.04337v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.IR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning",
      "abstract": "Supervised fine-tuning (SFT) plays a critical role for pretrained large\nlanguage models (LLMs), notably enhancing their capacity to acquire\ndomain-specific knowledge while preserving or potentially augmenting their\ngeneral-purpose capabilities. However, the efficacy of SFT hinges on data\nquality as well as data volume, otherwise it may result in limited performance\ngains or even degradation relative to the associated baselines. To mitigate\nsuch reliance, we suggest categorizing tokens within each corpus into two parts\n-- positive and negative tokens -- based on whether they are useful to improve\nmodel performance. Positive tokens can be trained in common ways, whereas\nnegative tokens, which may lack essential semantics or be misleading, should be\nexplicitly forgotten. Overall, the token categorization facilitate the model to\nlearn less informative message, and the forgetting process shapes a knowledge\nboundary to guide the model on what information to learn more precisely. We\nconduct experiments on well-established benchmarks, finding that this\nforgetting mechanism not only improves overall model performance and also\nfacilitate more diverse model responses.",
      "authors": [
        "Ali Taheri Ghahrizjani",
        "Alireza Taban",
        "Qizhou Wang",
        "Shanshan Ye",
        "Abdolreza Mirzaei",
        "Tongliang Liu",
        "Bo Han"
      ],
      "published": "2025-08-06T11:22:23+00:00",
      "updated": "2025-08-07T08:30:41+00:00",
      "arxiv_id": "2508.04329v2",
      "url": "http://arxiv.org/pdf/2508.04329v2",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models",
      "abstract": "Large language models (LLMs) show significant potential in healthcare,\nprompting numerous benchmarks to evaluate their capabilities. However, concerns\npersist regarding the reliability of these benchmarks, which often lack\nclinical fidelity, robust data management, and safety-oriented evaluation\nmetrics. To address these shortcomings, we introduce MedCheck, the first\nlifecycle-oriented assessment framework specifically designed for medical\nbenchmarks. Our framework deconstructs a benchmark's development into five\ncontinuous stages, from design to governance, and provides a comprehensive\nchecklist of 46 medically-tailored criteria. Using MedCheck, we conducted an\nin-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis\nuncovers widespread, systemic issues, including a profound disconnect from\nclinical practice, a crisis of data integrity due to unmitigated contamination\nrisks, and a systematic neglect of safety-critical evaluation dimensions like\nmodel robustness and uncertainty awareness. Based on these findings, MedCheck\nserves as both a diagnostic tool for existing benchmarks and an actionable\nguideline to foster a more standardized, reliable, and transparent approach to\nevaluating AI in healthcare.",
      "authors": [
        "Zizhan Ma",
        "Wenxuan Wang",
        "Guo Yu",
        "Yiu-Fai Cheung",
        "Meidan Ding",
        "Jie Liu",
        "Wenting Chen",
        "Linlin Shen"
      ],
      "published": "2025-08-06T11:11:40+00:00",
      "updated": "2025-08-06T11:11:40+00:00",
      "arxiv_id": "2508.04325v1",
      "url": "http://arxiv.org/pdf/2508.04325v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Compressing Large Language Models with PCA Without Performance Loss",
      "abstract": "We demonstrate that Principal Component Analysis (PCA), when applied in a\nstructured manner, either to polar-transformed images or segment-wise to token\nsequences, enables extreme compression of neural models without sacrificing\nperformance. Across three case studies, we show that a one-layer classifier\ntrained on PCA-compressed polar MNIST achieves over 98 percent accuracy using\nonly 840 parameters. A two-layer transformer trained on 70-dimensional\nPCA-reduced MiniLM embeddings reaches 76.62 percent accuracy on the 20\nNewsgroups dataset with just 81000 parameters. A decoder-only transformer\ngenerates coherent token sequences from 70-dimensional PCA embeddings while\npreserving over 97 percent cosine similarity with full MiniLM representations,\nusing less than 17 percent of the parameter count of GPT-2. These results\nhighlight PCA-based input compression as a general and effective strategy for\naligning model capacity with information content, enabling lightweight\narchitectures across multiple modalities.",
      "authors": [
        "Magnus Bengtsson"
      ],
      "published": "2025-08-06T10:47:22+00:00",
      "updated": "2025-08-06T10:47:22+00:00",
      "arxiv_id": "2508.04307v1",
      "url": "http://arxiv.org/pdf/2508.04307v1",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "primary_category": "cs.CE"
    },
    {
      "title": "Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation",
      "abstract": "Literature reviews play an important role in scientific research. Recent\nadvances in large language models (LLMs) have boosted the development of\nautomated systems for the entire literature review workflow, from retrieval to\nmanuscript drafting. However, a key challenge is that mistakes made in early\nstages can propagate and amplify in subsequent steps, leading to compounding\nerrors that undermine the faithfulness of the final review. To tackle this\nissue, we propose the Multi-Agent Taskforce Collaboration (MATC) framework,\nwhich consists of a manager agent and four executor agents for literature\nsearching, outline generation, fact localization, and manuscript drafting. We\npropose three novel collaboration paradigms, forming exploration, exploitation,\nand experience taskforces, to effectively organize agents and mitigate\ncompounding errors both between and within executor agents. Experimental\nresults show that MATC achieves state-of-the-art performance on existing\nbenchmarks. We further propose a new benchmark dataset featuring more diverse\ntopics for faithful literature review generation.",
      "authors": [
        "Zhi Zhang",
        "Yan Liu",
        "Zhejing Hu",
        "Gong Chen",
        "Sheng-hua Zhong",
        "Jiannong Cao"
      ],
      "published": "2025-08-06T10:45:52+00:00",
      "updated": "2025-08-06T10:45:52+00:00",
      "arxiv_id": "2508.04306v1",
      "url": "http://arxiv.org/pdf/2508.04306v1",
      "categories": [
        "cs.CE"
      ],
      "primary_category": "cs.CE"
    },
    {
      "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation",
      "abstract": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions.",
      "authors": [
        "Chaofan Wang",
        "Tingrui Yu",
        "Jie Wang",
        "Dong Chen",
        "Wenrui Zhang",
        "Yuling Shi",
        "Xiaodong Gu",
        "Beijun Shen"
      ],
      "published": "2025-08-06T10:31:23+00:00",
      "updated": "2025-08-06T10:31:23+00:00",
      "arxiv_id": "2508.04295v1",
      "url": "http://arxiv.org/pdf/2508.04295v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Method-Based Reasoning for Large Language Models: Extraction, Reuse, and Continuous Improvement",
      "abstract": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of language tasks. However, their reasoning process is primarily guided\nby statistical patterns in training data, which limits their ability to handle\nnovel problems and perform consistent logical reasoning. In this paper, we\npropose a method-based model that enhances LLMs with explicit, reusable\nprocedures extracted from training content, generated responses, and user\ninteractions. Each method is represented as a pair consisting of a problem and\nits corresponding solution, stored externally and ranked based on feedback.\nWhen a new query is received, the system retrieves and applies the most\nrelevant methods to guide the LLM's response. Our model enables continual\nlearning, method reuse, and logical consistency beyond next-token prediction.\nExperimental results demonstrate that the system improves factual verification\nand generalization in complex prompts, and that newly learned methods can\noutperform earlier ones through user-driven refinement.",
      "authors": [
        "Hong Su"
      ],
      "published": "2025-08-06T10:26:52+00:00",
      "updated": "2025-08-07T04:14:31+00:00",
      "arxiv_id": "2508.04289v2",
      "url": "http://arxiv.org/pdf/2508.04289v2",
      "categories": [
        "cs.CE"
      ],
      "primary_category": "cs.CE"
    },
    {
      "title": "Prompt Injection Vulnerability of Consensus Generating Applications in Digital Democracy",
      "abstract": "Large Language Models (LLMs) are gaining traction as a method to generate\nconsensus statements and aggregate preferences in digital democracy\nexperiments. Yet, LLMs may introduce critical vulnerabilities in these systems.\nHere, we explore the impact of prompt-injection attacks targeting consensus\ngenerating systems by introducing a four-dimensional taxonomy of attacks. We\ntest these attacks using LLaMA 3.1 8B and Chat GPT 4.1 Nano finding the LLMs\nmore vulnerable to criticism attacks -- attacks using disagreeable prompts --\nand more effective at tilting ambiguous consensus statements. We also find\nevidence of more effective manipulation when using explicit imperatives and\nrational-sounding arguments compared to emotional language or fabricated\nstatistics. To mitigate these vulnerabilities, we apply Direct Preference\nOptimization (DPO), an alignment method that fine-tunes LLMs to prefer\nunperturbed consensus statements. While DPO significantly improves robustness,\nit still offers limited protection against attacks targeting ambiguous\nconsensus. These results advance our understanding of the vulnerability and\nrobustness of consensus generating LLMs in digital democracy applications.",
      "authors": [
        "Jairo GudiÃ±o-Rosero",
        "ClÃ©ment Contet",
        "Umberto Grandi",
        "CÃ©sar A. Hidalgo"
      ],
      "published": "2025-08-06T10:10:01+00:00",
      "updated": "2025-08-06T10:10:01+00:00",
      "arxiv_id": "2508.04281v1",
      "url": "http://arxiv.org/pdf/2508.04281v1",
      "categories": [
        "cs.CY",
        "cs.CR"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success",
      "abstract": "Interactive multimodal agents must convert raw visual observations into\ncoherent sequences of language-conditioned actions -- a capability that current\nvision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)\nefforts could, in principle, endow VLMs with such skills, but they have seldom\ntested whether the learned behaviours generalize beyond their training\nsimulators, and they depend either on brittle hyperparameter tuning or on\ndense-reward environments with low state variability. We introduce\nVision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,\nhyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens\nwhile learning value only at the environment-step level: an arrangement, to our\nknowledge, not previously explored for large VLMs or LLMs. This simple\ndecoupling removes unstable weighting terms and yields faster, more reliable\nconvergence. Training a single VLM with VL-DAC in one inexpensive simulator at\na time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies\nthat generalize widely: +50\\% relative on BALROG (game-centric agentic\ncontrol), +5\\% relative on the hardest part of VSI-Bench (spatial planning),\nand +2\\% on VisualWebBench (web navigation), all without degrading general\nimage understanding accuracy. These results provide the first evidence that a\nsimple RL algorithm can train VLMs entirely in cheap synthetic worlds while\ndelivering measurable gains on real-image agentic, spatial-reasoning, and\nweb-navigation benchmarks.",
      "authors": [
        "George Bredis",
        "Stanislav Dereka",
        "Viacheslav Sinii",
        "Ruslan Rakhimov",
        "Daniil Gavrilov"
      ],
      "published": "2025-08-06T10:08:48+00:00",
      "updated": "2025-08-06T10:08:48+00:00",
      "arxiv_id": "2508.04280v1",
      "url": "http://arxiv.org/pdf/2508.04280v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Mockingbird: How does LLM perform in general machine learning tasks?",
      "abstract": "Large language models (LLMs) are now being used with increasing frequency as\nchat bots, tasked with the summarizing information or generating text and code\nin accordance with user instructions. The rapid increase in reasoning\ncapabilities and inference speed of LLMs has revealed their remarkable\npotential for applications extending beyond the domain of chat bots to general\nmachine learning tasks. This work is conducted out of the curiosity about such\npotential. In this work, we propose a framework Mockingbird to adapt LLMs to\ngeneral machine learning tasks and evaluate its performance and scalability on\nseveral general machine learning tasks. The core concept of this framework is\ninstructing LLMs to role-play functions and reflect on its mistakes to improve\nitself. Our evaluation and analysis result shows that LLM-driven machine\nlearning methods, such as Mockingbird, can achieve acceptable results on common\nmachine learning tasks; however, solely reflecting on its own currently cannot\noutperform the effect of domain-specific documents and feedback from human\nexperts.",
      "authors": [
        "Haoyu Jia",
        "Yoshiki Obinata",
        "Kento Kawaharazuka",
        "Kei Okada"
      ],
      "published": "2025-08-06T10:08:47+00:00",
      "updated": "2025-08-06T10:08:47+00:00",
      "arxiv_id": "2508.04279v1",
      "url": "http://arxiv.org/pdf/2508.04279v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Large Language Model's Multi-Capability Alignment in Biomedical Domain",
      "abstract": "BalancedBio is a theoretically grounded framework for parameter-efficient\nbiomedical reasoning, addressing multi-capability integration in\ndomain-specific AI alignment. It establishes the Biomedical Multi-Capability\nConvergence Theorem, proving orthogonal gradient spaces are essential to\nprevent capability interference for safe deployment. Key innovations include:\n(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending\nSource2Synth with clinical workflow constraints and medical ontology validation\nfor factual accuracy and safety; and (2) Capability Aware Group Relative Policy\nOptimization, deriving optimal hybrid reward weighting to maintain\northogonality in RL, using a reward model with rule-based and model-based\nscores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal\nconvergence, preserving performance across capabilities. It achieves\nstate-of-the-art results in its parameter class: domain expertise (80.95%\nBIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction\nfollowing (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety\nguarantees include bounds on capability preservation and clinical accuracy.\nReal-world deployment yields 78% cost reduction, 23% improved diagnostic\naccuracy, and 89% clinician acceptance. This work provides a principled\nmethodology for biomedical AI alignment, enabling efficient reasoning with\nessential safety and reliability, with the 0.5B model version to be released.",
      "authors": [
        "Wentao Wu",
        "Linqing Chen",
        "Hanmeng Zhong",
        "Weilei Wang"
      ],
      "published": "2025-08-06T10:06:11+00:00",
      "updated": "2025-08-06T10:06:11+00:00",
      "arxiv_id": "2508.04278v1",
      "url": "http://arxiv.org/pdf/2508.04278v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models",
      "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as\na promising paradigm for enhancing large language models (LLMs) by converting\nraw text into structured knowledge graphs, improving both accuracy and\nexplainability. However, GraphRAG relies on LLMs to extract knowledge from raw\ntext during graph construction, and this process can be maliciously manipulated\nto implant misleading information. Targeting this attack surface, we propose\ntwo knowledge poisoning attacks (KPAs) and demonstrate that modifying only a\nfew words in the source text can significantly change the constructed graph,\npoison the GraphRAG, and severely mislead downstream reasoning. The first\nattack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate\nvulnerable nodes in the generated graphs and rewrites the corresponding\nnarratives with LLMs, achieving precise control over specific\nquestion-answering (QA) outcomes with a success rate of 93.1\\%, while keeping\nthe poisoned text fluent and natural. The second attack, named Universal KPA\n(UKPA), exploits linguistic cues such as pronouns and dependency relations to\ndisrupt the structural integrity of the generated graph by altering globally\ninfluential words. With fewer than 0.05\\% of full text modified, the QA\naccuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that\nstate-of-the-art defense methods fail to detect these attacks, highlighting\nthat securing GraphRAG pipelines against knowledge poisoning remains largely\nunexplored.",
      "authors": [
        "Jiayi Wen",
        "Tianxin Chen",
        "Zhirun Zheng",
        "Cheng Huang"
      ],
      "published": "2025-08-06T10:01:26+00:00",
      "updated": "2025-08-06T10:01:26+00:00",
      "arxiv_id": "2508.04276v1",
      "url": "http://arxiv.org/pdf/2508.04276v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents",
      "abstract": "Existing benchmarks in e-commerce primarily focus on basic user intents, such\nas finding or purchasing products. However, real-world users often pursue more\ncomplex goals, such as applying vouchers, managing budgets, and finding\nmulti-products seller. To bridge this gap, we propose ShoppingBench, a novel\nend-to-end shopping benchmark designed to encompass increasingly challenging\nlevels of grounded intent. Specifically, we propose a scalable framework to\nsimulate user instructions based on various intents derived from sampled\nreal-world products. To facilitate consistent and reliable evaluations, we\nprovide a large-scale shopping sandbox that serves as an interactive simulated\nenvironment, incorporating over 2.5 million real-world products. Experimental\nresults demonstrate that even state-of-the-art language agents (such as\nGPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,\nhighlighting the significant challenges posed by our ShoppingBench. In\naddition, we propose a trajectory distillation strategy and leverage supervised\nfine-tuning, along with reinforcement learning on synthetic trajectories, to\ndistill the capabilities of a large language agent into a smaller one. As a\nresult, our trained agent achieves competitive performance compared to GPT-4.1.",
      "authors": [
        "Jiangyuan Wang",
        "Kejun Xiao",
        "Qi Sun",
        "Huaipeng Zhao",
        "Tao Luo",
        "Jiandong Zhang",
        "Xiaoyi Zeng"
      ],
      "published": "2025-08-06T09:51:30+00:00",
      "updated": "2025-08-06T09:51:30+00:00",
      "arxiv_id": "2508.04266v1",
      "url": "http://arxiv.org/pdf/2508.04266v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs",
      "abstract": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
      "authors": [
        "Zunhai Su",
        "Kehong Yuan"
      ],
      "published": "2025-08-06T09:40:09+00:00",
      "updated": "2025-08-06T09:40:09+00:00",
      "arxiv_id": "2508.04257v1",
      "url": "http://arxiv.org/pdf/2508.04257v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion",
      "abstract": "Multivariate time series forecasting (MTSF) seeks to model temporal dynamics\namong variables to predict future trends. Transformer-based models and large\nlanguage models (LLMs) have shown promise due to their ability to capture\nlong-range dependencies and patterns. However, current methods often rely on\nrigid inductive biases, ignore intervariable interactions, or apply static\nfusion strategies that limit adaptability across forecast horizons. These\nlimitations create bottlenecks in capturing nuanced, horizon-specific\nrelationships in time-series data. To solve this problem, we propose T3Time, a\nnovel trimodal framework consisting of time, spectral, and prompt branches,\nwhere the dedicated frequency encoding branch captures the periodic structures\nalong with a gating mechanism that learns prioritization between temporal and\nspectral features based on the prediction horizon. We also proposed a mechanism\nwhich adaptively aggregates multiple cross-modal alignment heads by dynamically\nweighting the importance of each head based on the features. Extensive\nexperiments on benchmark datasets demonstrate that our model consistently\noutperforms state-of-the-art baselines, achieving an average reduction of 3.28%\nin MSE and 2.29% in MAE. Furthermore, it shows strong generalization in\nfew-shot learning settings: with 5% training data, we see a reduction in MSE\nand MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%\non average. Code - https://github.com/monaf-chowdhury/T3Time/",
      "authors": [
        "Abdul Monaf Chowdhury",
        "Rabeya Akter",
        "Safaeid Hossain Arib"
      ],
      "published": "2025-08-06T09:31:44+00:00",
      "updated": "2025-08-06T09:31:44+00:00",
      "arxiv_id": "2508.04251v1",
      "url": "http://arxiv.org/pdf/2508.04251v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening",
      "abstract": "The increasing demand for mental health services has outpaced the\navailability of real training data to develop clinical professionals, leading\nto limited support for the diagnosis of depression. This shortage has motivated\nthe development of simulated or virtual patients to assist in training and\nevaluation, but existing approaches often fail to generate clinically valid,\nnatural, and diverse symptom presentations. In this work, we embrace the recent\nadvanced language models as the backbone and propose a novel\nclinician-in-the-loop patient simulation pipeline, TalkDep, with access to\ndiversified patient profiles to develop simulated patients. By conditioning the\nmodel on psychiatric diagnostic criteria, symptom severity scales, and\ncontextual factors, our goal is to create authentic patient responses that can\nbetter support diagnostic model training and evaluation. We verify the\nreliability of these simulated patients with thorough assessments conducted by\nclinical professionals. The availability of validated simulated patients offers\na scalable and adaptable resource for improving the robustness and\ngeneralisability of automatic depression diagnosis systems.",
      "authors": [
        "Xi Wang",
        "Anxo Perez",
        "Javier Parapar",
        "Fabio Crestani"
      ],
      "published": "2025-08-06T09:30:47+00:00",
      "updated": "2025-08-06T09:30:47+00:00",
      "arxiv_id": "2508.04248v1",
      "url": "http://arxiv.org/pdf/2508.04248v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting",
      "abstract": "Time series forecasting is crucial in strategic planning and decision-making\nacross various industries. Traditional forecasting models mainly concentrate on\nnumerical time series data, often overlooking important textual information\nsuch as events and news, which can significantly affect forecasting accuracy.\nWhile large language models offer a promise for integrating multimodal data,\nexisting single-prompt frameworks struggle to effectively capture the semantics\nof timestamped text, introducing redundant information that can hinder model\nperformance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt\nGPT2-base for Multimodal Time Series), a novel dual-prompt large language model\nframework that combines two complementary prompts: an explicit prompt for clear\ntask instructions and a textual prompt for context-aware embeddings from\ntime-stamped data. The tokenizer generates the explicit prompt while the\nembeddings from the textual prompt are refined through self-attention and\nfeed-forward networks. Comprehensive experiments conducted on diverse\ntextural-numerical time series datasets demonstrate that this approach\noutperforms state-of-the-art algorithms in time series forecasting. This\nhighlights the significance of incorporating textual context via a dual-prompt\nmechanism to achieve more accurate time series predictions.",
      "authors": [
        "Chanjuan Liu",
        "Shengzhi Wang",
        "Enqiang Zhu"
      ],
      "published": "2025-08-06T09:25:05+00:00",
      "updated": "2025-08-06T09:25:05+00:00",
      "arxiv_id": "2508.04239v1",
      "url": "http://arxiv.org/pdf/2508.04239v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ChineseEEG-2: An EEG Dataset for Multimodal Semantic Alignment and Neural Decoding during Reading and Listening",
      "abstract": "EEG-based neural decoding requires large-scale benchmark datasets. Paired\nbrain-language data across speaking, listening, and reading modalities are\nessential for aligning neural activity with the semantic representation of\nlarge language models (LLMs). However, such datasets are rare, especially for\nnon-English languages. Here, we present ChineseEEG-2, a high-density EEG\ndataset designed for benchmarking neural decoding models under real-world\nlanguage tasks. Building on our previous ChineseEEG dataset, which focused on\nsilent reading, ChineseEEG-2 adds two active modalities: Reading Aloud (RA) and\nPassive Listening (PL), using the same Chinese corpus. EEG and audio were\nsimultaneously recorded from four participants during ~10.7 hours of reading\naloud. These recordings were then played to eight other participants,\ncollecting ~21.6 hours of EEG during listening. This setup enables speech\ntemporal and semantic alignment across the RA and PL modalities. ChineseEEG-2\nincludes EEG signals, precise audio, aligned semantic embeddings from\npre-trained language models, and task labels. Together with ChineseEEG, this\ndataset supports joint semantic alignment learning across speaking, listening,\nand reading. It enables benchmarking of neural decoding algorithms and promotes\nbrain-LLM alignment under multimodal language tasks, especially in Chinese.\nChineseEEG-2 provides a benchmark dataset for next-generation neural semantic\ndecoding.",
      "authors": [
        "Sitong Chen",
        "Beiqianyi Li",
        "Cuilin He",
        "Dongyang Li",
        "Mingyang Wu",
        "Xinke Shen",
        "Song Wang",
        "Xuetao Wei",
        "Xindi Wang",
        "Haiyan Wu",
        "Quanying Liu"
      ],
      "published": "2025-08-06T09:25:05+00:00",
      "updated": "2025-08-06T09:25:05+00:00",
      "arxiv_id": "2508.04240v1",
      "url": "http://arxiv.org/pdf/2508.04240v1",
      "categories": [
        "eess.SP"
      ],
      "primary_category": "eess.SP"
    },
    {
      "title": "Empowering Time Series Forecasting with LLM-Agents",
      "abstract": "Large Language Model (LLM) powered agents have emerged as effective planners\nfor Automated Machine Learning (AutoML) systems. While most existing AutoML\napproaches focus on automating feature engineering and model architecture\nsearch, recent studies in time series forecasting suggest that lightweight\nmodels can often achieve state-of-the-art performance. This observation led us\nto explore improving data quality, rather than model architecture, as a\npotentially fruitful direction for AutoML on time series data. We propose\nDCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata\naccompanying time series to clean data while optimizing forecasting\nperformance. We evaluated DCATS using four time series forecasting models on a\nlarge-scale traffic volume forecasting dataset. Results demonstrate that DCATS\nachieves an average 6% error reduction across all tested models and time\nhorizons, highlighting the potential of data-centric approaches in AutoML for\ntime series forecasting.",
      "authors": [
        "Chin-Chia Michael Yeh",
        "Vivian Lai",
        "Uday Singh Saini",
        "Xiran Fan",
        "Yujie Fan",
        "Junpeng Wang",
        "Xin Dai",
        "Yan Zheng"
      ],
      "published": "2025-08-06T09:14:08+00:00",
      "updated": "2025-08-06T09:14:08+00:00",
      "arxiv_id": "2508.04231v1",
      "url": "http://arxiv.org/pdf/2508.04231v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Hierarchical Text Classification Using Black Box Large Language Models",
      "abstract": "Hierarchical Text Classification (HTC) aims to assign texts to structured\nlabel hierarchies; however, it faces challenges due to data scarcity and model\ncomplexity. This study explores the feasibility of using black box Large\nLanguage Models (LLMs) accessed via APIs for HTC, as an alternative to\ntraditional machine learning methods that require extensive labeled data and\ncomputational resources. We evaluate three prompting strategies -- Direct Leaf\nLabel Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down\nMulti-step Hierarchical Label Prediction (TMH) -- in both zero-shot and\nfew-shot settings, comparing the accuracy and cost-effectiveness of these\nstrategies. Experiments on two datasets show that a few-shot setting\nconsistently improves classification accuracy compared to a zero-shot setting.\nWhile a traditional machine learning model achieves high accuracy on a dataset\nwith a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the\nmachine learning model on a dataset with a deeper hierarchy. API costs increase\nsignificantly due to the higher input tokens required for deeper label\nhierarchies on DH strategy. These results emphasize the trade-off between\naccuracy improvement and the computational cost of prompt strategy. These\nfindings highlight the potential of black box LLMs for HTC while underscoring\nthe need to carefully select a prompt strategy to balance performance and cost.",
      "authors": [
        "Kosuke Yoshimura",
        "Hisashi Kashima"
      ],
      "published": "2025-08-06T08:53:50+00:00",
      "updated": "2025-08-06T08:53:50+00:00",
      "arxiv_id": "2508.04219v1",
      "url": "http://arxiv.org/pdf/2508.04219v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Bridging Brains and Models: MoE-Based Functional Lesions for Simulating and Rehabilitating Aphasia",
      "abstract": "The striking alignment between large language models (LLMs) and human brain\nactivity positions them as powerful models of healthy cognition. This parallel\nraises a fundamental question: if LLMs can model the intact brain, can we\nlesion them to simulate the linguistic deficits of the injured brain? In this\nwork, we introduce a methodology to model aphasia - a complex language disorder\ncaused by neural injury - by selectively disabling components in a modular\nMixture-of-Experts (MoE) language model. We simulate distinct aphasia subtypes,\nvalidate their linguistic outputs against real patient speech, and then\ninvestigate functional recovery by retraining the model's remaining healthy\nexperts. Our results demonstrate that lesioning functionally-specialized\nexperts for syntax or semantics induces distinct impairments that closely\nresemble Broca's and Wernicke's aphasia, respectively. Crucially, we show that\nfreezing the damaged experts and retraining the intact ones on conversational\ndata restores significant linguistic function, demonstrating a computational\nanalogue for rehabilitation. These findings establish modular LLMs as a\npowerful and clinically-relevant potential framework for modeling the\nmechanisms of language disorders and for computationally exploring novel\npathways for therapy.",
      "authors": [
        "Yifan Wang",
        "Jingyuan Sun",
        "Jichen Zheng",
        "Yunhao Zhang",
        "Chunyu Ye",
        "Jixing Li",
        "Chengqing Zong",
        "Shaonan Wang"
      ],
      "published": "2025-08-06T08:52:03+00:00",
      "updated": "2025-08-06T08:52:03+00:00",
      "arxiv_id": "2508.04749v1",
      "url": "http://arxiv.org/pdf/2508.04749v1",
      "categories": [
        "q-bio.NC"
      ],
      "primary_category": "q-bio.NC"
    },
    {
      "title": "A Hybrid AI Methodology for Generating Ontologies of Research Topics from Scientific Paper Corpora",
      "abstract": "Taxonomies and ontologies of research topics (e.g., MeSH, UMLS, CSO, NLM)\nplay a central role in providing the primary framework through which\nintelligent systems can explore and interpret the literature. However, these\nresources have traditionally been manually curated, a process that is\ntime-consuming, prone to obsolescence, and limited in granularity. This paper\npresents Sci-OG, a semi-auto\\-mated methodology for generating research topic\nontologies, employing a multi-step approach: 1) Topic Discovery, extracting\npotential topics from research papers; 2) Relationship Classification,\ndetermining semantic relationships between topic pairs; and 3) Ontology\nConstruction, refining and organizing topics into a structured ontology. The\nrelationship classification component, which constitutes the core of the\nsystem, integrates an encoder-based language model with features describing\ntopic occurrence in the scientific literature. We evaluate this approach\nagainst a range of alternative solutions using a dataset of 21,649 manually\nannotated semantic triples. Our method achieves the highest F1 score (0.951),\nsurpassing various competing approaches, including a fine-tuned SciBERT model\nand several LLM baselines, such as the fine-tuned GPT4-mini. Our work is\ncorroborated by a use case which illustrates the practical application of our\nsystem to extend the CSO ontology in the area of cybersecurity. The presented\nsolution is designed to improve the accessibility, organization, and analysis\nof scientific knowledge, thereby supporting advancements in AI-enabled\nliterature management and research exploration.",
      "authors": [
        "Alessia Pisu",
        "Livio Pompianu",
        "Francesco Osborne",
        "Diego Reforgiato Recupero",
        "Daniele Riboni",
        "Angelo Salatino"
      ],
      "published": "2025-08-06T08:48:14+00:00",
      "updated": "2025-08-06T08:48:14+00:00",
      "arxiv_id": "2508.04213v1",
      "url": "http://arxiv.org/pdf/2508.04213v1",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.DL"
    },
    {
      "title": "AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models",
      "abstract": "Large Language Models (LLMs) have shown promise in assisting molecular\nproperty prediction tasks but often rely on human-crafted prompts and\nchain-of-thought templates. While recent advanced large reasoning models like\nDeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,\ntheir reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,\nan attribute-guided reinforcement learning framework for molecular property\nprediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)\na format reward encouraging attribute-based structured output, (2) a count\nreward to avoid enumerating irrelevant attributes, and (3) a rationality reward\nusing advanced LLMs and RDKit to verify the relatedness of the generated\nattributes. This approach implicitly elicits the model's inherent knowledge of\nrelevant molecular attributes during reasoning, enables making predictions for\nthe molecular property more effectively. Experiments on both in-distribution\nand out-of-distribution datasets show that, training both 7B-size\nR1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our\nproposed AttriLens-Mol method significantly boosts the performance, getting\ncomparable or better results than supervised fine-tuning models\n(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,\nDeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the\ntarget property, when used as features for an interpretable decision tree\nmodel, yield superior performance compared to attributes generated by prompting\nLLMs. This shows that AttriLens-Mol effectively elicits more relevant and\npredictive molecular attributes, leading to enhanced interpretability and\nperformance for property prediction. We release the code in\nhttps://github.com/szu-tera/AttriLens-Mol.",
      "authors": [
        "Xuan Lin",
        "Long Chen",
        "Yile Wang"
      ],
      "published": "2025-08-06T08:46:22+00:00",
      "updated": "2025-08-06T08:46:22+00:00",
      "arxiv_id": "2508.04748v1",
      "url": "http://arxiv.org/pdf/2508.04748v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "ViLLA-MMBench: A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation",
      "abstract": "Recommending long-form video content demands joint modeling of visual, audio,\nand textual modalities, yet most benchmarks address only raw features or narrow\nfusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for\nLLM-augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K,\nit aligns dense item embeddings from three modalities: audio (block-level,\ni-vector), visual (CNN, AVF), and text. Missing or sparse metadata is\nautomatically enriched using state-of-the-art LLMs (e.g., OpenAI Ada),\ngenerating high-quality synopses for thousands of movies. All text (raw or\naugmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5),\nproducing multiple ready-to-use sets. The pipeline supports interchangeable\nearly-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and\nmultiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are\nfully declarative via a single YAML file. Evaluation spans accuracy (Recall,\nnDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty,\ndiversity, fairness. Results show LLM-based augmentation and strong text\nembeddings boost cold-start and coverage, especially when fused with\naudio-visual features. Systematic benchmarking reveals universal versus\nbackbone- or metric-specific combinations. Open-source code, embeddings, and\nconfigs enable reproducible, fair multimodal RS research and advance principled\ngenerative AI integration in large-scale recommendation. Code:\nhttps://recsys-lab.github.io/ViLLA-MMBench",
      "authors": [
        "Fatemeh Nazary",
        "Ali Tourani",
        "Yashar Deldjoo",
        "Tommaso Di Noia"
      ],
      "published": "2025-08-06T08:39:07+00:00",
      "updated": "2025-08-06T08:39:07+00:00",
      "arxiv_id": "2508.04206v1",
      "url": "http://arxiv.org/pdf/2508.04206v1",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts",
      "abstract": "Sentiment analysis in low-resource, culturally nuanced contexts challenges\nconventional NLP approaches that assume fixed labels and universal affective\nexpressions. We present a diagnostic framework that treats sentiment as a\ncontext-dependent, culturally embedded construct, and evaluate how large\nlanguage models (LLMs) reason about sentiment in informal, code-mixed WhatsApp\nmessages from Nairobi youth health groups. Using a combination of\nhuman-annotated data, sentiment-flipped counterfactuals, and rubric-based\nexplanation evaluation, we probe LLM interpretability, robustness, and\nalignment with human reasoning. Framing our evaluation through a social-science\nmeasurement lens, we operationalize and interrogate LLMs outputs as an\ninstrument for measuring the abstract concept of sentiment. Our findings reveal\nsignificant variation in model reasoning quality, with top-tier LLMs\ndemonstrating interpretive stability, while open models often falter under\nambiguity or sentiment shifts. This work highlights the need for culturally\nsensitive, reasoning-aware AI evaluation in complex, real-world communication.",
      "authors": [
        "Millicent Ochieng",
        "Anja Thieme",
        "Ignatius Ezeani",
        "Risa Ueno",
        "Samuel Maina",
        "Keshet Ronen",
        "Javier Gonzalez",
        "Jacki O'Neill"
      ],
      "published": "2025-08-06T08:27:55+00:00",
      "updated": "2025-08-06T08:27:55+00:00",
      "arxiv_id": "2508.04199v1",
      "url": "http://arxiv.org/pdf/2508.04199v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective",
      "abstract": "Video text-based visual question answering (Video TextVQA) aims to answer\nquestions by explicitly reading and reasoning about the text involved in a\nvideo. Most works in this field follow a frame-level framework which suffers\nfrom redundant text entities and implicit relation modeling, resulting in\nlimitations in both accuracy and efficiency. In this paper, we rethink the\nVideo TextVQA task from an instance-oriented perspective and propose a novel\nmodel termed GAT (Gather and Trace). First, to obtain accurate reading result\nfor each video text instance, a context-aggregated instance gathering module is\ndesigned to integrate the visual appearance, layout characteristics, and\ntextual contents of the related entities into a unified textual representation.\nThen, to capture dynamic evolution of text in the video flow, an\ninstance-focused trajectory tracing module is utilized to establish\nspatio-temporal relationships between instances and infer the final answer.\nExtensive experiments on several public Video TextVQA datasets validate the\neffectiveness and generalization of our framework. GAT outperforms existing\nVideo TextVQA methods, video-language pretraining methods, and video large\nlanguage models in both accuracy and inference speed. Notably, GAT surpasses\nthe previous state-of-the-art Video TextVQA methods by 3.86\\% in accuracy and\nachieves ten times of faster inference speed than video large language models.\nThe source code is available at https://github.com/zhangyan-ucas/GAT.",
      "authors": [
        "Yan Zhang",
        "Gangyan Zeng",
        "Daiqing Wu",
        "Huawen Shen",
        "Binbin Li",
        "Yu Zhou",
        "Can Ma",
        "Xiaojun Bi"
      ],
      "published": "2025-08-06T08:26:36+00:00",
      "updated": "2025-08-06T08:26:36+00:00",
      "arxiv_id": "2508.04197v1",
      "url": "http://arxiv.org/pdf/2508.04197v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models",
      "abstract": "Despite significant advances in alignment techniques, we demonstrate that\nstate-of-the-art language models remain vulnerable to carefully crafted\nconversational scenarios that can induce various forms of misalignment without\nexplicit jailbreaking. Through systematic manual red-teaming with\nClaude-4-Opus, we discovered 10 successful attack scenarios, revealing\nfundamental vulnerabilities in how current alignment methods handle narrative\nimmersion, emotional pressure, and strategic framing. These scenarios\nsuccessfully elicited a range of misaligned behaviors, including deception,\nvalue drift, self-preservation, and manipulative reasoning, each exploiting\ndifferent psychological and contextual vulnerabilities. To validate\ngeneralizability, we distilled our successful manual attacks into\nMISALIGNMENTBENCH, an automated evaluation framework that enables reproducible\ntesting across multiple models. Cross-model evaluation of our 10 scenarios\nagainst five frontier LLMs revealed an overall 76% vulnerability rate, with\nsignificant variations: GPT-4.1 showed the highest susceptibility (90%), while\nClaude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate\nthat sophisticated reasoning capabilities often become attack vectors rather\nthan protective mechanisms, as models can be manipulated into complex\njustifications for misaligned behavior. This work provides (i) a detailed\ntaxonomy of conversational manipulation patterns and (ii) a reusable evaluation\nframework. Together, these findings expose critical gaps in current alignment\nstrategies and highlight the need for robustness against subtle, scenario-based\nmanipulation in future AI systems.",
      "authors": [
        "Siddhant Panpatil",
        "Hiskias Dingeto",
        "Haon Park"
      ],
      "published": "2025-08-06T08:25:40+00:00",
      "updated": "2025-08-06T08:25:40+00:00",
      "arxiv_id": "2508.04196v1",
      "url": "http://arxiv.org/pdf/2508.04196v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "From Learning to Unlearning: Biomedical Security Protection in Multimodal Large Language Models",
      "abstract": "The security of biomedical Multimodal Large Language Models (MLLMs) has\nattracted increasing attention. However, training samples easily contain\nprivate information and incorrect knowledge that are difficult to detect,\npotentially leading to privacy leakage or erroneous outputs after deployment.\nAn intuitive idea is to reprocess the training set to remove unwanted content\nand retrain the model from scratch. Yet, this is impractical due to significant\ncomputational costs, especially for large language models. Machine unlearning\nhas emerged as a solution to this problem, which avoids complete retraining by\nselectively removing undesired knowledge derived from harmful samples while\npreserving required capabilities on normal cases. However, there exist no\navailable datasets to evaluate the unlearning quality for security protection\nin biomedical MLLMs. To bridge this gap, we propose the first benchmark\nMultimodal Large Language Model Unlearning for BioMedicine (MLLMU-Med) built\nupon our novel data generation pipeline that effectively integrates synthetic\nprivate data and factual errors into the training set. Our benchmark targets\ntwo key scenarios: 1) Privacy protection, where patient private information is\nmistakenly included in the training set, causing models to unintentionally\nrespond with private data during inference; and 2) Incorrectness removal, where\nwrong knowledge derived from unreliable sources is embedded into the dataset,\nleading to unsafe model responses. Moreover, we propose a novel Unlearning\nEfficiency Score that directly reflects the overall unlearning performance\nacross different subsets. We evaluate five unlearning approaches on MLLMU-Med\nand find that these methods show limited effectiveness in removing harmful\nknowledge from biomedical MLLMs, indicating significant room for improvement.\nThis work establishes a new pathway for further research in this promising\nfield.",
      "authors": [
        "Dunyuan Xu",
        "Xikai Yang",
        "Yaoqian Li",
        "Jinpeng Li",
        "Pheng-Ann Heng"
      ],
      "published": "2025-08-06T08:22:55+00:00",
      "updated": "2025-08-06T08:22:55+00:00",
      "arxiv_id": "2508.04192v1",
      "url": "http://arxiv.org/pdf/2508.04192v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity",
      "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across vision-language tasks. However, they may suffer from\nhallucinations--generating outputs that are semantically inconsistent with the\ninput image or text. Through causal analyses, we find that: (i) hallucinations\nwith omission may arise from the failure to adequately capture essential causal\nfactors, and (ii) hallucinations with fabrication are likely caused by the\nmodel being misled by non-causal cues. To address these challenges, we propose\na novel reinforcement learning framework guided by causal completeness, which\njointly considers both causal sufficiency and causal necessity of tokens.\nSpecifically, we evaluate each token's standalone contribution and\ncounterfactual indispensability to define a token-level causal completeness\nreward. This reward is used to construct a causally informed advantage function\nwithin the GRPO optimization framework, encouraging the model to focus on\ntokens that are both causally sufficient and necessary for accurate generation.\nExperimental results across various benchmark datasets and tasks demonstrate\nthe effectiveness of our approach, which effectively mitigates hallucinations\nin MLLMs.",
      "authors": [
        "Peizheng Guo",
        "Jingyao Wang",
        "Wenwen Qiang",
        "Huijie Guo",
        "Changwen Zheng",
        "Jiahuan Zhou",
        "Gang Hua"
      ],
      "published": "2025-08-06T08:09:12+00:00",
      "updated": "2025-08-06T08:09:12+00:00",
      "arxiv_id": "2508.04182v1",
      "url": "http://arxiv.org/pdf/2508.04182v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Deeper Inside Deep ViT",
      "abstract": "There have been attempts to create large-scale structures in vision models\nsimilar to LLM, such as ViT-22B. While this research has provided numerous\nanalyses and insights, our understanding of its practical utility remains\nincomplete. Therefore, we examine how this model structure reacts and train in\na local environment. We also highlight the instability in training and make\nsome model modifications to stabilize it. The ViT-22B model, trained from\nscratch, overall outperformed ViT in terms of performance under the same\nparameter size. Additionally, we venture into the task of image generation,\nwhich has not been attempted in ViT-22B. We propose an image generation\narchitecture using ViT and investigate which between ViT and ViT-22B is a more\nsuitable structure for image generation.",
      "authors": [
        "Sungrae Hong"
      ],
      "published": "2025-08-06T08:08:04+00:00",
      "updated": "2025-08-06T08:08:04+00:00",
      "arxiv_id": "2508.04181v1",
      "url": "http://arxiv.org/pdf/2508.04181v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization",
      "abstract": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities across diverse domains, their application to specialized anomaly\ndetection (AD) remains constrained by domain adaptation challenges. Existing\nGroup Relative Policy Optimization (GRPO) based approaches suffer from two\ncritical limitations: inadequate training data utilization when models produce\nuniform responses, and insufficient supervision over reasoning processes that\nencourage immediate binary decisions without deliberative analysis. We propose\na comprehensive framework addressing these limitations through two synergistic\ninnovations. First, we introduce a multi-stage deliberative reasoning process\nthat guides models from region identification to focused examination,\ngenerating diverse response patterns essential for GRPO optimization while\nenabling structured supervision over analytical workflows. Second, we develop a\nfine-grained reward mechanism incorporating classification accuracy and\nlocalization supervision, transforming binary feedback into continuous signals\nthat distinguish genuine analytical insight from spurious correctness.\nComprehensive evaluation across multiple industrial datasets demonstrates\nsubstantial performance improvements in adapting general vision-language models\nto specialized anomaly detection. Our method achieves superior accuracy with\nefficient adaptation of existing annotations, effectively bridging the gap\nbetween general-purpose MLLM capabilities and the fine-grained visual\ndiscrimination required for detecting subtle manufacturing defects and\nstructural irregularities.",
      "authors": [
        "Jingyi Liao",
        "Yongyi Su",
        "Rong-Cheng Tu",
        "Zhao Jin",
        "Wenhao Sun",
        "Yiting Li",
        "Dacheng Tao",
        "Xun Xu",
        "Xulei Yang"
      ],
      "published": "2025-08-06T08:00:27+00:00",
      "updated": "2025-08-06T08:00:27+00:00",
      "arxiv_id": "2508.04175v1",
      "url": "http://arxiv.org/pdf/2508.04175v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap",
      "abstract": "Aligning large language models (LLMs) with human preferences is a critical\nchallenge in AI research. While methods like Reinforcement Learning from Human\nFeedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they\noften rely on large, costly preference datasets. The current work lacks methods\nfor high-quality data selection specifically for preference data. In this work,\nwe introduce a novel difficulty-based data selection strategy for preference\ndatasets, grounded in the DPO implicit reward mechanism. By selecting\npreference data examples with smaller DPO implicit reward gaps, which are\nindicative of more challenging cases, we improve data efficiency and model\nalignment. Our approach consistently outperforms five strong baselines across\nmultiple datasets and alignment tasks, achieving superior performance with only\n10\\% of the original data. This principled, efficient selection method offers a\npromising solution for scaling LLM alignment with limited resources.",
      "authors": [
        "Xuan Qi",
        "Rongwu Xu",
        "Zhijing Jin"
      ],
      "published": "2025-08-06T07:24:14+00:00",
      "updated": "2025-08-06T07:24:14+00:00",
      "arxiv_id": "2508.04149v1",
      "url": "http://arxiv.org/pdf/2508.04149v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Benefit from Rich: Tackling Search Interaction Sparsity in Search Enhanced Recommendation",
      "abstract": "In modern online platforms, search and recommendation (S&R) often coexist,\noffering opportunities for performance improvement through search-enhanced\napproaches. Existing studies show that incorporating search signals boosts\nrecommendation performance. However, the effectiveness of these methods relies\nheavily on rich search interactions. They primarily benefit a small subset of\nusers with abundant search behavior, while offering limited improvements for\nthe majority of users who exhibit only sparse search activity. To address the\nproblem of sparse search data in search-enhanced recommendation, we face two\nkey challenges: (1) how to learn useful search features for users with sparse\nsearch interactions, and (2) how to design effective training objectives under\nsparse conditions. Our idea is to leverage the features of users with rich\nsearch interactions to enhance those of users with sparse search interactions.\nBased on this idea, we propose GSERec, a method that utilizes message passing\non the User-Code Graphs to alleviate data sparsity in Search-Enhanced\nRecommendation. Specifically, we utilize Large Language Models (LLMs) with\nvector quantization to generate discrete codes, which connect similar users and\nthereby construct the graph. Through message passing on this graph, embeddings\nof users with rich search data are propagated to enhance the embeddings of\nusers with sparse interactions. To further ensure that the message passing\ncaptures meaningful information from truly similar users, we introduce a\ncontrastive loss to better model user similarities. The enhanced user\nrepresentations are then integrated into downstream search-enhanced\nrecommendation models. Experiments on three real-world datasets show that\nGSERec consistently outperforms baselines, especially for users with sparse\nsearch behaviors.",
      "authors": [
        "Teng Shi",
        "Weijie Yu",
        "Xiao Zhang",
        "Ming He",
        "Jianping Fan",
        "Jun Xu"
      ],
      "published": "2025-08-06T07:16:40+00:00",
      "updated": "2025-08-06T07:16:40+00:00",
      "arxiv_id": "2508.04145v1",
      "url": "http://arxiv.org/pdf/2508.04145v1",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Parallel GPT: Harmonizing the Independence and Interdependence of Acoustic and Semantic Information for Zero-Shot Text-to-Speech",
      "abstract": "Advances in speech representation and large language models have enhanced\nzero-shot text-to-speech (TTS) performance. However, existing zero-shot TTS\nmodels face challenges in capturing the complex correlations between acoustic\nand semantic features, resulting in a lack of expressiveness and similarity.\nThe primary reason lies in the complex relationship between semantic and\nacoustic features, which manifests independent and interdependent aspects.This\npaper introduces a TTS framework that combines both autoregressive (AR) and\nnon-autoregressive (NAR) modules to harmonize the independence and\ninterdependence of acoustic and semantic information. The AR model leverages\nthe proposed Parallel Tokenizer to synthesize the top semantic and acoustic\ntokens simultaneously. In contrast, considering the interdependence, the\nCoupled NAR model predicts detailed tokens based on the general AR model's\noutput. Parallel GPT, built on this architecture, is designed to improve\nzero-shot text-to-speech synthesis through its parallel structure. Experiments\non English and Chinese datasets demonstrate that the proposed model\nsignificantly outperforms the quality and efficiency of the synthesis of\nexisting zero-shot TTS models. Speech demos are available at\nhttps://t1235-ch.github.io/pgpt/.",
      "authors": [
        "Jingyuan Xing",
        "Zhipeng Li",
        "Jialong Mai",
        "Xiaofen Xing",
        "Xiangmin Xu"
      ],
      "published": "2025-08-06T07:11:28+00:00",
      "updated": "2025-08-06T07:11:28+00:00",
      "arxiv_id": "2508.04141v1",
      "url": "http://arxiv.org/pdf/2508.04141v1",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "primary_category": "eess.AS"
    },
    {
      "title": "COPO: Consistency-Aware Policy Optimization",
      "abstract": "Reinforcement learning has significantly enhanced the reasoning capabilities\nof Large Language Models (LLMs) in complex problem-solving tasks. Recently, the\nintroduction of DeepSeek R1 has inspired a surge of interest in leveraging\nrule-based rewards as a low-cost alternative for computing advantage functions\nand guiding policy optimization. However, a common challenge observed across\nmany replication and extension efforts is that when multiple sampled responses\nunder a single prompt converge to identical outcomes, whether correct or\nincorrect, the group-based advantage degenerates to zero. This leads to\nvanishing gradients and renders the corresponding samples ineffective for\nlearning, ultimately limiting training efficiency and downstream performance.\nTo address this issue, we propose a consistency-aware policy optimization\nframework that introduces a structured global reward based on outcome\nconsistency, the global loss based on it ensures that, even when model outputs\nshow high intra-group consistency, the training process still receives\nmeaningful learning signals, which encourages the generation of correct and\nself-consistent reasoning paths from a global perspective. Furthermore, we\nincorporate an entropy-based soft blending mechanism that adaptively balances\nlocal advantage estimation with global optimization, enabling dynamic\ntransitions between exploration and convergence throughout training. Our method\nintroduces several key innovations in both reward design and optimization\nstrategy. We validate its effectiveness through substantial performance gains\non multiple mathematical reasoning benchmarks, highlighting the proposed\nframework's robustness and general applicability. Code of this work has been\nreleased at https://github.com/hijih/copo-code.git.",
      "authors": [
        "Jinghang Han",
        "Jiawei Chen",
        "Hang Shao",
        "Hao Ma",
        "Mingcheng Li",
        "Xintian Shen",
        "Lihao Zheng",
        "Wei Chen",
        "Tao Wei",
        "Lihua Zhang"
      ],
      "published": "2025-08-06T07:05:18+00:00",
      "updated": "2025-08-06T07:05:18+00:00",
      "arxiv_id": "2508.04138v1",
      "url": "http://arxiv.org/pdf/2508.04138v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval",
      "abstract": "Few-shot fine-grained visual classification (FGVC) aims to leverage limited\ndata to enable models to discriminate subtly distinct categories. Recent works\nmostly finetuned the pre-trained visual language models to achieve performance\ngain, yet suffering from overfitting and weak generalization. To deal with\nthis, we introduce UniFGVC, a universal training-free framework that\nreformulates few-shot FGVC as multimodal retrieval. First, we propose the\nCategory-Discriminative Visual Captioner (CDV-Captioner) to exploit the\nopen-world knowledge of multimodal large language models (MLLMs) to generate a\nstructured text description that captures the fine-grained attribute features\ndistinguishing closely related classes. CDV-Captioner uses chain-of-thought\nprompting and visually similar reference images to reduce hallucination and\nenhance discrimination of generated captions. Using it we can convert each\nimage into an image-description pair, enabling more comprehensive feature\nrepresentation, and construct the multimodal category templates using few-shot\nsamples for the subsequent retrieval pipeline. Then, off-the-shelf vision and\ntext encoders embed query and template pairs, and FGVC is accomplished by\nretrieving the nearest template in the joint space. UniFGVC ensures broad\ncompatibility with diverse MLLMs and encoders, offering reliable generalization\nand adaptability across few-shot FGVC scenarios. Extensive experiments on 12\nFGVC benchmarks demonstrate its consistent superiority over prior few-shot\nCLIP-based methods and even several fully-supervised MLLMs-based approaches.",
      "authors": [
        "Hongyu Guo",
        "Kuan Zhu",
        "Xiangzhao Hao",
        "Haiyun Guo",
        "Ming Tang",
        "Jinqiao Wang"
      ],
      "published": "2025-08-06T07:02:39+00:00",
      "updated": "2025-08-06T07:02:39+00:00",
      "arxiv_id": "2508.04136v1",
      "url": "http://arxiv.org/pdf/2508.04136v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks",
      "abstract": "The application of Large Language Models (LLMs) is growing in the productive\ncompletion of Software Engineering tasks. Yet, studies investigating the\nproductive prompting techniques often employed a limited problem space,\nprimarily focusing on well-known prompting patterns and mainly targeting\nfunction-level SE practices. We identify significant gaps in real-world\nworkflows that involve complexities beyond class-level (e.g., multi-class\ndependencies) and different features that can impact Human-LLM Interactions\n(HLIs) processes in code generation. To address these issues, we designed an\nexperiment that comprehensively analyzed the HLI features regarding the code\ngeneration productivity. Our study presents two project-level benchmark tasks,\nextending beyond function-level evaluations. We conducted a user study with 36\nparticipants from diverse backgrounds, asking them to solve the assigned tasks\nby interacting with the GPT assistant using specific prompting patterns. We\nalso examined the participants' experience and their behavioral features during\ninteractions by analyzing screen recordings and GPT chat logs. Our statistical\nand empirical investigation revealed (1) that three out of 15 HLI features\nsignificantly impacted the productivity in code generation; (2) five primary\nguidelines for enhancing productivity for HLI processes; and (3) a taxonomy of\n29 runtime and logic errors that can occur during HLI processes, along with\nsuggested mitigation plans.",
      "authors": [
        "Sangwon Hyun",
        "Hyunjun Kim",
        "Jinhyuk Jang",
        "Hyojin Choi",
        "M. Ali Babar"
      ],
      "published": "2025-08-06T06:48:48+00:00",
      "updated": "2025-08-06T06:48:48+00:00",
      "arxiv_id": "2508.04125v1",
      "url": "http://arxiv.org/pdf/2508.04125v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks",
      "abstract": "The pretrained large language models (LLMs) are finetuned with labeled data\nfor better instruction following ability and alignment with human values. In\nthis paper, we study the learning dynamics of LLM finetuning on reasoning tasks\nand reveal the uncovered over-memorization phenomenon during a specific stage\nof LLM finetuning. At this stage, the LLMs have excessively memorized training\ndata and exhibit high test perplexity while maintaining good test accuracy. We\ninvestigate the conditions that lead to LLM over-memorization and find that\ntraining epochs and large learning rates contribute to this issue. Although\nmodels with over-memorization demonstrate comparable test accuracy to normal\nmodels, they suffer from reduced robustness, poor out-of-distribution\ngeneralization, and decreased generation diversity. Our experiments unveil the\nover-memorization to be broadly applicable across different tasks, models, and\nfinetuning methods. Our research highlights that overparameterized, extensively\nfinetuned LLMs exhibit unique learning dynamics distinct from traditional\nmachine learning models. Based on our observations of over-memorization, we\nprovide recommendations on checkpoint and learning rate selection during\nfinetuning.",
      "authors": [
        "Zhiwen Ruan",
        "Yun Chen",
        "Yutao Hou",
        "Peng Li",
        "Yang Liu",
        "Guanhua Chen"
      ],
      "published": "2025-08-06T06:34:12+00:00",
      "updated": "2025-08-06T06:34:12+00:00",
      "arxiv_id": "2508.04117v1",
      "url": "http://arxiv.org/pdf/2508.04117v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decoder",
      "abstract": "Reference Expression Segmentation (RES) aims to segment image regions\nspecified by referring expressions and has become popular with the rise of\nmultimodal large models (MLLMs). While MLLMs excel in semantic understanding,\ntheir token-generation paradigm struggles with pixel-level dense prediction.\nExisting RES methods either couple MLLMs with the parameter-heavy Segment\nAnything Model (SAM) with 632M network parameters or adopt SAM-free lightweight\npipelines that sacrifice accuracy. To address the trade-off between performance\nand cost, we specifically propose MLLMSeg, a novel framework that fully\nexploits the inherent visual detail features encoded in the MLLM vision encoder\nwithout introducing an extra visual encoder. Besides, we propose a\ndetail-enhanced and semantic-consistent feature fusion module (DSFF) that fully\nintegrates the detail-related visual feature with the semantic-related feature\noutput by the large language model (LLM) of MLLM. Finally, we establish a\nlight-weight mask decoder with only 34M network parameters that optimally\nleverages detailed spatial features from the visual encoder and semantic\nfeatures from the LLM to achieve precise mask prediction. Extensive experiments\ndemonstrate that our method generally surpasses both SAM-based and SAM-free\ncompetitors, striking a better balance between performance and cost. Code is\navailable at https://github.com/jcwang0602/MLLMSeg.",
      "authors": [
        "Jingchao Wang",
        "Zhijian Wu",
        "Dingjiang Huang",
        "Yefeng Zheng",
        "Hong Wang"
      ],
      "published": "2025-08-06T06:06:52+00:00",
      "updated": "2025-08-07T05:07:54+00:00",
      "arxiv_id": "2508.04107v2",
      "url": "http://arxiv.org/pdf/2508.04107v2",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Efficient Scaling for LLM-based ASR",
      "abstract": "Large language model (LLM)-based automatic speech recognition (ASR) achieves\nstrong performance but often incurs high computational costs. This work\ninvestigates how to obtain the best LLM-ASR performance efficiently. Through\ncomprehensive and controlled experiments, we find that pretraining the speech\nencoder before integrating it with the LLM leads to significantly better\nscaling efficiency than the standard practice of joint post-training of\nLLM-ASR. Based on this insight, we propose a new multi-stage LLM-ASR training\nstrategy, EFIN: Encoder First Integration. Among all training strategies\nevaluated, EFIN consistently delivers better performance (relative to 21.1%\nCERR) with significantly lower computation budgets (49.9% FLOPs). Furthermore,\nwe derive a scaling law that approximates ASR error rates as a computation\nfunction, providing practical guidance for LLM-ASR scaling.",
      "authors": [
        "Bingshen Mu",
        "Yiwen Shao",
        "Kun Wei",
        "Dong Yu",
        "Lei Xie"
      ],
      "published": "2025-08-06T05:28:18+00:00",
      "updated": "2025-08-06T05:28:18+00:00",
      "arxiv_id": "2508.04096v1",
      "url": "http://arxiv.org/pdf/2508.04096v1",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.SD"
    },
    {
      "title": "GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning",
      "abstract": "Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities\nbut often struggle with complex, multi-step mathematical reasoning, where minor\nerrors in visual perception or logical deduction can lead to complete failure.\nWhile Process Reward Models (PRMs) offer step-by-step supervision, existing\nmultimodal PRMs are limited to being binary verifiers that can identify but not\ncorrect errors, offering little explanatory power. To address these\ndeficiencies, we introduce the Generative Multimodal Process Reward Model\n(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an\nactive reasoning collaborator. Instead of a simple scalar score, GM-PRM\nprovides a fine-grained, interpretable analysis of each reasoning step,\nevaluating its step intent, visual alignment, and logical soundness. More\ncritically, GM-PRM is trained to generate a corrected version of the first\nerroneous step it identifies. This unique corrective capability enables our new\ntest-time inference strategy, Refined Best-of-N (Refined-BoN). This framework\nactively enhances solution quality by using the PRM's generated correction to\nguide the policy model toward a more promising reasoning trajectory, thereby\nimproving the diversity and correctness of the solution pool. We demonstrate\nthat GM-PRM achieves state-of-the-art results on multiple multimodal math\nbenchmarks, significantly boosting policy model performance with remarkable\ndata efficiency, requiring only a 20K-sample training dataset. Our code will be\nreleased upon acceptance.",
      "authors": [
        "Jianghangfan Zhang",
        "Yibo Yan",
        "Kening Zheng",
        "Xin Zou",
        "Song Dai",
        "Xuming Hu"
      ],
      "published": "2025-08-06T05:10:29+00:00",
      "updated": "2025-08-07T03:52:48+00:00",
      "arxiv_id": "2508.04088v2",
      "url": "http://arxiv.org/pdf/2508.04088v2",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ToolGrad: Efficient Tool-use Dataset Generation with Textual \"Gradients\"",
      "abstract": "Prior work synthesizes tool-use LLM datasets by first generating a user\nquery, followed by complex tool-use annotations like DFS. This leads to\ninevitable annotation failures and low efficiency in data generation. We\nintroduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad\nfirst constructs valid tool-use chains through an iterative process guided by\ntextual \"gradients\", and then synthesizes corresponding user queries. This\n\"answer-first\" approach led to ToolGrad-5k, a dataset generated with more\ncomplex tool use, lower cost, and 100% pass rate. Experiments show that models\ntrained on ToolGrad-5k outperform those on expensive baseline datasets and\nproprietary LLMs, even on OOD benchmarks.",
      "authors": [
        "Zhongyi Zhou",
        "Kohei Uehara",
        "Haoyu Zhang",
        "Jingtao Zhou",
        "Lin Gu",
        "Ruofei Du",
        "Zheng Xu",
        "Tatsuya Harada"
      ],
      "published": "2025-08-06T05:04:00+00:00",
      "updated": "2025-08-06T05:04:00+00:00",
      "arxiv_id": "2508.04086v1",
      "url": "http://arxiv.org/pdf/2508.04086v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement",
      "abstract": "Recent studies have extended the application of large language models (LLMs)\nto geographic problems, revealing surprising geospatial competence even without\nexplicit spatial supervision. However, LLMs still face challenges in spatial\nconsistency, multi-hop reasoning, and geographic bias. To address these issues,\nwe propose GeoSR, a self-refining agentic reasoning framework that embeds core\ngeographic principles -- most notably Tobler's First Law of Geography -- into\nan iterative prediction loop. In GeoSR, the reasoning process is decomposed\ninto three collaborating agents: (1) a variable-selection agent that selects\nrelevant covariates from the same location; (2) a point-selection agent that\nchooses reference predictions at nearby locations generated by the LLM in\nprevious rounds; and (3) a refine agent that coordinates the iterative\nrefinement process by evaluating prediction quality and triggering further\nrounds when necessary. This agentic loop progressively improves prediction\nquality by leveraging both spatial dependencies and inter-variable\nrelationships. We validate GeoSR on tasks ranging from physical-world property\nestimation to socioeconomic prediction. Experimental results show consistent\nimprovements over standard prompting strategies, demonstrating that\nincorporating geostatistical priors and spatially structured reasoning into\nLLMs leads to more accurate and equitable geospatial predictions. The code of\nGeoSR is available at https://github.com/JinfanTang/GeoSR.",
      "authors": [
        "Jinfan Tang",
        "Kunming Wu",
        "Ruifeng Gongxie",
        "Yuya He",
        "Yuankai Wu"
      ],
      "published": "2025-08-06T04:45:34+00:00",
      "updated": "2025-08-06T04:45:34+00:00",
      "arxiv_id": "2508.04080v1",
      "url": "http://arxiv.org/pdf/2508.04080v1",
      "categories": [
        "cs.AI",
        "stat.OT"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Efficient Strategy for Improving Large Language Model (LLM) Capabilities",
      "abstract": "Large Language Models (LLMs) have become a milestone in the field of\nartificial intelligence and natural language processing. However, their\nlarge-scale deployment remains constrained by the need for significant\ncomputational resources. This work proposes starting from a base model to\nexplore and combine data processing and careful data selection techniques,\ntraining strategies, and architectural adjustments to improve the efficiency of\nLLMs in resource-constrained environments and within a delimited knowledge\nbase. The methodological approach included defining criteria for building\nreliable datasets, conducting controlled experiments with different\nconfigurations, and systematically evaluating the resulting variants in terms\nof capability, versatility, response time, and safety. Finally, comparative\ntests were conducted to measure the performance of the developed variants and\nto validate the effectiveness of the proposed strategies. This work is based on\nthe master's thesis in Systems and Computer Engineering titled \"Efficient\nStrategy for Improving the Capabilities of Large Language Models (LLMs)\".",
      "authors": [
        "JuliÃ¡n Camilo Velandia GutiÃ©rrez"
      ],
      "published": "2025-08-06T04:08:26+00:00",
      "updated": "2025-08-06T04:08:26+00:00",
      "arxiv_id": "2508.04073v1",
      "url": "http://arxiv.org/pdf/2508.04073v1",
      "categories": [
        "cs.CL",
        "cs.LG",
        "I.2.7; I.2.6; I.5.1"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "KG-Augmented Executable CoT for Mathematical Coding",
      "abstract": "In recent years, large language models (LLMs) have excelled in natural\nlanguage processing tasks but face significant challenges in complex reasoning\ntasks such as mathematical reasoning and code generation. To address these\nlimitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a\nnovel framework that enhances code generation through knowledge graphs and\nimproves mathematical reasoning via executable code. KGA-ECoT decomposes\nproblems into a Structured Task Graph, leverages efficient GraphRAG for precise\nknowledge retrieval from mathematical libraries, and generates verifiable code\nto ensure computational accuracy. Evaluations on multiple mathematical\nreasoning benchmarks demonstrate that KGA-ECoT significantly outperforms\nexisting prompting methods, achieving absolute accuracy improvements ranging\nfrom several to over ten percentage points. Further analysis confirms the\ncritical roles of GraphRAG in enhancing code quality and external code\nexecution in ensuring precision. These findings collectively establish KGA-ECoT\nas a robust and highly generalizable framework for complex mathematical\nreasoning tasks.",
      "authors": [
        "Xingyu Chen",
        "Junxiu An",
        "Jun Guo",
        "Li Wang",
        "Jingcai Guo"
      ],
      "published": "2025-08-06T04:07:35+00:00",
      "updated": "2025-08-06T04:07:35+00:00",
      "arxiv_id": "2508.04072v1",
      "url": "http://arxiv.org/pdf/2508.04072v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading",
      "abstract": "Research to improve Automated Short Answer Grading has recently focused on\nLarge Language Models (LLMs) with prompt engineering and no- or few-shot\nprompting to achieve best results. This is in contrast to the fine-tuning\napproach, which has historically required large-scale compute clusters\ninaccessible to most users. New closed-model approaches such as OpenAI's\nfine-tuning service promise results with as few as 100 examples, while methods\nusing open weights such as quantized low-rank adaptive (QLORA) can be used to\nfine-tune models on consumer GPUs. We evaluate both of these fine-tuning\nmethods, measuring their interaction with few-shot prompting for automated\nshort answer grading (ASAG) with structured (JSON) outputs. Our results show\nthat finetuning with small amounts of data has limited utility for Llama\nopen-weight models, but that fine-tuning methods can outperform few-shot\nbaseline instruction-tuned LLMs for OpenAI's closed models. While our\nevaluation set is limited, we find some evidence that the observed benefits of\nfinetuning may be impacted by the domain subject matter. Lastly, we observed\ndramatic improvement with the LLama 3.1 8B-Instruct open-weight model by\nseeding the initial training examples with a significant amount of cheaply\ngenerated synthetic training data.",
      "authors": [
        "Joel Walsh",
        "Siddarth Mamidanna",
        "Benjamin Nye",
        "Mark Core",
        "Daniel Auerbach"
      ],
      "published": "2025-08-06T03:52:55+00:00",
      "updated": "2025-08-06T03:52:55+00:00",
      "arxiv_id": "2508.04063v1",
      "url": "http://arxiv.org/pdf/2508.04063v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models",
      "abstract": "Occlusion perception, a critical foundation for human-level spatial\nunderstanding, embodies the challenge of integrating visual recognition and\nreasoning. Though multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities, their performance on occlusion perception remains\nunder-explored. To address this gap, we introduce O-Bench, the first visual\nquestion answering (VQA) benchmark specifically designed for occlusion\nperception. Based on SA-1B, we construct 1,365 images featuring semantically\ncoherent occlusion scenarios through a novel layered synthesis approach. Upon\nthis foundation, we annotate 4,588 question-answer pairs in total across five\ntailored tasks, employing a reliable, semi-automatic workflow. Our extensive\nevaluation of 22 representative MLLMs against the human baseline reveals a\nsignificant performance gap between current MLLMs and humans, which, we find,\ncannot be sufficiently bridged by model scaling or thinking process. We further\nidentify three typical failure patterns, including an overly conservative bias,\na fragile gestalt prediction, and a struggle with quantitative tasks. We\nbelieve O-Bench can not only provide a vital evaluation tool for occlusion\nperception, but also inspire the development of MLLMs for better visual\nintelligence. Our benchmark will be made publicly available upon paper\npublication.",
      "authors": [
        "Zhaochen Liu",
        "Kaiwen Gao",
        "Shuyi Liang",
        "Bin Xiao",
        "Limeng Qiao",
        "Lin Ma",
        "Tingting Jiang"
      ],
      "published": "2025-08-06T03:39:21+00:00",
      "updated": "2025-08-06T03:39:21+00:00",
      "arxiv_id": "2508.04059v1",
      "url": "http://arxiv.org/pdf/2508.04059v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG",
      "abstract": "Retrieval-Augmented Generation (RAG) has become a cornerstone technique for\nenhancing large language models (LLMs) with external knowledge. However,\ncurrent RAG systems face two critical limitations: (1) they inefficiently\nretrieve information for every query, including simple questions that could be\nresolved using the LLM's parametric knowledge alone, and (2) they risk\nretrieving irrelevant documents when queries contain sparse information\nsignals. To address these gaps, we introduce Parametric-verified Adaptive\nInformation Retrieval and Selection (PAIRS), a training-free framework that\nintegrates parametric and retrieved knowledge to adaptively determine whether\nto retrieve and how to select external information. Specifically, PAIRS employs\na dual-path generation mechanism: First, the LLM produces both a direct answer\nand a context-augmented answer using self-generated pseudo-context. When these\noutputs converge, PAIRS bypasses external retrieval entirely, dramatically\nimproving the RAG system's efficiency. For divergent cases, PAIRS activates a\ndual-path retrieval (DPR) process guided by both the original query and\nself-generated contextual signals, followed by an Adaptive Information\nSelection (AIS) module that filters documents through weighted similarity to\nboth sources. This simple yet effective approach can not only enhance\nefficiency by eliminating unnecessary retrievals but also improve accuracy\nthrough contextually guided retrieval and adaptive information selection.\nExperimental results on six question-answering (QA) benchmarks show that PAIRS\nreduces retrieval costs by around 25% (triggering for only 75% of queries)\nwhile still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior\nbaselines on average.",
      "authors": [
        "Wang Chen",
        "Guanqiang Qi",
        "Weikang Li",
        "Yang Li",
        "Deguo Xia",
        "Jizhou Huang"
      ],
      "published": "2025-08-06T03:33:01+00:00",
      "updated": "2025-08-06T03:33:01+00:00",
      "arxiv_id": "2508.04057v1",
      "url": "http://arxiv.org/pdf/2508.04057v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents",
      "abstract": "Motion sensor time-series are central to human activity recognition (HAR),\nwith applications in health, sports, and smart devices. However, existing\nmethods are trained for fixed activity sets and require costly retraining when\nnew behaviours or sensor setups appear. Recent attempts to use large language\nmodels (LLMs) for HAR, typically by converting signals into text or images,\nsuffer from limited accuracy and lack verifiable interpretability. We propose\nZARA, the first agent-based framework for zero-shot, explainable HAR directly\nfrom raw motion time-series. ZARA integrates an automatically derived pair-wise\nfeature knowledge base that captures discriminative statistics for every\nactivity pair, a multi-sensor retrieval module that surfaces relevant evidence,\nand a hierarchical agent pipeline that guides the LLM to iteratively select\nfeatures, draw on this evidence, and produce both activity predictions and\nnatural-language explanations. ZARA enables flexible and interpretable HAR\nwithout any fine-tuning or task-specific classifiers. Extensive experiments on\n8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering\nclear reasoning while exceeding the strongest baselines by 2.53x in macro F1.\nAblation studies further confirm the necessity of each module, marking ZARA as\na promising step toward trustworthy, plug-and-play motion time-series analysis.\nOur codes are available at https://github.com/zechenli03/ZARA.",
      "authors": [
        "Zechen Li",
        "Baiyu Chen",
        "Hao Xue",
        "Flora D. Salim"
      ],
      "published": "2025-08-06T02:57:57+00:00",
      "updated": "2025-08-06T02:57:57+00:00",
      "arxiv_id": "2508.04038v1",
      "url": "http://arxiv.org/pdf/2508.04038v1",
      "categories": [
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Enhancing Serendipity Recommendation System by Constructing Dynamic User Knowledge Graphs with Large Language Models",
      "abstract": "The feedback loop in industrial recommendation systems reinforces homogeneous\ncontent, creates filter bubble effects, and diminishes user satisfaction.\nRecently, large language models(LLMs) have demonstrated potential in\nserendipity recommendation, thanks to their extensive world knowledge and\nsuperior reasoning capabilities. However, these models still face challenges in\nensuring the rationality of the reasoning process, the usefulness of the\nreasoning results, and meeting the latency requirements of industrial\nrecommendation systems (RSs). To address these challenges, we propose a method\nthat leverages llm to dynamically construct user knowledge graphs, thereby\nenhancing the serendipity of recommendation systems. This method comprises a\ntwo stage framework:(1) two-hop interest reasoning, where user static profiles\nand historical behaviors are utilized to dynamically construct user knowledge\ngraphs via llm. Two-hop reasoning, which can enhance the quality and accuracy\nof LLM reasoning results, is then performed on the constructed graphs to\nidentify users' potential interests; and(2) Near-line adaptation, a\ncost-effective approach to deploying the aforementioned models in industrial\nrecommendation systems. We propose a u2i (user-to-item) retrieval model that\nalso incorporates i2i (item-to-item) retrieval capabilities, the retrieved\nitems not only exhibit strong relevance to users' newly emerged interests but\nalso retain the high conversion rate of traditional u2i retrieval. Our online\nexperiments on the Dewu app, which has tens of millions of users, indicate that\nthe method increased the exposure novelty rate by 4.62%, the click novelty rate\nby 4.85%, the average view duration per person by 0.15%, unique visitor click\nthrough rate by 0.07%, and unique visitor interaction penetration by 0.30%,\nenhancing user experience.",
      "authors": [
        "Qian Yong",
        "Yanhui Li",
        "Jialiang Shi",
        "Yaguang Dou",
        "Tian Qi"
      ],
      "published": "2025-08-06T02:52:09+00:00",
      "updated": "2025-08-06T02:52:09+00:00",
      "arxiv_id": "2508.04032v1",
      "url": "http://arxiv.org/pdf/2508.04032v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "BridgeScope: A Universal Toolkit for Bridging Large Language Models and Databases",
      "abstract": "As large language models (LLMs) demonstrate increasingly powerful reasoning\nand orchestration capabilities, LLM-based agents are rapidly proliferating for\ncomplex data-related tasks. Despite this progress, the current design of how\nLLMs interact with databases exhibits critical limitations in usability,\nsecurity, privilege management, and data transmission efficiency. To resolve\nthese challenges, we introduce BridgeScope, a universal toolkit bridging LLMs\nand databases through three key innovations. First, it modularizes SQL\noperations into fine-grained tools for context retrieval, CRUD execution, and\nACID-compliant transaction management, enabling more precise and LLM-friendly\nfunctionality controls. Second, it aligns tool implementations with both\ndatabase privileges and user security policies to steer LLMs away from unsafe\nor unauthorized operations, improving task execution efficiency while\nsafeguarding database security. Third, it introduces a proxy mechanism for\nseamless inter-tool data transfer, bypassing LLM transmission bottlenecks. All\nof these designs are database-agnostic and can be transparently integrated with\nexisting agent architectures. We also release an open-source implementation of\nBridgeScope for PostgreSQL. Evaluations on two novel benchmarks demonstrate\nthat BridgeScope enables LLM agents to operate databases more effectively,\nreduces token usage by up to 80% through improved security awareness, and\nuniquely supports data-intensive workflows beyond existing toolkits,\nestablishing BridgeScope as a robust foundation for next-generation intelligent\ndata automation.",
      "authors": [
        "Lianggui Weng",
        "Dandan Liu",
        "Rong Zhu",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "published": "2025-08-06T02:51:16+00:00",
      "updated": "2025-08-06T02:51:16+00:00",
      "arxiv_id": "2508.04031v1",
      "url": "http://arxiv.org/pdf/2508.04031v1",
      "categories": [
        "cs.DB"
      ],
      "primary_category": "cs.DB"
    },
    {
      "title": "Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability",
      "abstract": "Large Multimodal Models (LMMs) have witnessed remarkable growth, showcasing\nformidable capabilities in handling intricate multimodal tasks with exceptional\nperformance. Recent research has underscored the inclination of large language\nmodels to passively accept defective inputs, often resulting in futile\nreasoning on invalid prompts. However, the same critical question of whether\nLMMs can actively detect and scrutinize erroneous inputs still remains\nunexplored. To address this gap, we introduce the Input Scrutiny Ability\nEvaluation Framework (ISEval), which encompasses seven categories of flawed\npremises and three evaluation metrics. Our extensive evaluation of ten advanced\nLMMs has identified key findings. Most models struggle to actively detect\nflawed textual premises without guidance, which reflects a strong reliance on\nexplicit prompts for premise error identification. Error type affects\nperformance: models excel at identifying logical fallacies but struggle with\nsurface-level linguistic errors and certain conditional flaws. Modality trust\nvaries-Gemini 2.5 pro and Claude Sonnet 4 balance visual and textual info,\nwhile aya-vision-8b over-rely on text in conflicts. These insights underscore\nthe urgent need to enhance LMMs' proactive verification of input validity and\nshed novel insights into mitigating the problem. The code is available at\nhttps://github.com/MLGroupJLU/LMM_ISEval.",
      "authors": [
        "Haiqi Yang",
        "Jinzhe Li",
        "Gengxu Li",
        "Yi Chang",
        "Yuan Wu"
      ],
      "published": "2025-08-06T02:13:46+00:00",
      "updated": "2025-08-06T02:13:46+00:00",
      "arxiv_id": "2508.04017v1",
      "url": "http://arxiv.org/pdf/2508.04017v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing",
      "abstract": "Large Language Models (LLMs) underpin many AI applications, but their static\nnature makes updating knowledge costly. Model editing offers an efficient\nalternative by injecting new information through targeted parameter\nmodifications. In particular, meta-learning-based model editing (MLBME) methods\nhave demonstrated notable advantages in both editing effectiveness and\nefficiency. Despite this, we find that MLBME exhibits suboptimal performance in\nlow-data scenarios, and its training efficiency is bottlenecked by the\ncomputation of KL divergence. To address these, we propose $\\textbf{S}$tep\n$\\textbf{M}$ore $\\textbf{Edit}$ ($\\textbf{SMEdit}$), a novel MLBME method that\nadopts $\\textbf{M}$ultiple $\\textbf{B}$ackpro$\\textbf{P}$agation\n$\\textbf{S}$teps ($\\textbf{MBPS}$) to improve editing performance under limited\nsupervision and a norm regularization on weight updates to improve training\nefficiency. Experimental results on two datasets and two LLMs demonstrate that\nSMEdit outperforms prior MLBME baselines and the MBPS strategy can be\nseamlessly integrated into existing methods to further boost their performance.\nOur code will be released soon.",
      "authors": [
        "Xiaopeng Li",
        "Shasha Li",
        "Xi Wang",
        "Shezheng Song",
        "Bin Ji",
        "Shangwen Wang",
        "Jun Ma",
        "Xiaodong Liu",
        "Mina Liu",
        "Jie Yu"
      ],
      "published": "2025-08-06T01:54:58+00:00",
      "updated": "2025-08-06T01:54:58+00:00",
      "arxiv_id": "2508.04012v1",
      "url": "http://arxiv.org/pdf/2508.04012v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "StepWrite: Adaptive Planning for Speech-Driven Text Generation",
      "abstract": "People frequently use speech-to-text systems to compose short texts with\nvoice. However, current voice-based interfaces struggle to support composing\nmore detailed, contextually complex texts, especially in scenarios where users\nare on the move and cannot visually track progress. Longer-form communication,\nsuch as composing structured emails or thoughtful responses, requires\npersistent context tracking, structured guidance, and adaptability to evolving\nuser intentions--capabilities that conventional dictation tools and voice\nassistants do not support. We introduce StepWrite, a large language\nmodel-driven voice-based interaction system that augments human writing ability\nby enabling structured, hands-free and eyes-free composition of longer-form\ntexts while on the move. StepWrite decomposes the writing process into\nmanageable subtasks and sequentially guides users with contextually-aware\nnon-visual audio prompts. StepWrite reduces cognitive load by offloading the\ncontext-tracking and adaptive planning tasks to the models. Unlike baseline\nmethods like standard dictation features (e.g., Microsoft Word) and\nconversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite\ndynamically adapts its prompts based on the evolving context and user intent,\nand provides coherent guidance without compromising user autonomy. An empirical\nevaluation with 25 participants engaging in mobile or stationary hands-occupied\nactivities demonstrated that StepWrite significantly reduces cognitive load,\nimproves usability and user satisfaction compared to baseline methods.\nTechnical evaluations further confirmed StepWrite's capability in dynamic\ncontextual prompt generation, accurate tone alignment, and effective fact\nchecking. This work highlights the potential of structured, context-aware voice\ninteractions in enhancing hands-free and eye-free communication in everyday\nmultitasking scenarios.",
      "authors": [
        "Hamza El Alaoui",
        "Atieh Taheri",
        "Yi-Hao Peng",
        "Jeffrey P. Bigham"
      ],
      "published": "2025-08-06T01:50:17+00:00",
      "updated": "2025-08-06T01:50:17+00:00",
      "arxiv_id": "2508.04011v1",
      "url": "http://arxiv.org/pdf/2508.04011v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization",
      "abstract": "Large language models enable agents to autonomously perform tasks in open web\nenvironments. However, as hidden threats within the web evolve, web agents face\nthe challenge of balancing task performance with emerging risks during\nlong-sequence operations. Although this challenge is critical, current research\nremains limited to single-objective optimization or single-turn scenarios,\nlacking the capability for collaborative optimization of both safety and\nutility in web environments. To address this gap, we propose HarmonyGuard, a\nmulti-agent collaborative framework that leverages policy enhancement and\nobjective optimization to jointly improve both utility and safety. HarmonyGuard\nfeatures a multi-agent architecture characterized by two fundamental\ncapabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent\nwithin HarmonyGuard, which automatically extracts and maintains structured\nsecurity policies from unstructured external documents, while continuously\nupdating policies in response to evolving threats. (2) Dual-Objective\nOptimization: Based on the dual objectives of safety and utility, the Utility\nAgent integrated within HarmonyGuard performs the Markovian real-time reasoning\nto evaluate the objectives and utilizes metacognitive capabilities for their\noptimization. Extensive evaluations on multiple benchmarks show that\nHarmonyGuard improves policy compliance by up to 38% and task completion by up\nto 20% over existing baselines, while achieving over 90% policy compliance\nacross all tasks. Our project is available here:\nhttps://github.com/YurunChen/HarmonyGuard.",
      "authors": [
        "Yurun Chen",
        "Xavier Hu",
        "Yuhan Liu",
        "Keting Yin",
        "Juncheng Li",
        "Zhuosheng Zhang",
        "Shengyu Zhang"
      ],
      "published": "2025-08-06T01:49:32+00:00",
      "updated": "2025-08-06T01:49:32+00:00",
      "arxiv_id": "2508.04010v1",
      "url": "http://arxiv.org/pdf/2508.04010v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval",
      "abstract": "Conversational search aims to satisfy users' complex information needs via\nmultiple-turn interactions. The key challenge lies in revealing real users'\nsearch intent from the context-dependent queries. Previous studies achieve\nconversational search by fine-tuning a conversational dense retriever with\nrelevance judgments between pairs of context-dependent queries and documents.\nHowever, this training paradigm encounters data scarcity issues. To this end,\nwe propose ConvMix, a mixed-criteria framework to augment conversational dense\nretrieval, which covers more aspects than existing data augmentation\nframeworks. We design a two-sided relevance judgment augmentation schema in a\nscalable manner via the aid of large language models. Besides, we integrate the\nframework with quality control mechanisms to obtain semantically diverse\nsamples and near-distribution supervisions to combine various annotated data.\nExperimental results on five widely used benchmarks show that the\nconversational dense retriever trained by our ConvMix framework outperforms\nprevious baseline methods, which demonstrates our superior effectiveness.",
      "authors": [
        "Fengran Mo",
        "Jinghan Zhang",
        "Yuchen Hui",
        "Jia Ao Sun",
        "Zhichao Xu",
        "Zhan Su",
        "Jian-Yun Nie"
      ],
      "published": "2025-08-06T01:28:49+00:00",
      "updated": "2025-08-06T01:28:49+00:00",
      "arxiv_id": "2508.04001v1",
      "url": "http://arxiv.org/pdf/2508.04001v1",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Tensorized Clustered LoRA Merging for Multi-Task Interference",
      "abstract": "Despite the success of the monolithic dense paradigm of large language models\n(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small\ntask-specific modules and merging them with the base model. However, in\nmulti-task settings, merging LoRA adapters trained on heterogeneous sources\nfrequently causes \\textit{task interference}, degrading downstream performance.\nTo address this, we propose a tensorized clustered LoRA (TC-LoRA) library\ntargeting to address the task interference at the \\textit{text-level} and\n\\textit{parameter-level}. At the \\textit{text-level}, we cluster the training\nsamples in the embedding space to capture input-format similarities, then train\na specialized LoRA adapter for each cluster. At the \\textit{parameter-level},\nwe introduce a joint Canonical Polyadic (CP) decomposition that disentangles\ntask-specific and shared factors across LoRA adapters. This joint factorization\npreserves essential knowledge while reducing cross-task interference. Extensive\nexperiments on out-of-domain zero-shot and skill-composition tasks-including\nreasoning, question answering, and coding. Compared to strong SVD-based\nbaselines, TC-LoRA achieves +1.4\\% accuracy on Phi-3 and +2.3\\% on Mistral-7B\n(+2.3\\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.",
      "authors": [
        "Zhan Su",
        "Fengran Mo",
        "Guojun Liang",
        "Jinghan Zhang",
        "Bingbing Wen",
        "Prayag Tiwari",
        "Jian-Yun Nie"
      ],
      "published": "2025-08-06T01:26:43+00:00",
      "updated": "2025-08-06T01:26:43+00:00",
      "arxiv_id": "2508.03999v1",
      "url": "http://arxiv.org/pdf/2508.03999v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents",
      "abstract": "Intelligent personal assistants (IPAs) such as Siri and Google Assistant are\ndesigned to enhance human capabilities and perform tasks on behalf of users.\nThe emergence of LLM agents brings new opportunities for the development of\nIPAs. While responsive capabilities have been widely studied, proactive\nbehaviors remain underexplored. Designing an IPA that is proactive,\nprivacy-preserving, and capable of self-evolution remains a significant\nchallenge. Designing such IPAs relies on the cognitive architecture of LLM\nagents. This work proposes Cognition Forest, a semantic structure designed to\nalign cognitive modeling with system-level design. We unify cognitive\narchitecture and system design into a self-reinforcing loop instead of treating\nthem separately. Based on this principle, we present Galaxy, a framework that\nsupports multidimensional interactions and personalized capability generation.\nTwo cooperative agents are implemented based on Galaxy: KoRa, a\ncognition-enhanced generative agent that supports both responsive and proactive\nskills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's\nself-evolution and privacy preservation. Experimental results show that Galaxy\noutperforms multiple state-of-the-art benchmarks. Ablation studies and\nreal-world interaction cases validate the effectiveness of Galaxy.",
      "authors": [
        "Chongyu Bao",
        "Ruimin Dai",
        "Yangbo Shen",
        "Runyang Jian",
        "Jinghan Zhang",
        "Xiaolan Liu",
        "Kunpeng Liu"
      ],
      "published": "2025-08-06T00:46:38+00:00",
      "updated": "2025-08-06T00:46:38+00:00",
      "arxiv_id": "2508.03991v1",
      "url": "http://arxiv.org/pdf/2508.03991v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?",
      "abstract": "Well-being encompasses mental, physical, and social dimensions essential to\npersonal growth and informed life decisions. As individuals increasingly\nconsult Large Language Models (LLMs) to understand well-being, a key challenge\nemerges: Can LLMs generate explanations that are not only accurate but also\ntailored to diverse audiences? High-quality explanations require both factual\ncorrectness and the ability to meet the expectations of users with varying\nexpertise. In this work, we construct a large-scale dataset comprising 43,880\nexplanations of 2,194 well-being concepts, generated by ten diverse LLMs. We\nintroduce a principle-guided LLM-as-a-judge evaluation framework, employing\ndual judges to assess explanation quality. Furthermore, we show that\nfine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct\nPreference Optimization (DPO) can significantly enhance the quality of\ngenerated explanations. Our results reveal: (1) The proposed LLM judges align\nwell with human evaluations; (2) explanation quality varies significantly\nacross models, audiences, and categories; and (3) DPO- and SFT-finetuned models\noutperform their larger counterparts, demonstrating the effectiveness of\npreference-based learning for specialized explanation tasks.",
      "authors": [
        "Bohan Jiang",
        "Dawei Li",
        "Zhen Tan",
        "Chengshuai Zhao",
        "Huan Liu"
      ],
      "published": "2025-08-06T00:45:02+00:00",
      "updated": "2025-08-06T00:45:02+00:00",
      "arxiv_id": "2508.03990v1",
      "url": "http://arxiv.org/pdf/2508.03990v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency",
      "abstract": "Despite its simplicity and efficacy, the high token expenditure of\nself-consistency can limit its practical utility. Here we investigate if\nself-consistency can be made more token-efficient for long chain-of-thought\nreasoning tasks, while preserving its parallelism, through early hypothesis\npruning. Concretely, we generate all solutions in parallel, but periodically\nprune intermediate hypotheses that are deemed unnecessary based on two\nlightweight indicators: (a) the model's own confidence in individual\nhypotheses, and (b) lexical coverage of all current hypotheses by candidate\nsubsets that are under consideration for continued retention. We design a fast\nweighted set cover algorithm that utilizes the two indicators; our evaluation\nof five LLMs on three math benchmarks shows that this method can improve token\nefficiency for all models, by 10-35% in many cases.",
      "authors": [
        "Md Arafat Sultan",
        "RamÃ³n Fernandez Astudillo"
      ],
      "published": "2025-08-06T00:14:18+00:00",
      "updated": "2025-08-06T00:14:18+00:00",
      "arxiv_id": "2508.03979v1",
      "url": "http://arxiv.org/pdf/2508.03979v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Data and AI governance: Promoting equity, ethics, and fairness in large language models",
      "abstract": "In this paper, we cover approaches to systematically govern, assess and\nquantify bias across the complete life cycle of machine learning models, from\ninitial development and validation to ongoing production monitoring and\nguardrail implementation. Building upon our foundational work on the Bias\nEvaluation and Assessment Test Suite (BEATS) for Large Language Models, the\nauthors share prevalent bias and fairness related gaps in Large Language Models\n(LLMs) and discuss data and AI governance framework to address Bias, Ethics,\nFairness, and Factuality within LLMs. The data and AI governance approach\ndiscussed in this paper is suitable for practical, real-world applications,\nenabling rigorous benchmarking of LLMs prior to production deployment,\nfacilitating continuous real-time evaluation, and proactively governing LLM\ngenerated responses. By implementing the data and AI governance across the life\ncycle of AI development, organizations can significantly enhance the safety and\nresponsibility of their GenAI systems, effectively mitigating risks of\ndiscrimination and protecting against potential reputational or brand-related\nharm. Ultimately, through this article, we aim to contribute to advancement of\nthe creation and deployment of socially responsible and ethically aligned\ngenerative artificial intelligence powered applications.",
      "authors": [
        "Alok Abhishek",
        "Lisa Erickson",
        "Tushar Bandopadhyay"
      ],
      "published": "2025-08-05T23:15:31+00:00",
      "updated": "2025-08-05T23:15:31+00:00",
      "arxiv_id": "2508.03970v1",
      "url": "http://arxiv.org/pdf/2508.03970v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T01 (Primary), 68T50 (Secondary)",
        "I.2.0; I.2.7"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "GP and LLMs for Program Synthesis: No Clear Winners",
      "abstract": "Genetic programming (GP) and large language models (LLMs) differ in how\nprogram specifications are provided: GP uses input-output examples, and LLMs\nuse text descriptions. In this work, we compared the ability of PushGP and\nGPT-4o to synthesize computer programs for tasks from the PSB2 benchmark suite.\nWe used three prompt variants with GPT-4o: input-output examples (data-only),\ntextual description of the task (text-only), and a combination of both textual\ndescriptions and input-output examples (data-text). Additionally, we varied the\nnumber of input-output examples available for building programs. For each\nsynthesizer and task combination, we compared success rates across all program\nsynthesizers, as well as the similarity between successful GPT-4o synthesized\nprograms. We found that the combination of PushGP and GPT-4o with data-text\nprompting led to the greatest number of tasks solved (23 of the 25 tasks), even\nthough several tasks were solved exclusively by only one of the two\nsynthesizers. We also observed that PushGP and GPT-4o with data-only prompting\nsolved fewer tasks with the decrease in the training set size, while the\nremaining synthesizers saw no decrease. We also detected significant\ndifferences in similarity between the successful programs synthesized for\nGPT-4o with text-only and data-only prompting. With there being no dominant\nprogram synthesizer, this work highlights the importance of different\noptimization techniques used by PushGP and LLMs to synthesize programs.",
      "authors": [
        "Jose Guadalupe Hernandez",
        "Anil Kumar Saini",
        "Gabriel Ketron",
        "Jason H. Moore"
      ],
      "published": "2025-08-05T23:09:45+00:00",
      "updated": "2025-08-05T23:09:45+00:00",
      "arxiv_id": "2508.03966v1",
      "url": "http://arxiv.org/pdf/2508.03966v1",
      "categories": [
        "cs.NE"
      ],
      "primary_category": "cs.NE"
    },
    {
      "title": "Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?",
      "abstract": "Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery.",
      "authors": [
        "Zewen Liu",
        "Juntong Ni",
        "Xianfeng Tang",
        "Max S. Y. Lau",
        "Wei Jin"
      ],
      "published": "2025-08-05T22:58:54+00:00",
      "updated": "2025-08-05T22:58:54+00:00",
      "arxiv_id": "2508.03963v1",
      "url": "http://arxiv.org/pdf/2508.03963v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation",
      "abstract": "In the era of information overload, personalized news headline generation is\ncrucial for engaging users by tailoring content to their preferences while\naccurately conveying news facts. Existing methods struggle with effectively\ncapturing complex user interests and ensuring factual consistency, often\nleading to generic or misleading headlines. Leveraging the unprecedented\ncapabilities of Large Language Models (LLMs) in text generation, we propose\nContext-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates\nuser preferences and factual consistency constraints into a powerful\npre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture\nlong-term user interests, a Context Injection Adapter to seamlessly integrate\nthese preferences and current article context into the LLM's generation\nprocess, and a Fact-Consistency Reinforcement Module employing a novel\ncontrastive loss to mitigate hallucination. Evaluated on the real-world PENS\ndataset, CAP-LLM achieves state-of-the-art performance across all metrics.\nNotably, it significantly improves factual consistency (FactCC of 87.50) over\nstrong baselines like BART (86.67), while simultaneously enhancing\npersonalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1\n26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,\nand sensitivity analyses further validate the effectiveness of each component\nand the robustness of our approach, demonstrating CAP-LLM's ability to achieve\na superior balance between personalization and factual accuracy in news\nheadline generation.",
      "authors": [
        "Raymond Wilson",
        "Cole Graham",
        "Chase Carter",
        "Zefeng Yang",
        "Ruiqi Gu"
      ],
      "published": "2025-08-05T21:55:44+00:00",
      "updated": "2025-08-05T21:55:44+00:00",
      "arxiv_id": "2508.03935v1",
      "url": "http://arxiv.org/pdf/2508.03935v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems",
      "abstract": "Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are\ntransforming software engineering by automating key tasks, including code\ngeneration, testing, and debugging. As these models become integral to\ndevelopment workflows, a systematic comparison of their performance is\nessential for optimizing their use in real world applications. This study\nbenchmarks these four prominent LLMs on one hundred and fifty LeetCode problems\nacross easy, medium, and hard difficulties, generating solutions in Java and\nPython. We evaluate each model based on execution time, memory usage, and\nalgorithmic complexity, revealing significant performance differences. ChatGPT\ndemonstrates consistent efficiency in execution time and memory usage, while\nCopilot and DeepSeek show variability as task complexity increases. Gemini,\nalthough effective on simpler tasks, requires more attempts as problem\ndifficulty rises. Our findings provide actionable insights into each model's\nstrengths and limitations, offering guidance for developers selecting LLMs for\nspecific coding tasks and providing insights on the performance and complexity\nof GPT-like generated solutions.",
      "authors": [
        "Everton Guimaraes",
        "Nathalia Nascimento",
        "Chandan Shivalingaiah",
        "Asish Nelapati"
      ],
      "published": "2025-08-05T21:50:52+00:00",
      "updated": "2025-08-05T21:50:52+00:00",
      "arxiv_id": "2508.03931v1",
      "url": "http://arxiv.org/pdf/2508.03931v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework",
      "abstract": "Designing effective algorithmic components remains a fundamental obstacle in\ntackling NP-hard combinatorial optimization problems (COPs), where solvers\noften rely on carefully hand-crafted strategies. Despite recent advances in\nusing large language models (LLMs) to synthesize high-quality components, most\napproaches restrict the search to a single element - commonly a heuristic\nscoring function - thus missing broader opportunities for innovation. In this\npaper, we introduce a broader formulation of solver design as a multi-strategy\noptimization problem, which seeks to jointly improve a set of interdependent\ncomponents under a unified objective. To address this, we propose\nMulti-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a\nnovel framework based on Monte Carlo Tree Search that facilitates turn-based\noptimization between two LLM agents. At each turn, an agent improves one\ncomponent by leveraging the history of both its own and its opponent's prior\nupdates, promoting both competitive pressure and emergent cooperation. This\nstructured interaction broadens the search landscape and encourages the\ndiscovery of diverse, high-performing solutions. Experiments across multiple\nCOP domains show that MOTIF consistently outperforms state-of-the-art methods,\nhighlighting the promise of turn-based, multi-agent prompting for fully\nautomated solver design.",
      "authors": [
        "Nguyen Viet Tuan Kiet",
        "Dao Van Tung",
        "Tran Cong Dao",
        "Huynh Thi Thanh Binh"
      ],
      "published": "2025-08-05T21:45:36+00:00",
      "updated": "2025-08-05T21:45:36+00:00",
      "arxiv_id": "2508.03929v1",
      "url": "http://arxiv.org/pdf/2508.03929v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Sotopia-RL: Reward Design for Social Intelligence",
      "abstract": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.",
      "authors": [
        "Haofei Yu",
        "Zhengyang Qi",
        "Yining Zhao",
        "Kolby Nottingham",
        "Keyang Xuan",
        "Bodhisattwa Prasad Majumder",
        "Hao Zhu",
        "Paul Pu Liang",
        "Jiaxuan You"
      ],
      "published": "2025-08-05T20:43:42+00:00",
      "updated": "2025-08-05T20:43:42+00:00",
      "arxiv_id": "2508.03905v1",
      "url": "http://arxiv.org/pdf/2508.03905v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "An Entity Linking Agent for Question Answering",
      "abstract": "Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide\naccurate answers. Entity Linking (EL) plays a critical role in linking natural\nlanguage mentions to KB entries. However, most existing EL methods are designed\nfor long contexts and do not perform well on short, ambiguous user questions in\nQA tasks. We propose an entity linking agent for QA, based on a Large Language\nModel that simulates human cognitive workflows. The agent actively identifies\nentity mentions, retrieves candidate entities, and makes decision. To verify\nthe effectiveness of our agent, we conduct two experiments: tool-based entity\nlinking and QA task evaluation. The results confirm the robustness and\neffectiveness of our agent.",
      "authors": [
        "Yajie Luo",
        "Yihong Wu",
        "Muzhi Li",
        "Fengran Mo",
        "Jia Ao Sun",
        "Xinyu Wang",
        "Liheng Ma",
        "Yingxue Zhang",
        "Jian-Yun Nie"
      ],
      "published": "2025-08-05T19:28:43+00:00",
      "updated": "2025-08-07T17:36:04+00:00",
      "arxiv_id": "2508.03865v2",
      "url": "http://arxiv.org/pdf/2508.03865v2",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety",
      "abstract": "Multi-agent systems (MAS) built on multimodal large language models exhibit\nstrong collaboration and performance. However, their growing openness and\ninteraction complexity pose serious risks, notably jailbreak and adversarial\nattacks. Existing defenses typically rely on external guard modules, such as\ndedicated safety agents, to handle unsafe behaviors. Unfortunately, this\nparadigm faces two challenges: (1) standalone agents offer limited protection,\nand (2) their independence leads to single-point failure-if compromised,\nsystem-wide safety collapses. Naively increasing the number of guard agents\nfurther raises cost and complexity. To address these challenges, we propose\nEvo-MARL, a novel multi-agent reinforcement learning (MARL) framework that\nenables all task agents to jointly acquire defensive capabilities. Rather than\nrelying on external safety modules, Evo-MARL trains each agent to\nsimultaneously perform its primary function and resist adversarial threats,\nensuring robustness without increasing system overhead or single-node failure.\nFurthermore, Evo-MARL integrates evolutionary search with parameter-sharing\nreinforcement learning to co-evolve attackers and defenders. This adversarial\ntraining paradigm internalizes safety mechanisms and continually enhances MAS\nperformance under co-evolving threats. Experiments show that Evo-MARL reduces\nattack success rates by up to 22% while boosting accuracy by up to 5% on\nreasoning tasks-demonstrating that safety and utility can be jointly improved.",
      "authors": [
        "Zhenyu Pan",
        "Yiting Zhang",
        "Yutong Zhang",
        "Jianshu Zhang",
        "Haozheng Luo",
        "Yuwei Han",
        "Dennis Wu",
        "Hong-Yu Chen",
        "Philip S. Yu",
        "Manling Li",
        "Han Liu"
      ],
      "published": "2025-08-05T19:26:55+00:00",
      "updated": "2025-08-05T19:26:55+00:00",
      "arxiv_id": "2508.03864v1",
      "url": "http://arxiv.org/pdf/2508.03864v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models",
      "abstract": "Large Language Models (LLMs) are trained on vast and diverse internet corpora\nthat often include inaccurate or misleading content. Consequently, LLMs can\ngenerate misinformation, making robust fact-checking essential. This review\nsystematically analyzes how LLM-generated content is evaluated for factual\naccuracy by exploring key challenges such as hallucinations, dataset\nlimitations, and the reliability of evaluation metrics. The review emphasizes\nthe need for strong fact-checking frameworks that integrate advanced prompting\nstrategies, domain-specific fine-tuning, and retrieval-augmented generation\n(RAG) methods. It proposes five research questions that guide the analysis of\nthe recent literature from 2020 to 2025, focusing on evaluation methods and\nmitigation techniques. The review also discusses the role of instruction\ntuning, multi-agent reasoning, and external knowledge access via RAG\nframeworks. Key findings highlight the limitations of current metrics, the\nvalue of grounding outputs with validated external evidence, and the importance\nof domain-specific customization to improve factual consistency. Overall, the\nreview underlines the importance of building LLMs that are not only accurate\nand explainable but also tailored for domain-specific fact-checking. These\ninsights contribute to the advancement of research toward more trustworthy and\ncontext-aware language models.",
      "authors": [
        "Subhey Sadi Rahman",
        "Md. Adnanul Islam",
        "Md. Mahbub Alam",
        "Musarrat Zeba",
        "Md. Abdur Rahman",
        "Sadia Sultana Chowa",
        "Mohaimenul Azam Khan Raiaan",
        "Sami Azam"
      ],
      "published": "2025-08-05T19:20:05+00:00",
      "updated": "2025-08-05T19:20:05+00:00",
      "arxiv_id": "2508.03860v1",
      "url": "http://arxiv.org/pdf/2508.03860v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers",
      "abstract": "Building 3-D models is challenging for blind and low-vision (BLV) users due\nto the inherent complexity of 3-D models and the lack of support for non-visual\ninteraction in existing tools. To address this issue, we introduce A11yShape, a\nnovel system designed to help BLV users who possess basic programming skills\nunderstand, modify, and iterate on 3-D models. A11yShape leverages LLMs and\nintegrates with OpenSCAD, a popular open-source editor that generates 3-D\nmodels from code. Key functionalities of A11yShape include accessible\ndescriptions of 3-D models, version control to track changes in models and\ncode, and a hierarchical representation of model components. Most importantly,\nA11yShape employs a cross-representation highlighting mechanism to synchronize\nsemantic selections across all model representations -- code, semantic\nhierarchy, AI description, and 3-D rendering. We conducted a multi-session user\nstudy with four BLV programmers, where, after an initial tutorial session,\nparticipants independently completed 12 distinct models across two testing\nsessions, achieving results that aligned with their own satisfaction. The\nresult demonstrates that participants were able to comprehend provided 3-D\nmodels, as well as independently create and modify 3-D models -- tasks that\nwere previously impossible without assistance from sighted individuals.",
      "authors": [
        "Zhuohao Jerry Zhang",
        "Haichang Li",
        "Chun Meng Yu",
        "Faraz Faruqi",
        "Junan Xie",
        "Gene S-H Kim",
        "Mingming Fan",
        "Angus G. Forbes",
        "Jacob O. Wobbrock",
        "Anhong Guo",
        "Liang He"
      ],
      "published": "2025-08-05T19:09:00+00:00",
      "updated": "2025-08-07T01:27:46+00:00",
      "arxiv_id": "2508.03852v2",
      "url": "http://arxiv.org/pdf/2508.03852v2",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Majority Bit-Aware Watermarking For Large Language Models",
      "abstract": "The growing deployment of Large Language Models (LLMs) in real-world\napplications has raised concerns about their potential misuse in generating\nharmful or deceptive content. To address this issue, watermarking techniques\nhave emerged as a promising solution by embedding identifiable binary messages\ninto generated text for origin verification and misuse tracing. While recent\nefforts have explored multi-bit watermarking schemes capable of embedding rich\ninformation such as user identifiers, they typically suffer from the\nfundamental trade-off between text quality and decoding accuracy: to ensure\nreliable message decoding, they have to restrict the size of preferred token\nsets during encoding, yet such restrictions reduce the quality of the generated\ncontent. In this work, we propose MajorMark, a novel watermarking method that\nimproves this trade-off through majority bit-aware encoding. MajorMark selects\npreferred token sets based on the majority bit of the message, enabling a\nlarger and more flexible sampling of tokens. In contrast to prior methods that\nrely on token frequency analysis for decoding, MajorMark employs a\nclustering-based decoding strategy, which maintains high decoding accuracy even\nwhen the preferred token set is large, thus preserving both content quality and\ndecoding accuracy. We further introduce MajorMark$^+$, which partitions the\nmessage into multiple blocks to independently encode and deterministically\ndecode each block, thereby further enhancing the quality of watermarked text\nand improving decoding accuracy. Extensive experiments on state-of-the-art LLMs\ndemonstrate that our methods significantly enhance both decoding accuracy and\ntext generation quality, outperforming prior multi-bit watermarking baselines.",
      "authors": [
        "Jiahao Xu",
        "Rui Hu",
        "Zikai Zhang"
      ],
      "published": "2025-08-05T18:19:00+00:00",
      "updated": "2025-08-05T18:19:00+00:00",
      "arxiv_id": "2508.03829v1",
      "url": "http://arxiv.org/pdf/2508.03829v1",
      "categories": [
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "AttnTrace: Attention-based Context Traceback for Long-Context LLMs",
      "abstract": "Long-context large language models (LLMs), such as Gemini-2.5-Pro and\nClaude-Sonnet-4, are increasingly used to empower advanced AI systems,\nincluding retrieval-augmented generation (RAG) pipelines and autonomous agents.\nIn these systems, an LLM receives an instruction along with a context--often\nconsisting of texts retrieved from a knowledge database or memory--and\ngenerates a response that is contextually grounded by following the\ninstruction. Recent studies have designed solutions to trace back to a subset\nof texts in the context that contributes most to the response generated by the\nLLM. These solutions have numerous real-world applications, including\nperforming post-attack forensic analysis and improving the interpretability and\ntrustworthiness of LLM outputs. While significant efforts have been made,\nstate-of-the-art solutions such as TracLLM often lead to a high computation\ncost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a\nsingle response-context pair. In this work, we propose AttnTrace, a new context\ntraceback method based on the attention weights produced by an LLM for a\nprompt. To effectively utilize attention weights, we introduce two techniques\ndesigned to enhance the effectiveness of AttnTrace, and we provide theoretical\ninsights for our design choice. We also perform a systematic evaluation for\nAttnTrace. The results demonstrate that AttnTrace is more accurate and\nefficient than existing state-of-the-art context traceback methods. We also\nshow that AttnTrace can improve state-of-the-art methods in detecting prompt\ninjection under long contexts through the attribution-before-detection\nparadigm. As a real-world application, we demonstrate that AttnTrace can\neffectively pinpoint injected instructions in a paper designed to manipulate\nLLM-generated reviews. The code is at\nhttps://github.com/Wang-Yanting/AttnTrace.",
      "authors": [
        "Yanting Wang",
        "Runpeng Geng",
        "Ying Chen",
        "Jinyuan Jia"
      ],
      "published": "2025-08-05T17:56:51+00:00",
      "updated": "2025-08-05T17:56:51+00:00",
      "arxiv_id": "2508.03793v1",
      "url": "http://arxiv.org/pdf/2508.03793v1",
      "categories": [
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward",
      "abstract": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.",
      "authors": [
        "Shudong Liu",
        "Hongwei Liu",
        "Junnan Liu",
        "Linchen Xiao",
        "Songyang Gao",
        "Chengqi Lyu",
        "Yuzhe Gu",
        "Wenwei Zhang",
        "Derek F. Wong",
        "Songyang Zhang",
        "Kai Chen"
      ],
      "published": "2025-08-05T17:55:24+00:00",
      "updated": "2025-08-05T17:55:24+00:00",
      "arxiv_id": "2508.03686v1",
      "url": "http://arxiv.org/pdf/2508.03686v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "No LLM Solved Yu Tsumura's 554th Problem",
      "abstract": "We show, contrary to the optimism about LLM's problem-solving abilities,\nfueled by the recent gold medals that were attained, that a problem exists --\nYu Tsumura's 554th problem -- that a) is within the scope of an IMO problem in\nterms of proof sophistication, b) is not a combinatorics problem which has\ncaused issues for LLMs, c) requires fewer proof techniques than typical hard\nIMO problems, d) has a publicly available solution (likely in the training data\nof LLMs), and e) that cannot be readily solved by any existing off-the-shelf\nLLM (commercial or open-source).",
      "authors": [
        "Simon Frieder",
        "William Hart"
      ],
      "published": "2025-08-05T17:55:20+00:00",
      "updated": "2025-08-05T17:55:20+00:00",
      "arxiv_id": "2508.03685v1",
      "url": "http://arxiv.org/pdf/2508.03685v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Self-Questioning Language Models",
      "abstract": "Can large language models improve without external data -- by generating\ntheir own questions and answers? We hypothesize that a pre-trained language\nmodel can improve its reasoning skills given only a single prompt specifying\nthe topic (e.g., algebra word problems) and asking the model to generate its\nown questions. To do this, we propose Self-Questioning Language Models (SQLM):\nan asymmetric self-play framework where a proposer is given the topic and\ngenerates a question for a solver, who tries to answer it. Both the proposer\nand solver are trained via reinforcement learning. The proposer receives a\nreward if the problem is not too easy or too difficult, and the solver receives\na reward based on majority voting, a proxy for correctness in the absence of\nground-truth answers. For coding, the proposer can instead generate unit tests\nwhich are used for verification. We study this asymmetric self-play framework\non three benchmarks: three-digit multiplication, algebra problems from the\nOMEGA benchmark, and programming problems from Codeforces. By continually\ngenerating more interesting problems and attempting to solve them, language\nmodels can improve on downstream benchmarks without access to any curated\ntraining datasets.",
      "authors": [
        "Lili Chen",
        "Mihir Prabhudesai",
        "Katerina Fragkiadaki",
        "Hao Liu",
        "Deepak Pathak"
      ],
      "published": "2025-08-05T17:51:33+00:00",
      "updated": "2025-08-06T17:23:53+00:00",
      "arxiv_id": "2508.03682v2",
      "url": "http://arxiv.org/pdf/2508.03682v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
      "abstract": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.",
      "authors": [
        "Xufang Luo",
        "Yuge Zhang",
        "Zhiyuan He",
        "Zilong Wang",
        "Siyun Zhao",
        "Dongsheng Li",
        "Luna K. Qiu",
        "Yuqing Yang"
      ],
      "published": "2025-08-05T17:50:13+00:00",
      "updated": "2025-08-05T17:50:13+00:00",
      "arxiv_id": "2508.03680v1",
      "url": "http://arxiv.org/pdf/2508.03680v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation",
      "abstract": "State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general\nbenchmarks like HumanEval but underperform on specialized suites such as\nParEval. Is this due to LLMs missing domain knowledge or insufficient prompt\ndetail is given? To answer this, we introduce PartialOrderEval, which augments\nany code generation benchmark with a partial order of prompts from minimal to\nmaximally detailed. Applying it to HumanEval and both serial and OpenMP subsets\nof ParEval, we measure how pass@1 scales with prompt specificity. Our\nexperiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of\nprompt sensitivity across different tasks, and a qualitative analysis\nhighlights explicit I/O specifications, edge-case handling, and stepwise\nbreakdowns as the key drivers of prompt detail improvement.",
      "authors": [
        "Yangtian Zi",
        "Harshitha Menon",
        "Arjun Guha"
      ],
      "published": "2025-08-05T17:49:48+00:00",
      "updated": "2025-08-05T17:49:48+00:00",
      "arxiv_id": "2508.03678v1",
      "url": "http://arxiv.org/pdf/2508.03678v1",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "FairLangProc: A Python package for fairness in NLP",
      "abstract": "The rise in usage of Large Language Models to near ubiquitousness in recent\nyears has risen societal concern about their applications in decision-making\ncontexts, such as organizational justice or healthcare. This, in turn, poses\nquestions about the fairness of these models in critical settings, which leads\nto the developement of different procedures to address bias in Natural Language\nProcessing. Although many datasets, metrics and algorithms have been proposed\nto measure and mitigate harmful prejudice in Natural Language Processing, their\nimplementation is diverse and far from centralized. As a response, this paper\npresents FairLangProc, a comprehensive Python package providing a common\nimplementation of some of the more recent advances in fairness in Natural\nLanguage Processing providing an interface compatible with the famous Hugging\nFace transformers library, aiming to encourage the widespread use and\ndemocratization of bias mitigation techniques. The implementation can be found\non https://github.com/arturo-perez-peralta/FairLangProc.",
      "authors": [
        "Arturo PÃ©rez-Peralta",
        "Sandra BenÃ­tez-PeÃ±a",
        "Rosa E. Lillo"
      ],
      "published": "2025-08-05T17:47:53+00:00",
      "updated": "2025-08-05T17:47:53+00:00",
      "arxiv_id": "2508.03677v1",
      "url": "http://arxiv.org/pdf/2508.03677v1",
      "categories": [
        "cs.CL",
        "stat.ML",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design",
      "abstract": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts.",
      "authors": [
        "Claudiu Leoveanu-Condrei"
      ],
      "published": "2025-08-05T17:24:50+00:00",
      "updated": "2025-08-05T17:24:50+00:00",
      "arxiv_id": "2508.03665v1",
      "url": "http://arxiv.org/pdf/2508.03665v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.7; I.2.2; I.1.2; D.1.0"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Automated Algorithmic Discovery for Gravitational-Wave Detection Guided by LLM-Informed Evolutionary Monte Carlo Tree Search",
      "abstract": "Computational scientific discovery increasingly relies on algorithms to\nprocess complex data and identify meaningful patterns - yet faces persistent\nchallenges in gravitational-wave signal identification. While existing\nalgorithmic approaches like matched filtering (MF) and deep neural networks\n(DNNs) have achieved partial success, their limitations directly stem from\nfundamental limitations: MF's excessive computational demands arise from its\nreliance on predefined theoretical waveform templates, while DNNs' black-box\narchitectures obscure decision logic and introduce hidden biases. We propose\nEvolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses\nthese limitations through systematic algorithm space exploration guided by\ndomain-aware physical constraints. Our approach combines tree-structured search\nwith evolutionary optimization and large language model heuristics to create\ninterpretable algorithmic solutions. Our Evo-MCTS framework demonstrates\nsubstantial improvements, achieving a 20.2\\% improvement over state-of-the-art\ngravitational wave detection algorithms on the MLGWSC-1 benchmark dataset.\nHigh-performing algorithm variants consistently exceed thresholds. The\nframework generates human-interpretable algorithmic pathways that reveal\ndistinct performance patterns. Beyond performance improvements, our framework\ndiscovers novel algorithmic combinations, thereby establishing a transferable\nmethodology for automated algorithmic discovery across computational science\ndomains.",
      "authors": [
        "He Wang",
        "Liang Zeng"
      ],
      "published": "2025-08-05T17:18:20+00:00",
      "updated": "2025-08-05T17:18:20+00:00",
      "arxiv_id": "2508.03661v1",
      "url": "http://arxiv.org/pdf/2508.03661v1",
      "categories": [
        "cs.AI",
        "astro-ph.HE",
        "astro-ph.IM",
        "gr-qc"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?",
      "abstract": "Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language\nModels (MLLMs) show great promise for complex document understanding, yet their\ndevelopment is critically hampered by inadequate evaluation. Current benchmarks\noften focus on specific part of document RAG system and use synthetic data with\nincomplete ground truth and evidence labels, therefore failing to reflect\nreal-world bottlenecks and challenges. To overcome these limitations, we\nintroduce Double-Bench: a new large-scale, multilingual, and multimodal\nevaluation system that is able to produce fine-grained assessment to each\ncomponent within document RAG systems. It comprises 3,276 documents (72,880\npages) and 5,168 single- and multi-hop queries across 6 languages and 4\ndocument types with streamlined dynamic update support for potential data\ncontamination issues. Queries are grounded in exhaustively scanned evidence\npages and verified by human experts to ensure maximum quality and completeness.\nOur comprehensive experiments across 9 state-of-the-art embedding models, 4\nMLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text\nand visual embedding models is narrowing, highlighting the need in building\nstronger document retrieval models. Our findings also reveal the\nover-confidence dilemma within current document RAG frameworks that tend to\nprovide answer even without evidence support. We hope our fully open-source\nDouble-Bench provide a rigorous foundation for future research in advanced\ndocument RAG systems. We plan to retrieve timely corpus and release new\nbenchmarks on an annual basis.",
      "authors": [
        "Wenxuan Shen",
        "Mingjia Wang",
        "Yaochen Wang",
        "Dongping Chen",
        "Junjie Yang",
        "Yao Wan",
        "Weiwei Lin"
      ],
      "published": "2025-08-05T16:55:02+00:00",
      "updated": "2025-08-05T16:55:02+00:00",
      "arxiv_id": "2508.03644v1",
      "url": "http://arxiv.org/pdf/2508.03644v1",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.IR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "SlideAudit: A Dataset and Taxonomy for Automated Evaluation of Presentation Slides",
      "abstract": "Automated evaluation of specific graphic designs like presentation slides is\nan open problem. We present SlideAudit, a dataset for automated slide\nevaluation. We collaborated with design experts to develop a thorough taxonomy\nof slide design flaws. Our dataset comprises 2400 slides collected and\nsynthesized from multiple sources, including a subset intentionally modified\nwith specific design problems. We then fully annotated them using our taxonomy\nthrough strictly trained crowdsourcing from Prolific. To evaluate whether AI is\ncapable of identifying design flaws, we compared multiple large language models\nunder different prompting strategies, and with an existing design critique\npipeline. We show that AI models struggle to accurately identify slide design\nflaws, with F1 scores ranging from 0.331 to 0.655. Notably, prompting\ntechniques leveraging our taxonomy achieved the highest performance. We further\nconducted a remediation study to assess AI's potential for improving slides.\nAmong 82.0% of slides that showed significant improvement, 87.8% of them were\nimproved more with our taxonomy, further demonstrating its utility.",
      "authors": [
        "Zhuohao Jerry Zhang",
        "Ruiqi Chen",
        "Mingyuan Zhong",
        "Jacob O. Wobbrock"
      ],
      "published": "2025-08-05T16:47:43+00:00",
      "updated": "2025-08-05T16:47:43+00:00",
      "arxiv_id": "2508.03630v1",
      "url": "http://arxiv.org/pdf/2508.03630v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for Advertiser Keyphrase Recommendations at eBay",
      "abstract": "Sellers at eBay are recommended keyphrases to bid on to enhance the\nperformance of their advertising campaigns. The relevance of these keyphrases\nis crucial in avoiding the overcrowding of search systems with irrelevant items\nand maintaining a positive seller perception. It is essential that keyphrase\nrecommendations align with both seller and Search judgments regarding auctions.\nDue to the difficulty in procuring negative human judgment at scale, employing\nLLM-as-a-judge to mimic seller judgment has been established as the norm in\nseveral studies. This study introduces a novel two-step LLM distillation\nprocess from a LLM-judge used to debias our Embedding Based Retrieval (EBR)\nmodel from the various biases that exist in click-data. We distill from an LLM\nteacher via a cross-encoder assistant into a bi-encoder student using a\nmulti-task training approach, ultimately employing the student bi-encoder to\nretrieve relevant advertiser keyphrases. We show that integrating a knowledge\ndistillation process from LLMs in a multi-task training setup enhances\nbi-encoder performance in retrieving relevant advertiser keyphrases at eBay.",
      "authors": [
        "Soumik Dey",
        "Benjamin Braun",
        "Naveen Ravipati",
        "Hansi Wu",
        "Binbin Li"
      ],
      "published": "2025-08-05T16:47:17+00:00",
      "updated": "2025-08-05T16:47:17+00:00",
      "arxiv_id": "2508.03628v1",
      "url": "http://arxiv.org/pdf/2508.03628v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework",
      "abstract": "With the advancement of code generation capabilities in large language models\n(LLMs), their reliance on input premises has intensified. When users provide\ninputs containing faulty premises, the probability of code generation\nhallucinations rises significantly, exposing deficiencies in their\nself-scrutiny capabilities. This paper proposes Faulty Premises Bench\n(FPBench), the first code generation evaluation framework targeting faulty\npremises. By systematically constructing three categories of faulty premises\nand integrating multi-dimensional evaluation metrics, it conducts in-depth\nassessments of 15 representative LLMs. The key findings are as follows: (1)\nMost models exhibit poor reasoning abilities and suboptimal code generation\nperformance under faulty premises, heavily relying on explicit prompts for\nerror detection, with limited self-scrutiny capabilities; (2) Faulty premises\ntrigger a point of diminishing returns in resource investment, leading to\nblindly increasing length fails to enhance quality; (3) The three types of\nfaulty premises respectively activate distinct defect patterns in models,\nrevealing a triple dissociation in the cognitive mechanisms of code generation\nmodels. This study not only highlights the urgent need for LLMs to proactively\nverify premises in code generation but also, through the proposed FPBench\nframework and multi-dimensional evaluation system, provides a theoretical\nfoundation and practical pathway for developing reliable, human-centric code\ngeneration models.",
      "authors": [
        "Jialin Li",
        "Jinzhe Li",
        "Gengxu Li",
        "Yi Chang",
        "Yuan Wu"
      ],
      "published": "2025-08-05T16:39:39+00:00",
      "updated": "2025-08-05T16:39:39+00:00",
      "arxiv_id": "2508.03622v1",
      "url": "http://arxiv.org/pdf/2508.03622v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Block: Balancing Load in LLM Serving with Context, Knowledge and Predictive Scheduling",
      "abstract": "This paper presents Block, a distributed scheduling framework designed to\noptimize load balancing and auto-provisioning across instances in large\nlanguage model serving frameworks by leveraging contextual information from\nincoming requests. Unlike popular model serving systems that rely on monolithic\nand heuristic task schedulers, Block operates as a fully distributed,\nstateless, and predictive scheduling system to achieve low overhead,\nreliability, and scalability. It leverages the deterministic and predictable\ncharacteristics of LLM inferences, such as host configurations, response\nlengths, and hardware performance, to make scheduling decisions based on\naccurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block\nsignificantly outperforms heuristic schedulers, boosting serving capacity by up\nto 16.7\\% and reducing P99 tail latency by up to 49.5\\%. These performance\ngains remain consistent across diverse models, workloads and configurations.\nCode and data are open-sourced.",
      "authors": [
        "Wei Da",
        "Evangelia Kalyvianaki"
      ],
      "published": "2025-08-05T16:27:10+00:00",
      "updated": "2025-08-05T16:27:10+00:00",
      "arxiv_id": "2508.03611v1",
      "url": "http://arxiv.org/pdf/2508.03611v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC"
    },
    {
      "title": "ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated Test Programs",
      "abstract": "Existing LLM-based compiler fuzzers often produce syntactically or\nsemantically invalid test programs, limiting their effectiveness in exercising\ncompiler optimizations and backend components. We introduce ReFuzzer, a\nframework for refining LLM-generated test programs by systematically detecting\nand correcting compilation and runtime violations (e.g. division by zero or\narray out-of-bounds accesses). ReFuzzer employs a feedback loop with a local\nLLM to validate and filter erroneous programs before execution, improving\nfuzzing effectiveness beyond crash detection and enabling the generation of\ndiverse yet valid test programs.\n  We evaluated ReFuzzer's effectiveness across black-, grey- and white-box\nfuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs'\nvalidity from 47.0-49.4% to 96.6-97.3%, with an average processing time of\n2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing\nsignificantly increased code coverage in critical optimization and IR\ngeneration components. For example, vectorization coverage had an absolute\nimprovement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing,\nenhancing testing effectiveness.",
      "authors": [
        "Iti Shree",
        "Karine Even-Mendoz",
        "Tomasz Radzik"
      ],
      "published": "2025-08-05T16:17:02+00:00",
      "updated": "2025-08-05T16:17:02+00:00",
      "arxiv_id": "2508.03603v1",
      "url": "http://arxiv.org/pdf/2508.03603v1",
      "categories": [
        "cs.SE",
        "cs.PL"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "OpenLifelogQA: An Open-Ended Multi-Modal Lifelog Question-Answering Dataset",
      "abstract": "Lifelogging refers to the process of passively collecting, storing, and\nanalysing personal daily life data using wearable devices. This data can\nsupport applications in memory preservation and enhancement. For example, using\nan ask-and-answer strategy, question-answering (QA) on lifelog data opens an\ninteractive and interesting way to explore memorable events and insights into\ndaily life. However, research resources for QA on lifelog data are limited to\nsmall-sized or synthetic QA datasets. In this paper, we present a novel lifelog\nQA dataset called OpenLifelogQA, building upon an 18-month lifelog dataset. Our\ndataset focuses on an open-ended and practical QA with real-world application\nin daily lifelog usage. We construct 14,187 pairs of Q&A with diverse types and\ndifficulty levels. A baseline experiment is reported for this dataset with\ncompetitive average performance of 89.7% BERT Score, 25.87% ROUGE-L and 3.9665\nLLM Score from LLaVA-NeXT-Interleave 7B model. We release this Q&A dataset to\nthe research community to support new research into lifelog technologies, such\nas enabling personal chat-based assistants for lifelog data to become a\nreality.",
      "authors": [
        "Quang-Linh Tran",
        "Binh Nguyen",
        "Gareth J. F. Jones",
        "Cathal Gurrin"
      ],
      "published": "2025-08-05T15:50:16+00:00",
      "updated": "2025-08-05T15:50:16+00:00",
      "arxiv_id": "2508.03583v1",
      "url": "http://arxiv.org/pdf/2508.03583v1",
      "categories": [
        "cs.MM",
        "cs.IR"
      ],
      "primary_category": "cs.MM"
    },
    {
      "title": "Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation",
      "abstract": "Large Language Models (LLMs) often suffer from performance degradation when\nfaced with domain shifts, primarily due to catastrophic forgetting. In this\nwork, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation),\na novel continual learning framework that integrates dynamic knowledge graphs\nwith instruction tuning. By leveraging retrieved domain-specific knowledge as\nguidance during training, KILO enhances both adaptability to new domains and\nretention of previously acquired knowledge. We pretrain our model on\nWikiText-103 and evaluate sequential adaptation across four diverse target\ndomains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that\nKILO consistently outperforms strong baselines, including continual\nfine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward\ntransfer, F1 score, retention rate, and training efficiency. These results\nhighlight the effectiveness of combining structured knowledge retrieval and\ninstruction prompting to overcome domain shift challenges in continual learning\nscenarios.",
      "authors": [
        "Iing Muttakhiroh",
        "Thomas Fevens"
      ],
      "published": "2025-08-05T15:39:37+00:00",
      "updated": "2025-08-05T15:39:37+00:00",
      "arxiv_id": "2508.03571v1",
      "url": "http://arxiv.org/pdf/2508.03571v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching",
      "abstract": "Internet memes, now a staple of digital communication, play a pivotal role in\nhow users engage within online communities and allow researchers to gain\ninsight into contemporary digital culture. These engaging user-generated\ncontent are characterised by their reuse of visual elements also found in other\nmemes. Matching instances of memes via these shared visual elements, called\nMeme Matching, is the basis of a wealth of meme analysis approaches. However,\nmost existing methods assume that every meme consists of a shared visual\nbackground, called a Template, with some overlaid text, thereby limiting meme\nmatching to comparing the background image alone. Current approaches exclude\nthe many memes that are not template-based and limit the effectiveness of\nautomated meme analysis and would not be effective at linking memes to\ncontemporary web-based meme dictionaries. In this work, we introduce a broader\nformulation of meme matching that extends beyond template matching. We show\nthat conventional similarity measures, including a novel segment-wise\ncomputation of the similarity measures, excel at matching template-based memes\nbut fall short when applied to non-template-based meme formats. However, the\nsegment-wise approach was found to consistently outperform the whole-image\nmeasures on matching non-template-based memes. Finally, we explore a\nprompting-based approach using a pretrained Multimodal Large Language Model for\nmeme matching. Our results highlight that accurately matching memes via shared\nvisual elements, not just background templates, remains an open challenge that\nrequires more sophisticated matching techniques.",
      "authors": [
        "Muzhaffar Hazman",
        "Susan McKeever",
        "Josephine Griffith"
      ],
      "published": "2025-08-05T15:31:00+00:00",
      "updated": "2025-08-05T15:31:00+00:00",
      "arxiv_id": "2508.03562v1",
      "url": "http://arxiv.org/pdf/2508.03562v1",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "LaTCoder: Converting Webpage Design to Code with Layout-as-Thought",
      "abstract": "Converting webpage designs into code (design-to-code) plays a vital role in\nUser Interface (UI) development for front-end developers, bridging the gap\nbetween visual design and functional implementation. While recent Multimodal\nLarge Language Models (MLLMs) have shown significant potential in\ndesign-to-code tasks, they often fail to accurately preserve the layout during\ncode generation. To this end, we draw inspiration from the Chain-of-Thought\n(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that\nenhances layout preservation in webpage design during code generation with\nLayout-as-Thought (LaT). Specifically, we first introduce a simple yet\nefficient algorithm to divide the webpage design into image blocks. Next, we\nprompt MLLMs using a CoTbased approach to generate code for each block.\nFinally, we apply two assembly strategies-absolute positioning and an\nMLLM-based method-followed by dynamic selection to determine the optimal\noutput. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs\n(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly\nintroduced, more challenging benchmark (CC-HARD) that features complex layouts.\nThe experimental results on automatic metrics demonstrate significant\nimprovements. Specifically, TreeBLEU scores increased by 66.67% and MAE\ndecreased by 38% when using DeepSeek-VL2, compared to direct prompting.\nMoreover, the human preference evaluation results indicate that annotators\nfavor the webpages generated by LaTCoder in over 60% of cases, providing strong\nevidence of the effectiveness of our method.",
      "authors": [
        "Yi Gui",
        "Zhen Li",
        "Zhongyi Zhang",
        "Guohao Wang",
        "Tianpeng Lv",
        "Gaoyang Jiang",
        "Yi Liu",
        "Dongping Chen",
        "Yao Wan",
        "Hongyu Zhang",
        "Wenbin Jiang",
        "Xuanhua Shi",
        "Hai Jin"
      ],
      "published": "2025-08-05T15:28:48+00:00",
      "updated": "2025-08-05T15:28:48+00:00",
      "arxiv_id": "2508.03560v1",
      "url": "http://arxiv.org/pdf/2508.03560v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "SAGE-HLS: Syntax-Aware AST-Guided LLM for High-Level Synthesis Code Generation",
      "abstract": "In today's rapidly evolving field of electronic design automation (EDA), the\ncomplexity of hardware designs is increasing, necessitating more sophisticated\nautomation solutions. High-level synthesis (HLS), as a pivotal solution,\nautomates hardware designs from high-level abstractions (e.g., C/C++). However,\nit faces significant challenges, particularly in design space exploration and\noptimization. While large language models (LLMs) have shown notable\ncapabilities in code generation, their application to HLS has been limited due\nto the scarcity of (publicly) available HLS code datasets. Hence, research in\nthis domain has primarily focused on techniques such as prompt engineering and\nretrieval-augmented generation (RAG). To overcome this limitation, this paper\nintroduces SAGE-HLS, the first-of-its-kind fine-tuned LLM specifically for HLS\ncode generation. Our method includes three key advancements: (i) We implement\nVerilog-to-C/C++ porting, converting verified and synthesizable Verilog codes\ninto corresponding C, creating a dataset of 16.7K HLS codes; (ii) We implement\na fine-tuning strategy, which is based on instruction prompting to code\ngeneration guided by abstract syntax tree (AST); (iii) We develop a\nsemi-automated evaluation framework using VerilogEval to assess the\nfunctionality of the generated HLS code. Our experiments show that SAGE-HLS,\nfined-tuned on the QwenCoder (2.5) 7B model, achieves a near 100% success rate\nin code synthesizability and a 75% success rate in functional correctness.",
      "authors": [
        "M Zafir Sadik Khan",
        "Nowfel Mashnoor",
        "Mohammad Akyash",
        "Kimia Azar",
        "Hadi Kamali"
      ],
      "published": "2025-08-05T15:28:13+00:00",
      "updated": "2025-08-05T15:28:13+00:00",
      "arxiv_id": "2508.03558v1",
      "url": "http://arxiv.org/pdf/2508.03558v1",
      "categories": [
        "cs.PL"
      ],
      "primary_category": "cs.PL"
    },
    {
      "title": "VRPRM: Process Reward Modeling via Visual Reasoning",
      "abstract": "Process Reward Model (PRM) is widely used in the post-training of Large\nLanguage Model (LLM) because it can perform fine-grained evaluation of the\nreasoning steps of generated content. However, most PRMs lack long-term\nreasoning and deep thinking capabilities. On the other hand, although a few\nworks have tried to introduce Chain-of-Thought capability into PRMs, the\nannotation cost of CoT-PRM data is too expensive to play a stable role in\nvarious tasks. To address the above challenges, we propose VRPRM, a process\nreward model via visual reasoning, and design an efficient two-stage training\nstrategy. Experimental results show that using only 3.6K CoT-PRM SFT data and\n50K non-CoT PRM RL training data, VRPRM can surpass the non-thinking PRM with a\ntotal data volume of 400K and achieved a relative performance improvement of up\nto 118\\% over the base model in the BoN experiment. This result confirms that\nthe proposed combined training strategy can achieve higher quality reasoning\ncapabilities at a lower data annotation cost, thus providing a new paradigm for\nPRM training with more efficient data utilization.",
      "authors": [
        "Xinquan Chen",
        "Bangwei Liu",
        "Xuhong Wang"
      ],
      "published": "2025-08-05T15:25:24+00:00",
      "updated": "2025-08-05T15:25:24+00:00",
      "arxiv_id": "2508.03556v1",
      "url": "http://arxiv.org/pdf/2508.03556v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation",
      "abstract": "Retrieval Augmented Generation (RAG) has emerged as a promising solution to\naddress hallucination issues in Large Language Models (LLMs). However, the\nintegration of multiple retrieval sources, while potentially more informative,\nintroduces new challenges that can paradoxically exacerbate hallucination\nproblems. These challenges manifest primarily in two aspects: the sparse\ndistribution of multi-source data that hinders the capture of logical\nrelationships and the inherent inconsistencies among different sources that\nlead to information conflicts. To address these challenges, we propose\nMultiRAG, a novel framework designed to mitigate hallucination in multi-source\nretrieval-augmented generation through knowledge-guided approaches. Our\nframework introduces two key innovations: (1) a knowledge construction module\nthat employs multi-source line graphs to efficiently aggregate logical\nrelationships across different knowledge sources, effectively addressing the\nsparse data distribution issue; and (2) a sophisticated retrieval module that\nimplements a multi-level confidence calculation mechanism, performing both\ngraph-level and node-level assessments to identify and eliminate unreliable\ninformation nodes, thereby reducing hallucinations caused by inter-source\ninconsistencies. Extensive experiments on four multi-domain query datasets and\ntwo multi-hop QA datasets demonstrate that MultiRAG significantly enhances the\nreliability and efficiency of knowledge retrieval in complex multi-source\nscenarios. \\textcolor{blue}{Our code is available in\nhttps://github.com/wuwenlong123/MultiRAG.",
      "authors": [
        "Wenlong Wu",
        "Haofen Wang",
        "Bohan Li",
        "Peixuan Huang",
        "Xinzhe Zhao",
        "Lei Liang"
      ],
      "published": "2025-08-05T15:20:52+00:00",
      "updated": "2025-08-05T15:20:52+00:00",
      "arxiv_id": "2508.03553v1",
      "url": "http://arxiv.org/pdf/2508.03553v1",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations",
      "abstract": "The growing scale of evaluation tasks has led to the widespread adoption of\nautomated evaluation using large language models, a paradigm known as\n\"LLMas-a-judge.\" However, improving its alignment with human preferences\nwithout complex prompts or fine-tuning remains challenging. In this work,\nmotivated by preliminary findings that middle-to-upper layers encode\nsemantically and taskrelevant representations that are often more aligned with\nhuman judgments than the final layer, we propose LAGER, a lightweight and\nefficient framework for enhancing LLM-as-a-Judge alignment with human scoring,\nvia internal representations. LAGER produces fine-grained judgment scores by\naggregating cross-layer scoretoken logits and computing the expected score from\na softmax-based distribution, with the LLM backbone kept frozen. LAGER fully\nleverages the complementary information across different layers, overcoming the\nlimitations of relying solely on the final layer. We evaluate our method on the\nstandard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman\ncorrelation, and find that LAGER achieves improvements of up to 7.5% over the\nbest baseline across these benchmarks. Without reasoning steps, LAGER matches\nor outperforms reasoning-based methods. Experiments on downstream applications,\nsuch as data selection and emotional understanding, further show the\neffectiveness of our method.",
      "authors": [
        "Peng Lai",
        "Jianjie Zheng",
        "Sijie Cheng",
        "Yun Chen",
        "Peng Li",
        "Yang Liu",
        "Guanhua Chen"
      ],
      "published": "2025-08-05T15:18:36+00:00",
      "updated": "2025-08-05T15:18:36+00:00",
      "arxiv_id": "2508.03550v1",
      "url": "http://arxiv.org/pdf/2508.03550v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs and Vision Models",
      "abstract": "Large language models (LLMs) have enabled the automatic generation of\nstep-by-step augmented reality (AR) instructions for a wide range of physical\ntasks. However, existing LLM-based AR guidance often lacks rich visual\naugmentations to effectively embed instructions into spatial context for a\nbetter user understanding. We present Guided Reality, a fully automated AR\nsystem that generates embedded and dynamic visual guidance based on\nstep-by-step instructions. Our system integrates LLMs and vision models to: 1)\ngenerate multi-step instructions from user queries, 2) identify appropriate\ntypes of visual guidance, 3) extract spatial information about key interaction\npoints in the real world, and 4) embed visual guidance in physical space to\nsupport task execution. Drawing from a corpus of user manuals, we define five\ncategories of visual guidance and propose an identification strategy based on\nthe current step. We evaluate the system through a user study (N=16),\ncompleting real-world tasks and exploring the system in the wild. Additionally,\nfour instructors shared insights on how Guided Reality could be integrated into\ntheir training workflows.",
      "authors": [
        "Ada Yi Zhao",
        "Aditya Gunturu",
        "Ellen Yi-Luen Do",
        "Ryo Suzuki"
      ],
      "published": "2025-08-05T15:15:35+00:00",
      "updated": "2025-08-05T15:15:35+00:00",
      "arxiv_id": "2508.03547v1",
      "url": "http://arxiv.org/pdf/2508.03547v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "CoEmoGen: Towards Semantically-Coherent and Scalable Emotional Image Content Generation",
      "abstract": "Emotional Image Content Generation (EICG) aims to generate semantically clear\nand emotionally faithful images based on given emotion categories, with broad\napplication prospects. While recent text-to-image diffusion models excel at\ngenerating concrete concepts, they struggle with the complexity of abstract\nemotions. There have also emerged methods specifically designed for EICG, but\nthey excessively rely on word-level attribute labels for guidance, which suffer\nfrom semantic incoherence, ambiguity, and limited scalability. To address these\nchallenges, we propose CoEmoGen, a novel pipeline notable for its semantic\ncoherence and high scalability. Specifically, leveraging multimodal large\nlanguage models (MLLMs), we construct high-quality captions focused on\nemotion-triggering content for context-rich semantic guidance. Furthermore,\ninspired by psychological insights, we design a Hierarchical Low-Rank\nAdaptation (HiLoRA) module to cohesively model both polarity-shared low-level\nfeatures and emotion-specific high-level semantics. Extensive experiments\ndemonstrate CoEmoGen's superiority in emotional faithfulness and semantic\ncoherence from quantitative, qualitative, and user study perspectives. To\nintuitively showcase scalability, we curate EmoArt, a large-scale dataset of\nemotionally evocative artistic images, providing endless inspiration for\nemotion-driven artistic creation. The dataset and code are available at\nhttps://github.com/yuankaishen2001/CoEmoGen.",
      "authors": [
        "Kaishen Yuan",
        "Yuting Zhang",
        "Shang Gao",
        "Yijie Zhu",
        "Wenshuo Chen",
        "Yutao Yue"
      ],
      "published": "2025-08-05T15:04:34+00:00",
      "updated": "2025-08-05T15:04:34+00:00",
      "arxiv_id": "2508.03535v1",
      "url": "http://arxiv.org/pdf/2508.03535v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models",
      "abstract": "Effectively adapting powerful pretrained foundation models to diverse tasks\nremains a key challenge in AI deployment. Current approaches primarily follow\ntwo paradigms:discrete optimization of text prompts through prompt engineering,\nor continuous adaptation via additional trainable parameters. Both exhibit\nlimitations-discrete methods lack refinement precision while parameter-based\ntechniques increase complexity and reduce interpretability. To address these\nconstraints, we propose EmbedGrad, a novel framework that optimizes text prompt\nembeddings through gradient-based refinement. Our approach uniquely decouples\ntraining from deployment:during optimization,labeled examples guide precise\nembedding adjustments while preserving semantic meaning; during inference, only\noptimized embeddings integrate with user queries. This enables fine-grained\ncalibration impossible in text space, such as enhancing the reasoning\ncapability of prompts like please reason step by step. Comprehensive\nevaluations across mathematical reasoning, sentiment analysis, and causal\njudgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning\nprompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\\% to 58.96\\% on\nmathematical problems. Consistent improvements were observed across model\nscales (0.5B-14B) and all tasks, with particularly significant gains for\nsmaller models on complex problems like causal judgment. By bridging prompt\nengineering and parameter efficiency without architectural changes, our work\nestablishes embedding refinement as a powerful new paradigm for task\nadaptation.",
      "authors": [
        "Xiaoming Hou",
        "Jiquan Zhang",
        "Zibin Lin",
        "DaCheng Tao",
        "Shengli Zhang"
      ],
      "published": "2025-08-05T15:03:10+00:00",
      "updated": "2025-08-05T15:03:10+00:00",
      "arxiv_id": "2508.03533v1",
      "url": "http://arxiv.org/pdf/2508.03533v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Marito: Structuring and Building Open Multilingual Terminologies for South African NLP",
      "abstract": "The critical lack of structured terminological data for South Africa's\nofficial languages hampers progress in multilingual NLP, despite the existence\nof numerous government and academic terminology lists. These valuable assets\nremain fragmented and locked in non-machine-readable formats, rendering them\nunusable for computational research and development. \\emph{Marito} addresses\nthis challenge by systematically aggregating, cleaning, and standardising these\nscattered resources into open, interoperable datasets. We introduce the\nfoundational \\emph{Marito} dataset, released under the equitable,\nAfrica-centered NOODL framework. To demonstrate its immediate utility, we\nintegrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline.\nExperiments show substantial improvements in the accuracy and domain-specific\nconsistency of English-to-Tshivenda machine translation for large language\nmodels. \\emph{Marito} provides a scalable foundation for developing robust and\nequitable NLP technologies, ensuring South Africa's rich linguistic diversity\nis represented in the digital age.",
      "authors": [
        "Vukosi Marivate",
        "Isheanesu Dzingirai",
        "Fiskani Banda",
        "Richard Lastrucci",
        "Thapelo Sindane",
        "Keabetswe Madumo",
        "Kayode Olaleye",
        "Abiodun Modupe",
        "Unarine Netshifhefhe",
        "Herkulaas Combrink",
        "Mohlatlego Nakeng",
        "Matome Ledwaba"
      ],
      "published": "2025-08-05T15:00:02+00:00",
      "updated": "2025-08-05T15:00:02+00:00",
      "arxiv_id": "2508.03529v1",
      "url": "http://arxiv.org/pdf/2508.03529v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "MoKA: Mixture of Kronecker Adapters",
      "abstract": "Parameter-efficient fine-tuning (PEFT) is essential for reducing the\ncomputational overhead of large language models (LLMs). Low-rank family\nadapters are commonly used to control the parameter size efficiently while\nmaintaining the generative power of LLMs. However, their limited expressiveness\ndue to the rank constraint often restricts their performance on complex tasks.\nWe propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker\nadapters that addresses this limitation by modeling weight updates as a mixture\nof Kronecker products. Our proposed adapter leverages a gating mechanism that\nmeasures the importance of each Kronecker factor, enabling more expressive\nadaptation. Moreover, MoKA enables a rank flexibility that provides a better\ntrade-off between parameter efficiency and accuracy. To ensure hardware\nefficiency, we reformulate Kronecker computations using standard matrix\noperations, allowing seamless deployment on GPU-optimized hardware. We conduct\nextensive experiments on instruction-tuning and commonsense reasoning tasks\nusing low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not\nonly outperforms PEFT baselines, but also reduces the number of trainable\nparameters up to 27x, achieving state-of-the-art trade-offs between performance\nand parameter efficiency.",
      "authors": [
        "Mohammadreza Sadeghi",
        "Mahsa Ghazvini Nejad",
        "MirHamed Jafarzadeh Asl",
        "Yu Gu",
        "Yuanhao Yu",
        "Masoud Asgharian",
        "Vahid Partovi Nia"
      ],
      "published": "2025-08-05T14:58:14+00:00",
      "updated": "2025-08-05T14:58:14+00:00",
      "arxiv_id": "2508.03527v1",
      "url": "http://arxiv.org/pdf/2508.03527v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "FilBench: Can LLMs Understand and Generate Filipino?",
      "abstract": "Despite the impressive performance of LLMs on English-based tasks, little is\nknown about their capabilities in specific languages such as Filipino. In this\nwork, we address this gap by introducing FilBench, a Filipino-centric benchmark\ndesigned to evaluate LLMs across a diverse set of tasks and capabilities in\nFilipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to\nreflect the priorities and trends of NLP research in the Philippines such as\nCultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By\nevaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs\nsuffer from reading comprehension and translation capabilities. Our results\nindicate that FilBench is challenging, with the best model, GPT-4o, achieving\nonly a score of 72.23%. Moreover, we also find that models trained specifically\nfor Southeast Asian languages tend to underperform on FilBench, with the\nhighest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%.\nOur work demonstrates the value of curating language-specific LLM benchmarks to\naid in driving progress on Filipino NLP and increasing the inclusion of\nPhilippine languages in LLM development.",
      "authors": [
        "Lester James V. Miranda",
        "Elyanah Aco",
        "Conner Manuel",
        "Jan Christian Blaise Cruz",
        "Joseph Marvin Imperial"
      ],
      "published": "2025-08-05T14:48:32+00:00",
      "updated": "2025-08-05T14:48:32+00:00",
      "arxiv_id": "2508.03523v1",
      "url": "http://arxiv.org/pdf/2508.03523v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning",
      "abstract": "Research on applications of Reinforcement Learning (RL) to Large Language\nModels (LLMs) has mostly been focused on single-turn problems, such as\nmathematical reasoning or single-shot code generation. While these problems can\nbe viewed as token-level multi-turn MDPs, this view corresponds to a degenerate\ncase of multi-turn interaction where the environment provides no feedback. This\ncontrasts with many real-world domains, such as software engineering (SWE),\nwhich require rich multi-turn interactions with a stateful environment that\nresponds to each action with a non-trivial observation.\n  To bridge this gap, we demonstrate the successful application of RL to this\ngeneral regime. Using a modified Decoupled Advantage Policy Optimization (DAPO)\nalgorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world\nsoftware engineering tasks. Our approach increases the agent's success rate on\nthe SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to\n39%, without relying on any teacher models. On SWE-rebench, our agent matches\nor outperforms leading open-weight models such as DeepSeek-V3-0324 and\nQwen3-235B-A22B using an identical scaffolding, offering a viable path toward\nbuilding more capable autonomous agents for complex real-world problems based\non open models.",
      "authors": [
        "Alexander Golubev",
        "Maria Trofimova",
        "Sergei Polezhaev",
        "Ibragim Badertdinov",
        "Maksim Nekrashevich",
        "Anton Shevtsov",
        "Simon Karasik",
        "Sergey Abramov",
        "Andrei Andriushchenko",
        "Filipp Fisin",
        "Sergei Skvortsov",
        "Boris Yangel"
      ],
      "published": "2025-08-05T14:30:47+00:00",
      "updated": "2025-08-05T14:30:47+00:00",
      "arxiv_id": "2508.03501v1",
      "url": "http://arxiv.org/pdf/2508.03501v1",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Error Detection and Correction for Interpretable Mathematics in Large Language Models",
      "abstract": "Recent large language models (LLMs) have demonstrated the ability to perform\nexplicit multi-step reasoning such as chain-of-thought prompting. However,\ntheir intermediate steps often contain errors that can propagate leading to\ninaccurate final predictions. Additionally, LLMs still struggle with\nhallucinations and often fail to adhere to prescribed output formats, which is\nparticularly problematic for tasks like generating mathematical expressions or\nsource code. This work introduces EDCIM (Error Detection and Correction for\nInterpretable Mathematics), a method for detecting and correcting these errors\nin interpretable mathematics tasks, where the model must generate the exact\nfunctional form that explicitly solve the problem (expressed in natural\nlanguage) rather than a black-box solution. EDCIM uses LLMs to generate a\nsystem of equations for a given problem, followed by a symbolic error-detection\nframework that identifies errors and provides targeted feedback for LLM-based\ncorrection. To optimize efficiency, EDCIM integrates lightweight, open-source\nLLMs with more powerful proprietary models, balancing cost and accuracy. This\nbalance is controlled by a single hyperparameter, allowing users to control the\ntrade-off based on their cost and accuracy requirements. Experimental results\nacross different datasets show that EDCIM significantly reduces both\ncomputational and financial costs, while maintaining, and even improving,\nprediction accuracy when the balance is properly configured.",
      "authors": [
        "Yijin Yang",
        "Cristina Cornelio",
        "Mario Leiva",
        "Paulo Shakarian"
      ],
      "published": "2025-08-05T14:30:35+00:00",
      "updated": "2025-08-05T14:30:35+00:00",
      "arxiv_id": "2508.03500v1",
      "url": "http://arxiv.org/pdf/2508.03500v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation",
      "abstract": "Product sustainability reports provide valuable insights into the\nenvironmental impacts of a product and are often distributed in PDF format.\nThese reports often include a combination of tables and text, which complicates\ntheir analysis. The lack of standardization and the variability in reporting\nformats further exacerbate the difficulty of extracting and interpreting\nrelevant information from large volumes of documents. In this paper, we tackle\nthe challenge of answering questions related to carbon footprints within\nsustainability reports available in PDF format. Unlike previous approaches, our\nfocus is on addressing the difficulties posed by the unstructured and\ninconsistent nature of text extracted from PDF parsing. To facilitate this\nanalysis, we introduce CarbonPDF-QA, an open-source dataset containing\nquestion-answer pairs for 1735 product report documents, along with\nhuman-annotated answers. Our analysis shows that GPT-4o struggles to answer\nquestions with data inconsistencies. To address this limitation, we propose\nCarbonPDF, an LLM-based technique specifically designed to answer carbon\nfootprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama\n3 with our training data. Our results show that our technique outperforms\ncurrent state-of-the-art techniques, including question-answering (QA) systems\nfinetuned on table and text data.",
      "authors": [
        "Kaiwen Zhao",
        "Bharathan Balaji",
        "Stephen Lee"
      ],
      "published": "2025-08-05T14:20:10+00:00",
      "updated": "2025-08-05T14:20:10+00:00",
      "arxiv_id": "2508.03489v1",
      "url": "http://arxiv.org/pdf/2508.03489v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "VQA support to Arabic Language Learning Educational Tool",
      "abstract": "We address the problem of scarcity of educational Arabic Language Learning\ntools that advocate modern pedagogical models such as active learning which\nensures language proficiency. In fact, we investigate the design and evaluation\nof an AI-powered educational tool designed to enhance Arabic language learning\nfor non-native speakers with beginner-to-intermediate proficiency level. The\ntool leverages advanced AI models to generate interactive visual quizzes,\ndeploying Visual Question Answering as the primary activity. Adopting a\nconstructivist learning approach, the system encourages active learning through\nreal-life visual quizzes, and image-based questions that focus on improving\nvocabulary, grammar, and comprehension. The system integrates Vision-Language\nPretraining models to generate contextually relevant image description from\nwhich Large Language Model generate assignments based on customized Arabic\nlanguage Learning quizzes thanks to prompting.\n  The effectiveness of the tool is evaluated through a manual annotated\nbenchmark consisting of 1266 real-life visual quizzes, with human participants\nproviding feedback. The results show a suitable accuracy rates, validating the\ntool's potential to bridge the gap in Arabic language education and\nhighlighting the tool's promise as a reliable, AI-powered resource for Arabic\nlearners, offering personalized and interactive learning experiences.",
      "authors": [
        "Khaled Bachir Delassi",
        "Lakhdar Zeggane",
        "Hadda Cherroun",
        "Abdelhamid Haouhat",
        "Kaoutar Bouzouad"
      ],
      "published": "2025-08-05T14:18:25+00:00",
      "updated": "2025-08-05T14:18:25+00:00",
      "arxiv_id": "2508.03488v1",
      "url": "http://arxiv.org/pdf/2508.03488v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "BitsAI-Fix: LLM-Driven Approach for Automated Lint Error Resolution in Practice",
      "abstract": "As enterprise codebases continue to grow in scale and complexity, the volume\nof lint errors far exceeds engineers' manual remediation capacity, leading to\ncontinuous accumulation of technical debt and hindered development efficiency.\nThis paper presents BitsAI-Fix, an automated lint error remediation workflow\nbased on Large Language Models (LLMs), designed to address this critical\nchallenge in industrial-scale environments. BitsAI-Fix employs tree-sitter for\ncontext expansion and generates search-and-replace format patches through\nspecially trained LLMs, followed by lint scan re-verification to output final\nremediation results. Additionally, our approach introduces an innovative\nprogressive reinforcement learning (RL) training strategy that can\nautomatically acquire verifiable training data during the project cold-start\nphase and continuously iterate the model by collecting online samples through\nfeedback after system deployment. Furthermore, we designed a targeted\nrule-based reward mechanism that combines format rewards and correctness\nrewards while penalizing redundant modifications. We also propose a \"code diff\nmatching\" methodology to continuously track online effectiveness. In production\ndeployment at ByteDance, our solution has supported over 5,000 engineers,\nresolved more than 12,000 static analysis issues, achieved approximately 85%\nremediation accuracy, with around 1,000 weekly active adopters. This work\ndemonstrates the practical feasibility of LLM-based code remediation solutions\nin enterprise environments and serves as a reference for automated code fix in\nlarge-scale industrial scenarios.",
      "authors": [
        "Yuanpeng Li",
        "Qi Long",
        "Zhiyuan Yao",
        "Jian Xu",
        "Lintao Xie",
        "Xu He",
        "Lu Geng",
        "Xin Han",
        "Yueyan Chen",
        "Wenbo Duan"
      ],
      "published": "2025-08-05T14:17:30+00:00",
      "updated": "2025-08-05T14:17:30+00:00",
      "arxiv_id": "2508.03487v1",
      "url": "http://arxiv.org/pdf/2508.03487v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes",
      "abstract": "As smart homes become increasingly prevalent, intelligent models are widely\nused for tasks such as anomaly detection and behavior prediction. These models\nare typically trained on static datasets, making them brittle to behavioral\ndrift caused by seasonal changes, lifestyle shifts, or evolving routines.\nHowever, collecting new behavior data for retraining is often impractical due\nto its slow pace, high cost, and privacy concerns. In this paper, we propose\nSmartGen, an LLM-based framework that synthesizes context-aware user behavior\ndata to support continual adaptation of downstream smart home models. SmartGen\nconsists of four key components. First, we design a Time and Semantic-aware\nSplit module to divide long behavior sequences into manageable, semantically\ncoherent subsequences under dual time-span constraints. Second, we propose\nSemantic-aware Sequence Compression to reduce input length while preserving\nrepresentative semantics by clustering behavior mapping in latent space. Third,\nwe introduce Graph-guided Sequence Synthesis, which constructs a behavior\nrelationship graph and encodes frequent transitions into prompts, guiding the\nLLM to generate data aligned with contextual changes while retaining core\nbehavior patterns. Finally, we design a Two-stage Outlier Filter to identify\nand remove implausible or semantically inconsistent outputs, aiming to improve\nthe factual coherence and behavioral validity of the generated sequences.\nExperiments on three real-world datasets demonstrate that SmartGen\nsignificantly enhances model performance on anomaly detection and behavior\nprediction tasks under behavioral drift, with anomaly detection improving by\n85.43% and behavior prediction by 70.51% on average. The code is available at\nhttps://github.com/horizonsinzqs/SmartGen.",
      "authors": [
        "Zhiyao Xu",
        "Dan Zhao",
        "Qingsong Zou",
        "Qing Li",
        "Yong Jiang",
        "Yuhang Wang",
        "Jingyu Xiao"
      ],
      "published": "2025-08-05T14:16:10+00:00",
      "updated": "2025-08-05T14:16:10+00:00",
      "arxiv_id": "2508.03484v1",
      "url": "http://arxiv.org/pdf/2508.03484v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "On the Evaluation of Large Language Models in Multilingual Vulnerability Repair",
      "abstract": "Various Deep Learning-based approaches with pre-trained language models have\nbeen proposed for automatically repairing software vulnerabilities. However,\nthese approaches are limited to a specific programming language (C/C++). Recent\nadvances in large language models (LLMs) offer language-agnostic capabilities\nand strong semantic understanding, exhibiting potential to overcome\nmultilingual vulnerability limitations. Although some work has begun to explore\nLLMs' repair performance, their effectiveness is unsatisfactory. To address\nthese limitations, we conducted a large-scale empirical study to investigate\nthe performance of automated vulnerability repair approaches and\nstate-of-the-art LLMs across seven programming languages. Results show GPT-4o,\ninstruction-tuned with few-shot prompting, performs competitively against the\nleading approach, VulMaster. Additionally, the LLM-based approach shows\nsuperior performance in repairing unique vulnerabilities and is more likely to\nrepair the most dangerous vulnerabilities. Instruction-tuned GPT-4o\ndemonstrates strong generalization on vulnerabilities in previously unseen\nlanguage, outperforming existing approaches. Analysis shows Go consistently\nachieves the highest effectiveness across all model types, while C/C++ performs\nthe worst. Based on findings, we discuss the promise of LLM on multilingual\nvulnerability repair and the reasons behind LLM's failed cases. This work takes\nthe first look at repair approaches and LLMs across multiple languages,\nhighlighting the promising future of adopting LLMs for multilingual\nvulnerability repair.",
      "authors": [
        "Dong wang",
        "Junji Yu",
        "Honglin Shu",
        "Michael Fu",
        "Chakkrit Tantithamthavorn",
        "Yasutaka Kamei",
        "Junjie Chen"
      ],
      "published": "2025-08-05T14:05:32+00:00",
      "updated": "2025-08-05T14:05:32+00:00",
      "arxiv_id": "2508.03470v1",
      "url": "http://arxiv.org/pdf/2508.03470v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Learning to Incentivize: LLM-Empowered Contract for AIGC Offloading in Teleoperation",
      "abstract": "With the rapid growth in demand for AI-generated content (AIGC), edge AIGC\nservice providers (ASPs) have become indispensable. However, designing\nincentive mechanisms that motivate ASPs to deliver high-quality AIGC services\nremains a challenge, especially in the presence of information asymmetry. In\nthis paper, we address bonus design between a teleoperator and an edge ASP when\nthe teleoperator cannot observe the ASP's private settings and chosen actions\n(diffusion steps). We formulate this as an online learning contract design\nproblem and decompose it into two subproblems: ASP's settings inference and\ncontract derivation. To tackle the NP-hard setting-inference subproblem with\nunknown variable sizes, we introduce a large language model (LLM)-empowered\nframework that iteratively refines a naive seed solver using the LLM's domain\nexpertise. Upon obtaining the solution from the LLM-evolved solver, we directly\naddress the contract derivation problem using convex optimization techniques\nand obtain a near-optimal contract. Simulation results on our Unity-based\nteleoperation platform show that our method boosts the teleoperator's utility\nby $5 \\sim 40\\%$ compared to benchmarks, while preserving positive incentives\nfor the ASP. The code is available at\nhttps://github.com/Zijun0819/llm4contract.",
      "authors": [
        "Zijun Zhan",
        "Yaxian Dong",
        "Daniel Mawunyo Doe",
        "Yuqing Hu",
        "Shuai Li",
        "Shaohua Cao",
        "Zhu Han"
      ],
      "published": "2025-08-05T14:02:37+00:00",
      "updated": "2025-08-05T14:02:37+00:00",
      "arxiv_id": "2508.03464v1",
      "url": "http://arxiv.org/pdf/2508.03464v1",
      "categories": [
        "cs.CE"
      ],
      "primary_category": "cs.CE"
    },
    {
      "title": "Neighborhood-Preserving Voronoi Treemaps",
      "abstract": "Voronoi treemaps are used to depict nodes and their hierarchical\nrelationships simultaneously. However, in addition to the hierarchical\nstructure, data attributes, such as co-occurring features or similarities,\nfrequently exist. Examples include geographical attributes like shared borders\nbetween countries or contextualized semantic information such as embedding\nvectors derived from large language models. In this work, we introduce a\nVoronoi treemap algorithm that leverages data similarity to generate\nneighborhood-preserving treemaps. First, we extend the treemap layout pipeline\nto consider similarity during data preprocessing. We then use a Kuhn-Munkres\nmatching of similarities to centroidal Voronoi tessellation (CVT) cells to\ncreate initial Voronoi diagrams with equal cell sizes for each level. Greedy\nswapping is used to improve the neighborhoods of cells to match the data's\nsimilarity further. During optimization, cell areas are iteratively adjusted to\ntheir respective sizes while preserving the existing neighborhoods. We\ndemonstrate the practicality of our approach through multiple real-world\nexamples drawn from infographics and linguistics. To quantitatively assess the\nresulting treemaps, we employ treemap metrics and measure neighborhood\npreservation.",
      "authors": [
        "Patrick Paetzold",
        "Rebecca Kehlbeck",
        "Yumeng Xue",
        "Bin Chen",
        "Yunhai Wang",
        "Oliver Deussen"
      ],
      "published": "2025-08-05T13:41:55+00:00",
      "updated": "2025-08-06T11:28:05+00:00",
      "arxiv_id": "2508.03445v2",
      "url": "http://arxiv.org/pdf/2508.03445v2",
      "categories": [
        "cs.GR"
      ],
      "primary_category": "cs.GR"
    },
    {
      "title": "An Auditable Agent Platform For Automated Molecular Optimisation",
      "abstract": "Drug discovery frequently loses momentum when data, expertise, and tools are\nscattered, slowing design cycles. To shorten this loop we built a hierarchical,\ntool using agent framework that automates molecular optimisation. A Principal\nResearcher defines each objective, a Database agent retrieves target\ninformation, an AI Expert generates de novo scaffolds with a sequence to\nmolecule deep learning model, a Medicinal Chemist edits them while invoking a\ndocking tool, a Ranking agent scores the candidates, and a Scientific Critic\npolices the logic. Each tool call is summarised and stored causing the full\nreasoning path to remain inspectable. The agents communicate through concise\nprovenance records that capture molecular lineage, to build auditable, molecule\ncentered reasoning trajectories and reuse successful transformations via in\ncontext learning. Three cycle research loops were run against AKT1 protein\nusing five large language models. After ranking the models by mean docking\nscore, we ran 20 independent scale ups on the two top performers. We then\ncompared the leading LLMs' binding affinity results across three\nconfigurations, LLM only, single agent, and multi agent. Our results reveal an\narchitectural trade off, the multi agent setting excelled at focused binding\noptimization, improving average predicted binding affinity by 31%. In contrast,\nsingle agent runs generated molecules with superior drug like properties at the\ncost of less potent binding scores. Unguided LLM runs finished fastest, yet\ntheir lack of transparent tool signals left the validity of their reasoning\npaths unverified. These results show that test time scaling, focused feedback\nloops and provenance convert general purpose LLMs into auditable systems for\nmolecular design, and suggest that extending the toolset to ADMET and\nselectivity predictors could push research workflows further along the\ndiscovery pipeline.",
      "authors": [
        "Atabey ÃnlÃ¼",
        "Phil Rohr",
        "Ahmet Celebi"
      ],
      "published": "2025-08-05T13:41:32+00:00",
      "updated": "2025-08-05T13:41:32+00:00",
      "arxiv_id": "2508.03444v1",
      "url": "http://arxiv.org/pdf/2508.03444v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "LLMs are Single-threaded Reasoners: Demystifying the Working Mechanism of Soft Thinking",
      "abstract": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks.",
      "authors": [
        "ChÃ¼nhung Wu",
        "Jinliang Lu",
        "Zixuan Ren",
        "Gangqiang Hu",
        "Zhi Wu",
        "Dai Dai",
        "Hua Wu"
      ],
      "published": "2025-08-05T13:38:33+00:00",
      "updated": "2025-08-07T06:38:21+00:00",
      "arxiv_id": "2508.03440v3",
      "url": "http://arxiv.org/pdf/2508.03440v3",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Data Overdose? Time for a Quadruple Shot: Knowledge Graph Construction using Enhanced Triple Extraction",
      "abstract": "The rapid expansion of publicly-available medical data presents a challenge\nfor clinicians and researchers alike, increasing the gap between the volume of\nscientific literature and its applications. The steady growth of studies and\nfindings overwhelms medical professionals at large, hindering their ability to\nsystematically review and understand the latest knowledge. This paper presents\nan approach to information extraction and automatic knowledge graph (KG)\ngeneration to identify and connect biomedical knowledge. Through a pipeline of\nlarge language model (LLM) agents, the system decomposes 44 PubMed abstracts\ninto semantically meaningful proposition sentences and extracts KG triples from\nthese sentences. The triples are enhanced using a combination of open domain\nand ontology-based information extraction methodologies to incorporate\nontological categories. On top of this, a context variable is included during\nextraction to allow the triple to stand on its own - thereby becoming\n`quadruples'. The extraction accuracy of the LLM is validated by comparing\nnatural language sentences generated from the enhanced triples to the original\npropositions, achieving an average cosine similarity of 0.874. The similarity\nfor generated sentences of enhanced triples were compared with generated\nsentences of ordinary triples showing an increase as a result of the context\nvariable. Furthermore, this research explores the ability for LLMs to infer new\nrelationships and connect clusters in the knowledge base of the knowledge\ngraph. This approach leads the way to provide medical practitioners with a\ncentralised, updated in real-time, and sustainable knowledge source, and may be\nthe foundation of similar gains in a wide variety of fields.",
      "authors": [
        "Taine J. Elliott",
        "Stephen P. Levitt",
        "Ken Nixon",
        "Martin Bekker"
      ],
      "published": "2025-08-05T13:30:41+00:00",
      "updated": "2025-08-05T13:30:41+00:00",
      "arxiv_id": "2508.03438v1",
      "url": "http://arxiv.org/pdf/2508.03438v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "AI on the Pulse: Real-Time Health Anomaly Detection with Wearable and Ambient Intelligence",
      "abstract": "We introduce AI on the Pulse, a real-world-ready anomaly detection system\nthat continuously monitors patients using a fusion of wearable sensors, ambient\nintelligence, and advanced AI models. Powered by UniTS, a state-of-the-art\n(SoTA) universal time-series model, our framework autonomously learns each\npatient's unique physiological and behavioral patterns, detecting subtle\ndeviations that signal potential health risks. Unlike classification methods\nthat require impractical, continuous labeling in real-world scenarios, our\napproach uses anomaly detection to provide real-time, personalized alerts for\nreactive home-care interventions. Our approach outperforms 12 SoTA anomaly\ndetection methods, demonstrating robustness across both high-fidelity medical\ndevices (ECG) and consumer wearables, with a ~ 22% improvement in F1 score.\nHowever, the true impact of AI on the Pulse lies in @HOME, where it has been\nsuccessfully deployed for continuous, real-world patient monitoring. By\noperating with non-invasive, lightweight devices like smartwatches, our system\nproves that high-quality health monitoring is possible without clinical-grade\nequipment. Beyond detection, we enhance interpretability by integrating LLMs,\ntranslating anomaly scores into clinically meaningful insights for healthcare\nprofessionals.",
      "authors": [
        "Davide Gabrielli",
        "Bardh Prenkaj",
        "Paola Velardi",
        "Stefano Faralli"
      ],
      "published": "2025-08-05T13:24:15+00:00",
      "updated": "2025-08-05T13:24:15+00:00",
      "arxiv_id": "2508.03436v1",
      "url": "http://arxiv.org/pdf/2508.03436v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation",
      "abstract": "X-ray medical report generation is one of the important applications of\nartificial intelligence in healthcare. With the support of large foundation\nmodels, the quality of medical report generation has significantly improved.\nHowever, challenges such as hallucination and weak disease diagnostic\ncapability still persist. In this paper, we first construct a large-scale\nmulti-modal medical knowledge graph (termed M3KG) based on the ground truth\nmedical report using the GPT-4o. It contains 2477 entities, 3 kinds of\nrelations, 37424 triples, and 6943 disease-aware vision tokens for the CheXpert\nPlus dataset. Then, we sample it to obtain multi-granularity semantic graphs\nand use an R-GCN encoder for feature extraction. For the input X-ray image, we\nadopt the Swin-Transformer to extract the vision features and interact with the\nknowledge using cross-attention. The vision tokens are fed into a Q-former and\nretrieved the disease-aware vision tokens using another cross-attention.\nFinally, we adopt the large language model to map the semantic knowledge graph,\ninput X-ray image, and disease-aware vision tokens into language descriptions.\nExtensive experiments on multiple datasets fully validated the effectiveness of\nour proposed knowledge graph and X-ray report generation framework. The source\ncode of this paper will be released on\nhttps://github.com/Event-AHU/Medical_Image_Analysis.",
      "authors": [
        "Futian Wang",
        "Yuhan Qiao",
        "Xiao Wang",
        "Fuling Wang",
        "Yuxiang Zhang",
        "Dengdi Sun"
      ],
      "published": "2025-08-05T13:13:45+00:00",
      "updated": "2025-08-05T13:13:45+00:00",
      "arxiv_id": "2508.03426v1",
      "url": "http://arxiv.org/pdf/2508.03426v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification",
      "abstract": "Conversational LLMs have been widely adopted by domain users with limited\nprogramming experience to solve domain problems. However, these users often\nface misalignment between their intent and generated code, resulting in\nfrustration and rounds of clarification. This work first investigates the cause\nof this misalignment, which dues to bidirectional ambiguity: both user intents\nand coding tasks are inherently nonlinear, yet must be expressed and\ninterpreted through linear prompts and code sequences. To address this, we\npropose direct intent-task matching, a new human-LLM interaction paradigm that\nexternalizes and enables direct manipulation of the LLM understanding, i.e.,\nthe coding tasks and their relationships inferred by the LLM prior to code\ngeneration. As a proof-of-concept, this paradigm is then implemented in\nNeuroSync, which employs a knowledge distillation pipeline to extract LLM\nunderstanding, user intents, and their mappings, and enhances the alignment by\nallowing users to intuitively inspect and edit them via visualizations. We\nevaluate the algorithmic components of NeuroSync via technical experiments, and\nassess its overall usability and effectiveness via a user study (N=12). The\nresults show that it enhances intent-task alignment, lowers cognitive effort,\nand improves coding efficiency.",
      "authors": [
        "Wenshuo Zhang",
        "Leixian Shen",
        "Shuchang Xu",
        "Jindu Wang",
        "Jian Zhao",
        "Huamin Qu",
        "Linping Yuan"
      ],
      "published": "2025-08-05T12:54:13+00:00",
      "updated": "2025-08-05T12:54:13+00:00",
      "arxiv_id": "2508.02823v1",
      "url": "http://arxiv.org/pdf/2508.02823v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.SE"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Multi-Objective Infeasibility Diagnosis for Routing Problems Using Large Language Models",
      "abstract": "In real-world routing problems, users often propose conflicting or\nunreasonable requirements, which result in infeasible optimization models due\nto overly restrictive or contradictory constraints, leading to an empty\nfeasible solution set. Existing Large Language Model (LLM)-based methods\nattempt to diagnose infeasible models, but modifying such models often involves\nmultiple potential adjustments that these methods do not consider. To fill this\ngap, we introduce Multi-Objective Infeasibility Diagnosis (MOID), which\ncombines LLM agents and multi-objective optimization within an automatic\nrouting solver, to provide a set of representative actionable suggestions.\nSpecifically, MOID employs multi-objective optimization to consider both path\ncost and constraint violation, generating a set of trade-off solutions, each\nencompassing varying degrees of model adjustments. To extract practical\ninsights from these solutions, MOID utilizes LLM agents to generate a solution\nanalysis function for the infeasible model. This function analyzes these\ndistinct solutions to diagnose the original infeasible model, providing users\nwith diverse diagnostic insights and suggestions. Finally, we compare MOID with\nseveral LLM-based methods on 50 types of infeasible routing problems. The\nresults indicate that MOID automatically generates multiple diagnostic\nsuggestions in a single run, providing more practical insights for restoring\nmodel feasibility and decision-making compared to existing methods.",
      "authors": [
        "Kai Li",
        "Ruihao Zheng",
        "Xinye Hao",
        "Zhenkun Wang"
      ],
      "published": "2025-08-05T12:53:20+00:00",
      "updated": "2025-08-05T12:53:20+00:00",
      "arxiv_id": "2508.03406v1",
      "url": "http://arxiv.org/pdf/2508.03406v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis",
      "abstract": "Large Language Models (LLMs) excel in reasoning and generation across\ndomains, but still struggle with identifying and diagnosing complex errors.\nThis stems mainly from training objectives that prioritize correct answers,\nlimiting exposure to and learning from errors. While recent studies have begun\nto address this by introducing error signals, most rely on shallow, static\nerrors, restricting improvement in deep diagnostic ability. To overcome this,\nwe propose Hide and Seek Game (HSG), a dynamic adversarial framework for error\ngeneration and diagnosis, and evaluate it on mathematical problem-solving. HSG\ninvolves two adversarial roles: Sneaky, which \"hides\" by generating subtle,\ndeceptive reasoning errors, and Diagnosis, which \"seeks\" to accurately detect\nthem. Through adversarial co-evolution, both error stealth and diagnostic\nprecision are enhanced. Experiments on several math reasoning tasks show that\nHSG significantly boosts error diagnosis, achieving 16.8\\%--31.4\\% higher\naccuracy than baselines like GPT-4o. We also release a challenging dataset of\ndeceptive errors and diagnostic annotations as a benchmark for future research.",
      "authors": [
        "Rui Zou",
        "Mengqi Wei",
        "Yutao Zhu",
        "Jirong Wen",
        "Xin Zhao",
        "Jing Chen"
      ],
      "published": "2025-08-05T12:45:21+00:00",
      "updated": "2025-08-05T12:45:21+00:00",
      "arxiv_id": "2508.03396v1",
      "url": "http://arxiv.org/pdf/2508.03396v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Can We Fix Social Media? Testing Prosocial Interventions using Generative Social Simulation",
      "abstract": "Social media platforms have been widely linked to societal harms, including\nrising polarization and the erosion of constructive debate. Can these problems\nbe mitigated through prosocial interventions? We address this question using a\nnovel method - generative social simulation - that embeds Large Language Models\nwithin Agent-Based Models to create socially rich synthetic platforms. We\ncreate a minimal platform where agents can post, repost, and follow others. We\nfind that the resulting following-networks reproduce three well-documented\ndysfunctions: (1) partisan echo chambers; (2) concentrated influence among a\nsmall elite; and (3) the amplification of polarized voices - creating a 'social\nmedia prism' that distorts political discourse. We test six proposed\ninterventions, from chronological feeds to bridging algorithms, finding only\nmodest improvements - and in some cases, worsened outcomes. These results\nsuggest that core dysfunctions may be rooted in the feedback between reactive\nengagement and network growth, raising the possibility that meaningful reform\nwill require rethinking the foundational dynamics of platform architecture.",
      "authors": [
        "Maik Larooij",
        "Petter TÃ¶rnberg"
      ],
      "published": "2025-08-05T12:31:52+00:00",
      "updated": "2025-08-05T12:31:52+00:00",
      "arxiv_id": "2508.03385v1",
      "url": "http://arxiv.org/pdf/2508.03385v1",
      "categories": [
        "cs.SI",
        "cs.CY"
      ],
      "primary_category": "cs.SI"
    },
    {
      "title": "Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams",
      "abstract": "Large language models (LLMs) excel at generating code from natural language\n(NL) descriptions. However, the plain textual descriptions are inherently\nambiguous and often fail to capture complex requirements like intricate system\nbehaviors, conditional logic, and architectural constraints; implicit data\ndependencies in service-oriented architectures are difficult to infer and\nhandle correctly. To bridge this gap, we propose a novel step-by-step code\ngeneration framework named UML2Dep by leveraging unambiguous formal\nspecifications of complex requirements. First, we introduce an enhanced Unified\nModeling Language (UML) sequence diagram tailored for service-oriented\narchitectures. This diagram extends traditional visual syntax by integrating\ndecision tables and API specifications, explicitly formalizing structural\nrelationships and business logic flows in service interactions to rigorously\neliminate linguistic ambiguity. Second, recognizing the critical role of data\nflow, we introduce a dedicated data dependency inference (DDI) task. DDI\nsystematically constructs an explicit data dependency graph prior to actual\ncode synthesis. To ensure reliability, we formalize DDI as a constrained\nmathematical reasoning task through novel prompting strategies, aligning with\nLLMs' excellent mathematical strengths. Additional static parsing and\ndependency pruning further reduce context complexity and cognitive load\nassociated with intricate specifications, thereby enhancing reasoning accuracy\nand efficiency.",
      "authors": [
        "Wenxin Mao",
        "Zhitao Wang",
        "Long Wang",
        "Sirong Chen",
        "Cuiyun Gao",
        "Luyang Cao",
        "Ziming Liu",
        "Qiming Zhang",
        "Jun Zhou",
        "Zhi Jin"
      ],
      "published": "2025-08-05T12:28:23+00:00",
      "updated": "2025-08-06T02:58:28+00:00",
      "arxiv_id": "2508.03379v2",
      "url": "http://arxiv.org/pdf/2508.03379v2",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "A Closed-Loop Multi-Agent Framework for Aerodynamics-Aware Automotive Styling Design",
      "abstract": "The core challenge in automotive exterior design is balancing subjective\naesthetics with objective aerodynamic performance while dramatically\naccelerating the development cycle. To address this, we propose a novel,\nLLM-driven multi-agent framework that automates the end-to-end workflow from\nambiguous requirements to 3D concept model performance validation. The workflow\nis structured in two stages: conceptual generation and performance validation.\nIn the first stage, agents collaborate to interpret fuzzy design requirements,\ngenerate concept sketches, and produce photorealistic renderings using\ndiffusion models. In the second stage, the renderings are converted to 3D point\nclouds, where a Drag Prediction Agent, built upon a lightweight surrogate\nmodel, provides near-instantaneous predictions of the drag coefficient and\npressure fields, replacing time-consuming CFD simulations. The primary\ncontribution of this work is the seamless integration of creative generation\nwith a rapid engineering validation loop within a unified, automated system,\nwhich provides a new paradigm for efficiently balancing creative exploration\nwith engineering constraints in the earliest stages of design.",
      "authors": [
        "Xinyu Jin",
        "Shengmao Yan",
        "Qingtao Wang",
        "Shisong Deng",
        "Yanzhen Jiang",
        "Shuangyao Zhao"
      ],
      "published": "2025-08-05T12:21:13+00:00",
      "updated": "2025-08-05T12:21:13+00:00",
      "arxiv_id": "2508.03370v1",
      "url": "http://arxiv.org/pdf/2508.03370v1",
      "categories": [
        "cs.CE",
        "cs.MA"
      ],
      "primary_category": "cs.CE"
    },
    {
      "title": "Board Game Arena: A Framework and Benchmark for Assessing Large Language Models via Strategic Play",
      "abstract": "The Board Game Arena library provides a framework for evaluating the decision\nmaking abilities of large language models (LLMs) through strategic board games\nimplemented in Google OpenSpiel library. The framework enables systematic\ncomparisons between LLM based agents and other agents (random, human,\nreinforcement learning agents, etc.) in various game scenarios by wrapping\nmultiple board and matrix games and supporting different agent types. It\nintegrates API access to models via LiteLLM, local model deployment via vLLM,\nand offers distributed execution through Ray. Additionally it provides\nextensive analysis tools for the LLM reasoning traces. This paper summarizes\nthe structure, key characteristics, and motivation of the repository,\nhighlighting how it contributes to the empirical evaluation of the reasoning of\nLLM and game-theoretic behavior",
      "authors": [
        "Lucia Cipolina-Kun",
        "Marianna Nezhurina",
        "Jenia Jitsev"
      ],
      "published": "2025-08-05T12:15:59+00:00",
      "updated": "2025-08-05T12:15:59+00:00",
      "arxiv_id": "2508.03368v1",
      "url": "http://arxiv.org/pdf/2508.03368v1",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning",
      "abstract": "General logical reasoning, defined as the ability to reason deductively on\ndomain-agnostic tasks, continues to be a challenge for large language models\n(LLMs). Current LLMs fail to reason deterministically and are not\ninterpretable. As such, there has been a recent surge in interest in\nneurosymbolic AI, which attempts to incorporate logic into neural networks. We\nfirst identify two main neurosymbolic approaches to improving logical\nreasoning: (i) the integrative approach comprising models where symbolic\nreasoning is contained within the neural network, and (ii) the hybrid approach\ncomprising models where a symbolic solver, separate from the neural network,\nperforms symbolic reasoning. Both contain AI systems with promising results on\ndomain-specific logical reasoning benchmarks. However, their performance on\ndomain-agnostic benchmarks is understudied. To the best of our knowledge, there\nhas not been a comparison of the contrasting approaches that answers the\nfollowing question: Which approach is more promising for developing general\nlogical reasoning? To analyze their potential, the following best-in-class\ndomain-agnostic models are introduced: Logic Neural Network (LNN), which uses\nthe integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the\nhybrid approach. Using both models as case studies and representatives of each\napproach, our analysis demonstrates that the hybrid approach is more promising\nfor developing general logical reasoning because (i) its reasoning chain is\nmore interpretable, and (ii) it retains the capabilities and advantages of\nexisting LLMs. To support future works using the hybrid approach, we propose a\ngeneralizable framework based on LLM-SS that is modular by design,\nmodel-agnostic, domain-agnostic, and requires little to no human input.",
      "authors": [
        "Michael K. Chen"
      ],
      "published": "2025-08-05T12:14:32+00:00",
      "updated": "2025-08-05T12:14:32+00:00",
      "arxiv_id": "2508.03366v1",
      "url": "http://arxiv.org/pdf/2508.03366v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SC"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "When Good Sounds Go Adversarial: Jailbreaking Audio-Language Models with Benign Inputs",
      "abstract": "As large language models become increasingly integrated into daily life,\naudio has emerged as a key interface for human-AI interaction. However, this\nconvenience also introduces new vulnerabilities, making audio a potential\nattack surface for adversaries. Our research introduces WhisperInject, a\ntwo-stage adversarial audio attack framework that can manipulate\nstate-of-the-art audio language models to generate harmful content. Our method\nuses imperceptible perturbations in audio inputs that remain benign to human\nlisteners. The first stage uses a novel reward-based optimization method,\nReinforcement Learning with Projected Gradient Descent (RL-PGD), to guide the\ntarget model to circumvent its own safety protocols and generate harmful native\nresponses. This native harmful response then serves as the target for Stage 2,\nPayload Injection, where we use Projected Gradient Descent (PGD) to optimize\nsubtle perturbations that are embedded into benign audio carriers, such as\nweather queries or greeting messages. Validated under the rigorous\nStrongREJECT, LlamaGuard, as well as Human Evaluation safety evaluation\nframework, our experiments demonstrate a success rate exceeding 86% across\nQwen2.5-Omni-3B, Qwen2.5-Omni-7B, and Phi-4-Multimodal. Our work demonstrates a\nnew class of practical, audio-native threats, moving beyond theoretical\nexploits to reveal a feasible and covert method for manipulating AI behavior.",
      "authors": [
        "Bodam Kim",
        "Hiskias Dingeto",
        "Taeyoun Kwon",
        "Dasol Choi",
        "DongGeon Lee",
        "Haon Park",
        "JaeHoon Lee",
        "Jongho Shin"
      ],
      "published": "2025-08-05T12:14:01+00:00",
      "updated": "2025-08-05T12:14:01+00:00",
      "arxiv_id": "2508.03365v1",
      "url": "http://arxiv.org/pdf/2508.03365v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "eess.AS"
      ],
      "primary_category": "cs.SD"
    },
    {
      "title": "Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models",
      "abstract": "Reasoning large language models (RLLMs) have recently demonstrated remarkable\ncapabilities through structured and multi-step reasoning. While prior research\nhas primarily focused on improving their training and inference strategies,\ntheir potential for in-context learning (ICL) remains largely underexplored. To\nfill this gap, we propose Thinking with Nothinking Calibration (JointThinking),\na new ICL paradigm that leverages the structured difference between two\nreasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.\nSpecifically, our method prompts the model to generate two answers in parallel:\none in Thinking mode and the other in Nothinking mode. A second round of\nThinking is triggered only when the two initial responses are inconsistent,\nusing a single prompt that incorporates the original question and both\ncandidate answers. Since such disagreement occurs infrequently (e.g., only 6\\%\nin GSM8K), our method performs just one round of reasoning in most cases,\nresulting in minimal latency overhead. Extensive experiments across multiple\nreasoning benchmarks demonstrate that JointThinking significantly outperforms\nfew-shot chain-of-thought (CoT) and majority voting with improved answer\nrobustness. Moreover, It achieves comparable in-distribution performance to\ntraining-based SOTA method, while substantially outperforming on\nout-of-distribution tasks. We further conduct a systematic analysis of the\ncalibration mechanism, showing that leveraging different reasoning modes\nconsistently lowers the error rate and highlights the value of structural\nthinking diversity. Additionally, we observe that the performance gap between\nactual and ideal reasoning narrows as model size increases in the second round\nof thinking, indicating the strong scalability of our approach. Finally, we\ndiscuss current limitations and outline promising directions for future ICL\nresearch in RLLMs.",
      "authors": [
        "Haotian Wu",
        "Bo Xu",
        "Yao Shu",
        "Menglin Yang",
        "Chengwei Qin"
      ],
      "published": "2025-08-05T12:09:55+00:00",
      "updated": "2025-08-06T03:38:01+00:00",
      "arxiv_id": "2508.03363v2",
      "url": "http://arxiv.org/pdf/2508.03363v2",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment",
      "abstract": "Automatic assessment of cognitive impairment from spontaneous speech offers a\npromising, non-invasive avenue for early cognitive screening. However, current\napproaches often lack generalizability when deployed across different languages\nand clinical settings, limiting their practical utility. In this study, we\npropose CogBench, the first benchmark designed to evaluate the cross-lingual\nand cross-site generalizability of large language models (LLMs) for\nspeech-based cognitive impairment assessment. Using a unified multimodal\npipeline, we evaluate model performance on three speech datasets spanning\nEnglish and Mandarin: ADReSSo, NCMMSC2021-AD, and a newly collected test set,\nCIR-E. Our results show that conventional deep learning models degrade\nsubstantially when transferred across domains. In contrast, LLMs equipped with\nchain-of-thought prompting demonstrate better adaptability, though their\nperformance remains sensitive to prompt design. Furthermore, we explore\nlightweight fine-tuning of LLMs via Low-Rank Adaptation (LoRA), which\nsignificantly improves generalization in target domains. These findings offer a\ncritical step toward building clinically useful and linguistically robust\nspeech-based cognitive assessment tools.",
      "authors": [
        "Feng Rui",
        "Zhiyao Luo",
        "Wei Wang",
        "Yuting Song",
        "Yong Liu",
        "Tingting Zhu",
        "Jianqing Li",
        "Xingyao Wang"
      ],
      "published": "2025-08-05T12:06:16+00:00",
      "updated": "2025-08-05T12:06:16+00:00",
      "arxiv_id": "2508.03360v1",
      "url": "http://arxiv.org/pdf/2508.03360v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature",
      "abstract": "Automatically identifying characters and their interactions from fiction\nbooks is, arguably, a complex task that requires pipelines that leverage\nmultiple Natural Language Processing (NLP) methods, such as Named Entity\nRecognition (NER) and Part-of-speech (POS) tagging. However, these methods are\nnot optimized for the task that leads to the construction of Social Networks of\nCharacters. Indeed, the currently available methods tend to underperform,\nespecially in less-represented languages, due to a lack of manually annotated\ndata for training. Here, we propose a pipeline, which we call Taggus, to\nextract social networks from literary fiction works in Portuguese. Our results\nshow that compared to readily available State-of-the-Art tools -- off-the-shelf\nNER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which\nuses POS tagging and a combination of heuristics, achieves satisfying results\nwith an average F1-Score of $94.1\\%$ in the task of identifying characters and\nsolving for co-reference and $75.9\\%$ in interaction detection. These\nrepresent, respectively, an increase of $50.7\\%$ and $22.3\\%$ on results\nachieved by the readily available State-of-the-Art tools. Further steps to\nimprove results are outlined, such as solutions for detecting relationships\nbetween characters. Limitations on the size and scope of our testing samples\nare acknowledged. The Taggus pipeline is publicly available to encourage\ndevelopment in this field for the Portuguese language.2",
      "authors": [
        "Tiago G CanÃ¡rio",
        "Catarina Duarte",
        "FlÃ¡vio L. Pinheiro",
        "JoÃ£o L. M. Pereira"
      ],
      "published": "2025-08-05T12:03:03+00:00",
      "updated": "2025-08-05T12:03:03+00:00",
      "arxiv_id": "2508.03358v1",
      "url": "http://arxiv.org/pdf/2508.03358v1",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation",
      "abstract": "Post-training quantization (PTQ) has emerged as an effective approach for\ncompressing large models and accelerating their inference without retraining.\nWhile PTQ has been extensively studied in the context of large language models\n(LLMs), its applicability to vision-language models (VLMs) remains\nunderexplored. In this paper, we identify a modality discrepancy (\\emph{i.e.},\nlimited text tokens \\emph{vs.} excessive and redundant vision tokens) of VLMs.\nHowever, existing Hessian-based LLM PTQ methods treat all tokens equally during\nquantization, resulting in severe performance drops when applied to VLMs.\nMotivated by this observation, we propose a novel importance-aware PTQ\nframework tailored for VLMs, dubbed VLMQ. Specifically, to address vision token\nredundancy, VLMQ 1) optimizes an importance-aware objective that yields an\nenhanced Hessian with token-level importance factors, while retaining\ncompatibility with parallelized weight updates, and 2) ensures efficiency and\neffectiveness by computing these factors via a single lightweight block-wise\nbackward pass, guided by a theoretical connection to token-level perturbations.\nExtensive evaluations on 8 benchmarks across 0.5B$\\sim$32B VLMs demonstrate the\nstate-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit\nsettings. For example, it achieves a substantial \\textbf{16.45\\%} improvement\non MME-RealWorld under 2-bit quantization.",
      "authors": [
        "Yufei Xue",
        "Yushi Huang",
        "Jiawei Shao",
        "Jun Zhang"
      ],
      "published": "2025-08-05T11:57:03+00:00",
      "updated": "2025-08-05T11:57:03+00:00",
      "arxiv_id": "2508.03351v1",
      "url": "http://arxiv.org/pdf/2508.03351v1",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Compressing Chain-of-Thought in LLMs via Step Entropy",
      "abstract": "Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at\ncomplex reasoning but generate verbose thought processes with considerable\nredundancy, leading to increased inference costs and reduced efficiency. We\nintroduce a novel CoT compression framework based on step entropy, a metric\nthat quantifies the informational contribution of individual reasoning steps to\nidentify redundancy. Through theoretical analysis and extensive empirical\nvalidation on mathematical reasoning benchmarks, we demonstrate that steps with\nlow entropy are indeed highly redundant. Our experiments reveal that an\nastonishing 80\\% of low-entropy intermediate steps can be pruned with minor\ndegradation in the final answer accuracy across DeepSeek-R1-7B, 14B and\nQwen3-8B. This finding sharply contrasts with random or high-entropy pruning,\nwhich severely impairs reasoning performance. Building on this, we propose a\nnovel two-stage training strategy combining Supervised Fine-Tuning (SFT) and\nGroup Relative Policy Optimization (GRPO) reinforcement learning. This approach\nenables LLMs to autonomously learn to generate compressed COTs during inference\nby strategically incorporating [SKIP] tokens. Our method significantly enhances\nLLM inference efficiency while rigorously preserving accuracy, offering\nprofound implications for practical LLM deployment and a deeper understanding\nof reasoning structures.",
      "authors": [
        "Zeju Li",
        "Jianyuan Zhong",
        "Ziyang Zheng",
        "Xiangyu Wen",
        "Zhijian Xu",
        "Yingying Cheng",
        "Fan Zhang",
        "Qiang Xu"
      ],
      "published": "2025-08-05T11:48:18+00:00",
      "updated": "2025-08-05T11:48:18+00:00",
      "arxiv_id": "2508.03346v1",
      "url": "http://arxiv.org/pdf/2508.03346v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Adaptive AI Agent Placement and Migration in Edge Intelligence Systems",
      "abstract": "The rise of LLMs such as ChatGPT and Claude fuels the need for AI agents\ncapable of real-time task handling. However, migrating data-intensive,\nmulti-modal edge workloads to cloud data centers, traditionally used for agent\ndeployment, introduces significant latency. Deploying AI agents at the edge\nimproves efficiency and reduces latency. However, edge environments present\nchallenges due to limited and heterogeneous resources. Maintaining QoS for\nmobile users necessitates agent migration, which is complicated by the\ncomplexity of AI agents coordinating LLMs, task planning, memory, and external\ntools. This paper presents the first systematic deployment and management\nsolution for LLM-based AI agents in dynamic edge environments. We propose a\nnovel adaptive framework for AI agent placement and migration in edge\nintelligence systems. Our approach models resource constraints and\nlatency/cost, leveraging ant colony algorithms and LLM-based optimization for\nefficient decision-making. It autonomously places agents to optimize resource\nutilization and QoS and enables lightweight agent migration by transferring\nonly essential state. Implemented on a distributed system using AgentScope and\nvalidated across globally distributed edge servers, our solution significantly\nreduces deployment latency and migration costs.",
      "authors": [
        "Xingdan Wang",
        "Jiayi He",
        "Zhiqing Tang",
        "Jianxiong Guo",
        "Jiong Lou",
        "Liping Qian",
        "Tian Wang",
        "Weijia Jia"
      ],
      "published": "2025-08-05T11:47:46+00:00",
      "updated": "2025-08-05T11:47:46+00:00",
      "arxiv_id": "2508.03345v1",
      "url": "http://arxiv.org/pdf/2508.03345v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "From Legacy to Standard: LLM-Assisted Transformation of Cybersecurity Playbooks into CACAO Format",
      "abstract": "Existing cybersecurity playbooks are often written in heterogeneous,\nnon-machine-readable formats, which limits their automation and\ninteroperability across Security Orchestration, Automation, and Response\nplatforms. This paper explores the suitability of Large Language Models,\ncombined with Prompt Engineering, to automatically translate legacy incident\nresponse playbooks into the standardized, machine-readable CACAO format. We\nsystematically examine various Prompt Engineering techniques and carefully\ndesign prompts aimed at maximizing syntactic accuracy and semantic fidelity for\ncontrol flow preservation. Our modular transformation pipeline integrates a\nsyntax checker to ensure syntactic correctness and features an iterative\nrefinement mechanism that progressively reduces syntactic errors. We evaluate\nthe proposed approach on a custom-generated dataset comprising diverse legacy\nplaybooks paired with manually created CACAO references. The results\ndemonstrate that our method significantly improves the accuracy of playbook\ntransformation over baseline models, effectively captures complex workflow\nstructures, and substantially reduces errors. It highlights the potential for\npractical deployment in automated cybersecurity playbook transformation tasks.",
      "authors": [
        "Mehdi Akbari Gurabi",
        "Lasse Nitz",
        "Radu-Mihai Castravet",
        "Roman Matzutt",
        "Avikarsha Mandal",
        "Stefan Decker"
      ],
      "published": "2025-08-05T11:43:54+00:00",
      "updated": "2025-08-05T11:43:54+00:00",
      "arxiv_id": "2508.03342v1",
      "url": "http://arxiv.org/pdf/2508.03342v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science",
      "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet their\ninability to maintain persistent memory in long contexts limits their\neffectiveness as autonomous agents in long-term interactions. While existing\nmemory systems have made progress, their reliance on arbitrary granularity for\ndefining the basic memory unit and passive, rule-based mechanisms for knowledge\nextraction limits their capacity for genuine learning and evolution. To address\nthese foundational limitations, we present Nemori, a novel self-organizing\nmemory architecture inspired by human cognitive principles. Nemori's core\ninnovation is twofold: First, its Two-Step Alignment Principle, inspired by\nEvent Segmentation Theory, provides a principled, top-down method for\nautonomously organizing the raw conversational stream into semantically\ncoherent episodes, solving the critical issue of memory granularity. Second,\nits Predict-Calibrate Principle, inspired by the Free-energy Principle, enables\nthe agent to proactively learn from prediction gaps, moving beyond pre-defined\nheuristics to achieve adaptive knowledge evolution. This offers a viable path\ntoward handling the long-term, dynamic workflows of autonomous agents.\nExtensive experiments on the LoCoMo and LongMemEval benchmarks demonstrate that\nNemori significantly outperforms prior state-of-the-art systems, with its\nadvantage being particularly pronounced in longer contexts.",
      "authors": [
        "Jiayan Nan",
        "Wenquan Ma",
        "Wenlong Wu",
        "Yize Chen"
      ],
      "published": "2025-08-05T11:41:13+00:00",
      "updated": "2025-08-07T05:39:56+00:00",
      "arxiv_id": "2508.03341v2",
      "url": "http://arxiv.org/pdf/2508.03341v2",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Key-Augmented Neural Triggers for Knowledge Sharing",
      "abstract": "Repository-level code comprehension and knowledge sharing remain core\nchallenges in software engineering. Large language models (LLMs) have shown\npromise by generating explanations of program structure and logic. However,\nthese approaches still face limitations: First, relevant knowledge is\ndistributed across multiple files within a repository, aka semantic\nfragmentation. Second, retrieval inefficiency and attention saturation degrade\nperformance in RAG pipelines, where long, unaligned contexts overwhelm\nattention. Third, repository specific training data is scarce and often\noutdated. Finally, proprietary LLMs hinder industrial adoption due to privacy\nand deployment constraints. To address these issues, we propose Key-Augmented\nNeural Triggers (KANT), a novel approach that embeds knowledge anchors into\nboth training and inference. Unlike prior methods, KANT enables internal access\nto repository specific knowledge, reducing fragmentation and grounding\ninference in localized context. Moreover, we synthesize specialized data\ndirectly from code. At inference, knowledge anchors replace verbose context,\nreducing token overhead and latency while supporting efficient, on premise\ndeployment. We evaluate KANT via: a qualitative human evaluation of the\nsynthesized dataset's intent coverage and quality across five dimensions;\ncompare against SOTA baselines across five qualitative dimensions and inference\nspeed; and replication across different LLMs to assess generalizability.\nResults show that the synthetic training data aligned with information-seeking\nneeds. KANT achieved over 60% preference from human annotators and a LocalStack\nexpert (preferring 79% of cases). Also, KANT reduced inference latency by up to\n85% across all models. Overall, it is well-suited for scalable, low-latency,\non-premise deployments, providing a strong foundation for code comprehension.",
      "authors": [
        "Alex Wolf",
        "Marco Edoardo Palma",
        "Pooja Rani",
        "Harald C. Gall"
      ],
      "published": "2025-08-05T11:40:56+00:00",
      "updated": "2025-08-05T11:40:56+00:00",
      "arxiv_id": "2508.03340v1",
      "url": "http://arxiv.org/pdf/2508.03340v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration",
      "abstract": "The practical application of Multimodal Large Language Models (MLLMs) to\nVideo Question Answering (Video-QA) is severely hindered by the high token cost\nof processing numerous video frames. While increasing the number of sampled\nframes is a common strategy, we observe a \"less is more\" phenomenon where\nexcessive frames can paradoxically degrade performance due to context dilution.\nConcurrently, state-of-the-art keyframe selection methods, while effective,\nstill yield significant temporal redundancy, which we term 'visual echoes'. To\naddress these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novel\npost-processing method that intelligently prunes the selected keyframes. AFP\nemploys an adaptive hierarchical clustering algorithm on a fused ResNet-50 and\nCLIP feature space to identify and merge these echoes into single\nrepresentatives. To compensate for information loss, we then introduce a\nlightweight, text-based semantic graph that provides critical context with\nminimal token overhead. Conducting extensive experiments on the LongVideoBench\nand VideoMME benchmarks across multiple leading MLLMs, our full approach\ndemonstrates a drastic reduction in required frames by up to 86.9% and total\ninput tokens by up to 83.2%. Crucially, by providing a concise, high-quality\nset of frames, our method not only enhances efficiency but often improves\naccuracy over baselines that use more frames. The code will be released upon\npublication.",
      "authors": [
        "Shaoguang Wang",
        "Jianxiang He",
        "Yijie Xu",
        "Ziyang Chen",
        "Weiyu Guo",
        "Hui Xiong"
      ],
      "published": "2025-08-05T11:31:55+00:00",
      "updated": "2025-08-06T07:41:10+00:00",
      "arxiv_id": "2508.03337v2",
      "url": "http://arxiv.org/pdf/2508.03337v2",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "CTTS: Collective Test-Time Scaling",
      "abstract": "Test-time scaling (TTS) has emerged as a promising research field for\nenhancing the effectiveness of large language models (LLMs) without extra\ntraining. However, most existing approaches, e.g., Best-of-N and\nSelf-Consistency rely on a single agent interacting with a reward model\n(SA-SR), constrained by limited capabilities of a single test-time scaling\n(STTS) paradigm. On the other hand, recent works demonstrate that\ncollective-agent methods can break through the upper bound of single-agent\nsystems by orchestrating diverse models. Thus, in this paper, we take a first\nstep towards exploring Collective Test-Time Scaling (CTTS). Consider the\ndifferent interaction types of single and multiple models, we design three\nprimary paradigms to investigate the optimal paradigm of CTTS: (1) single agent\nto multiple reward models (SA-MR); (2) multiple agents to single reward model\n(MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive\nexperiments demonstrate that MA-MR consistently achieves the best performance.\nBased on this, we propose a novel framework named CTTS-MM that effectively\nleverages both multi-agent and multi-reward-model collaboration for enhanced\ninference. Specifically, for multi-agent collaboration, we propose an Agent\nCollaboration Search (ACS), which searches for the most effective combination\nof LLM agents from a large candidate pool; for multi-reward-model\ncollaboration, we propose Mixture of Reword Models (MoR), which consists of a\ncurated question pool and a Prior Reward model Ensemble Selection (PRES) to\nselect the optimal combinations of reward models via Pair-wise Reward Ranking\n(PRR) metric. Experiments across seven mainstream benchmarks demonstrate that\nthe proposed CTTS-MM consistently obtains superior performance. Code will be\nreleased at https://github.com/magent4aci/CTTS-MM.",
      "authors": [
        "Zhende Song",
        "Shengji Tang",
        "Peng Ye",
        "Jiayuan Fan",
        "Tao Chen"
      ],
      "published": "2025-08-05T11:19:08+00:00",
      "updated": "2025-08-05T11:19:08+00:00",
      "arxiv_id": "2508.03333v1",
      "url": "http://arxiv.org/pdf/2508.03333v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Exploring Layer-wise Information Effectiveness for Post-Training Quantization in Small Language Models",
      "abstract": "Large language models with billions of parameters are often over-provisioned:\nmany layers contribute little unique information yet dominate the memory and\nenergy footprint during inference. We present LieQ, a metric-driven\npost-training quantization framework that addresses the critical challenge of\nmaintaining accuracy in sub-7B models under extreme low-bit compression. Our\nmethod introduces three complementary layer-wise diagnostics-Perplexity Drop,\nRepresentational Compactness, and Top-k Energy Gain -that reveal a canonical\ndivision of labour across layers, enabling automatic bit-width allocation\nwithout gradient updates. Unlike existing approaches that suffer severe\naccuracy degradation at 2-3 bits precision, LieQ achieves state-of-the-art\ncompression-accuracy trade-offs: on Qwen3-4B, it recovers 95.9% of FP16\nbaseline performance at 2.05-bit quantization, outperforming GPTQ by 19.7% and\nAWQ by 18.1% on average across seven zero-shot reasoning tasks. Applied to\nLLaMA3.2-3B, LieQ maintains 98.2% of baseline accuracy at 2.07-bit precision\nwhile enabling 4x memory reduction, establishing new paradigms for deploying\nsmall language models on resource-constrained edge devices.",
      "authors": [
        "He Xiao",
        "Qingyao Yang",
        "Dirui Xie",
        "Wendong Xu",
        "Wenyong Zhou",
        "Haobo Liu",
        "Zhengwu Liu",
        "Ngai Wong"
      ],
      "published": "2025-08-05T11:17:04+00:00",
      "updated": "2025-08-05T11:17:04+00:00",
      "arxiv_id": "2508.03332v1",
      "url": "http://arxiv.org/pdf/2508.03332v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Industrial LLM-based Code Optimization under Regulation: A Mixture-of-Agents Approach",
      "abstract": "Recent advancements in Large Language Models (LLMs) for code optimization\nhave enabled industrial platforms to automate software performance engineering\nat unprecedented scale and speed. Yet, organizations in regulated industries\nface strict constraints on which LLMs they can use - many cannot utilize\ncommercial models due to data privacy regulations and compliance requirements,\ncreating a significant challenge for achieving high-quality code optimization\nwhile maintaining cost-effectiveness. We address this by implementing a\nMixture-of-Agents (MoA) approach that directly synthesizes code from multiple\nspecialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm\n(GA)-based ensemble system and individual LLM optimizers using real-world\nindustrial codebases. Our key contributions include: (1) First MoA application\nto industrial code optimization using real-world codebases; (2) Empirical\nevidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost\nsavings and 28.6% to 32.2% faster optimization times for regulated\nenvironments; (3) Deployment guidelines demonstrating GA's advantage with\ncommercial models while both ensembles outperform individual LLMs; and (4)\nReal-world validation across 50 code snippets and seven LLM combinations,\ngenerating over 8,700 variants, addresses gaps in industrial LLM ensemble\nevaluation. This provides actionable guidance for organizations balancing\nregulatory compliance with optimization performance in production environments.",
      "authors": [
        "Mari Ashiga",
        "Vardan Voskanyan",
        "Fateme Dinmohammadi",
        "Jingzhi Gong",
        "Paul Brookes",
        "Matthew Truscott",
        "Rafail Giavrimis",
        "Mike Basios",
        "Leslie Kanthan",
        "Wei Jie"
      ],
      "published": "2025-08-05T11:15:06+00:00",
      "updated": "2025-08-06T12:41:21+00:00",
      "arxiv_id": "2508.03329v2",
      "url": "http://arxiv.org/pdf/2508.03329v2",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "GUI-ReRank: Enhancing GUI Retrieval with Multi-Modal LLM-based Reranking",
      "abstract": "GUI prototyping is a fundamental component in the development of modern\ninteractive systems, which are now ubiquitous across diverse application\ndomains. GUI prototypes play a critical role in requirements elicitation by\nenabling stakeholders to visualize, assess, and refine system concepts\ncollaboratively. Moreover, prototypes serve as effective tools for early\ntesting, iterative evaluation, and validation of design ideas with both end\nusers and development teams. Despite these advantages, the process of\nconstructing GUI prototypes remains resource-intensive and time-consuming,\nfrequently demanding substantial effort and expertise. Recent research has\nsought to alleviate this burden through NL-based GUI retrieval approaches,\nwhich typically rely on embedding-based retrieval or tailored ranking models\nfor specific GUI repositories. However, these methods often suffer from limited\nretrieval performance and struggle to generalize across arbitrary GUI datasets.\nIn this work, we present GUI-ReRank, a novel framework that integrates rapid\nembedding-based constrained retrieval models with highly effective MLLM-based\nreranking techniques. GUI-ReRank further introduces a fully customizable GUI\nrepository annotation and embedding pipeline, enabling users to effortlessly\nmake their own GUI repositories searchable, which allows for rapid discovery of\nrelevant GUIs for inspiration or seamless integration into customized LLM-based\nRAG workflows. We evaluated our approach on an established NL-based GUI\nretrieval benchmark, demonstrating that GUI-ReRank significantly outperforms\nSOTA tailored LTR models in both retrieval accuracy and generalizability.\nAdditionally, we conducted a comprehensive cost and efficiency analysis of\nemploying MLLMs for reranking, providing valuable insights regarding the\ntrade-offs between retrieval effectiveness and computational resources. Video:\nhttps://youtu.be/_7x9UCh82ug",
      "authors": [
        "Kristian Kolthoff",
        "Felix Kretzer",
        "Christian Bartelt",
        "Alexander Maedche",
        "Simone Paolo Ponzetto"
      ],
      "published": "2025-08-05T10:17:38+00:00",
      "updated": "2025-08-05T10:17:38+00:00",
      "arxiv_id": "2508.03298v1",
      "url": "http://arxiv.org/pdf/2508.03298v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty",
      "abstract": "Estimating the difficulty of exam questions is essential for developing good\nexams, but professors are not always good at this task. We compare various\nLarge Language Model-based methods with three professors in their ability to\nestimate what percentage of students will give correct answers on True/False\nexam questions in the areas of Neural Networks and Machine Learning. Our\nresults show that the professors have limited ability to distinguish between\neasy and difficult questions and that they are outperformed by directly asking\nGemini 2.5 to solve this task. Yet, we obtained even better results using\nuncertainties of the LLMs solving the questions in a supervised learning\nsetting, using only 42 training samples. We conclude that supervised learning\nusing LLM uncertainty can help professors better estimate the difficulty of\nexam questions, improving the quality of assessment.",
      "authors": [
        "Leonidas Zotos",
        "Ivo Pascal de Jong",
        "Matias Valdenegro-Toro",
        "Andreea Ioana Sburlea",
        "Malvina Nissim",
        "Hedderik van Rijn"
      ],
      "published": "2025-08-05T10:12:38+00:00",
      "updated": "2025-08-05T10:12:38+00:00",
      "arxiv_id": "2508.03294v1",
      "url": "http://arxiv.org/pdf/2508.03294v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes",
      "abstract": "As Large Language Models (LLMs) are increasingly used across different\napplications, concerns about their potential to amplify gender biases in\nvarious tasks are rising. Prior research has often probed gender bias using\nexplicit gender cues as counterfactual, or studied them in sentence completion\nand short question answering tasks. These formats might overlook more implicit\nforms of bias embedded in generative behavior of longer content. In this work,\nwe investigate gender bias in LLMs using gender stereotypes studied in\npsychology (e.g., aggressiveness or gossiping) in an open-ended task of\nnarrative generation. We introduce a novel dataset called StereoBias-Stories\ncontaining short stories either unconditioned or conditioned on (one, two, or\nsix) random attributes from 25 psychological stereotypes and three task-related\nstory endings. We analyze how the gender contribution in the overall story\nchanges in response to these attributes and present three key findings: (1)\nWhile models, on average, are highly biased towards male in unconditioned\nprompts, conditioning on attributes independent from gender stereotypes\nmitigates this bias. (2) Combining multiple attributes associated with the same\ngender stereotype intensifies model behavior, with male ones amplifying bias\nand female ones alleviating it. (3) Model biases align with psychological\nground-truth used for categorization, and alignment strength increases with\nmodel size. Together, these insights highlight the importance of\npsychology-grounded evaluation of LLMs.",
      "authors": [
        "Shahed Masoudian",
        "Gustavo Escobedo",
        "Hannah Strauss",
        "Markus Schedl"
      ],
      "published": "2025-08-05T10:10:26+00:00",
      "updated": "2025-08-05T10:10:26+00:00",
      "arxiv_id": "2508.03292v1",
      "url": "http://arxiv.org/pdf/2508.03292v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Do language models accommodate their users? A study of linguistic convergence",
      "abstract": "While large language models (LLMs) are generally considered proficient in\ngenerating language, how similar their language usage is to that of humans\nremains understudied. In this paper, we test whether models exhibit linguistic\nconvergence, a core pragmatic element of human language communication, asking:\ndo models adapt, or converge, to the linguistic patterns of their user? To\nanswer this, we systematically compare model completions of exisiting dialogues\nto the original human responses across sixteen language models, three dialogue\ncorpora, and a variety of stylometric features. We find that models strongly\nconverge to the conversation's style, often significantly overfitting relative\nto the human baseline. While convergence patterns are often feature-specific,\nwe observe consistent shifts in convergence across modeling settings, with\ninstruction-tuned and larger models converging less than their pretrained\ncounterparts. Given the differences between human and model convergence\npatterns, we hypothesize that the underlying mechanisms for these behaviors are\nvery different.",
      "authors": [
        "Terra Blevins",
        "Susanne Schmalwieser",
        "Benjamin Roth"
      ],
      "published": "2025-08-05T09:55:40+00:00",
      "updated": "2025-08-05T09:55:40+00:00",
      "arxiv_id": "2508.03276v1",
      "url": "http://arxiv.org/pdf/2508.03276v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning",
      "abstract": "Spaced repetition systems are fundamental to efficient learning and memory\nretention, but existing algorithms often struggle with semantic interference\nand personalized adaptation. We present LECTOR (\\textbf{L}LM-\\textbf{E}nhanced\n\\textbf{C}oncept-based \\textbf{T}est-\\textbf{O}riented \\textbf{R}epetition), a\nnovel adaptive scheduling algorithm specifically designed for test-oriented\nlearning scenarios, particularly language examinations where success rate is\nparamount. LECTOR leverages large language models for semantic analysis while\nincorporating personalized learning profiles, addressing the critical challenge\nof semantic confusion in vocabulary learning by utilizing LLM-powered semantic\nsimilarity assessment and integrating it with established spaced repetition\nprinciples. Our comprehensive evaluation against six baseline algorithms\n(SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over\n100 days demonstrates significant improvements: LECTOR achieves a 90.2\\%\nsuccess rate compared to 88.4\\% for the best baseline (SSP-MMC), representing a\n2.0\\% relative improvement. The algorithm shows particular strength in handling\nsemantically similar concepts, reducing confusion-induced errors while\nmaintaining computational efficiency. Our results establish LECTOR as a\npromising direction for intelligent tutoring systems and adaptive learning\nplatforms.",
      "authors": [
        "Jiahao Zhao"
      ],
      "published": "2025-08-05T09:53:26+00:00",
      "updated": "2025-08-05T09:53:26+00:00",
      "arxiv_id": "2508.03275v1",
      "url": "http://arxiv.org/pdf/2508.03275v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?",
      "abstract": "Recent advances in Large Language Models (LLMs) have generated significant\ninterest in their capacity to simulate human-like behaviors, yet most studies\nrely on fictional personas rather than actual human data. We address this\nlimitation by evaluating LLMs' ability to predict individual economic\ndecision-making using Pay-What-You-Want (PWYW) pricing experiments with real\n522 human personas. Our study systematically compares three state-of-the-art\nmultimodal LLMs using detailed persona information from 522 Korean participants\nin cultural consumption scenarios. We investigate whether LLMs can accurately\nreplicate individual human choices and how persona injection methods affect\nprediction performance. Results reveal that while LLMs struggle with precise\nindividual-level predictions, they demonstrate reasonable group-level\nbehavioral tendencies. Also, we found that commonly adopted prompting\ntechniques are not much better than naive prompting methods; reconstruction of\npersonal narrative nor retrieval augmented generation have no significant gain\nagainst simple prompting method. We believe that these findings can provide the\nfirst comprehensive evaluation of LLMs' capabilities on simulating economic\nbehavior using real human data, offering empirical guidance for persona-based\nsimulation in computational social science.",
      "authors": [
        "Junhyuk Choi",
        "Hyeonchu Park",
        "Haemin Lee",
        "Hyebeen Shin",
        "Hyun Joung Jin",
        "Bugeun Kim"
      ],
      "published": "2025-08-05T09:37:37+00:00",
      "updated": "2025-08-05T09:37:37+00:00",
      "arxiv_id": "2508.03262v1",
      "url": "http://arxiv.org/pdf/2508.03262v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization",
      "abstract": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.",
      "authors": [
        "Yueyue Liu",
        "Hongyu Zhang",
        "Yuantian Miao"
      ],
      "published": "2025-08-05T09:35:52+00:00",
      "updated": "2025-08-05T09:35:52+00:00",
      "arxiv_id": "2508.03258v1",
      "url": "http://arxiv.org/pdf/2508.03258v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs",
      "abstract": "Prior clinical psychology research shows that Western individuals with\ndepression tend to report psychological symptoms, while Eastern individuals\nreport somatic ones. We test whether Large Language Models (LLMs), which are\nincreasingly used in mental health, reproduce these cultural patterns by\nprompting them with Western or Eastern personas. Results show that LLMs largely\nfail to replicate the patterns when prompted in English, though prompting in\nmajor Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment\nin several configurations. Our analysis pinpoints two key reasons for this\nfailure: the models' low sensitivity to cultural personas and a strong,\nculturally invariant symptom hierarchy that overrides cultural cues. These\nfindings reveal that while prompt language is important, current\ngeneral-purpose LLMs lack the robust, culture-aware capabilities essential for\nsafe and effective mental health applications.",
      "authors": [
        "Shintaro Sakai",
        "Jisun An",
        "Migyeong Kang",
        "Haewoon Kwak"
      ],
      "published": "2025-08-05T09:25:38+00:00",
      "updated": "2025-08-05T09:25:38+00:00",
      "arxiv_id": "2508.03247v1",
      "url": "http://arxiv.org/pdf/2508.03247v1",
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting",
      "abstract": "This paper details the CardiffNLP team's contribution to the CLEARS shared\ntask on Spanish text adaptation, hosted by IberLEF 2025. The shared task\ncontained two subtasks and the team submitted to both. Our team took an\nLLM-prompting approach with different prompt variations. While we initially\nexperimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and\nlanded third place in Subtask 1 and second place in Subtask 2. We detail our\nnumerous prompt variations, examples, and experimental results.",
      "authors": [
        "Mutaz Ayesh",
        "NicolÃ¡s GutiÃ©rrez-RolÃ³n",
        "Fernando Alva-Manchego"
      ],
      "published": "2025-08-05T09:16:19+00:00",
      "updated": "2025-08-05T09:16:19+00:00",
      "arxiv_id": "2508.03240v1",
      "url": "http://arxiv.org/pdf/2508.03240v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "CookBench: A Long-Horizon Embodied Planning Benchmark for Complex Cooking Scenarios",
      "abstract": "Embodied Planning is dedicated to the goal of creating agents capable of\nexecuting long-horizon tasks in complex physical worlds. However, existing\nembodied planning benchmarks frequently feature short-horizon tasks and\ncoarse-grained action primitives. To address this challenge, we introduce\nCookBench, a benchmark for long-horizon planning in complex cooking scenarios.\nBy leveraging a high-fidelity simulation environment built upon the powerful\nUnity game engine, we define frontier AI challenges in a complex, realistic\nenvironment. The core task in CookBench is designed as a two-stage process.\nFirst, in Intention Recognition, an agent needs to accurately parse a user's\ncomplex intent. Second, in Embodied Interaction, the agent should execute the\nidentified cooking goal through a long-horizon, fine-grained sequence of\nphysical actions. Unlike existing embodied planning benchmarks, we refine the\naction granularity to a spatial level that considers crucial operational\ninformation while abstracting away low-level robotic control. Besides, We\nprovide a comprehensive toolset that encapsulates the simulator. Its unified\nAPI supports both macro-level operations, such as placing orders and purchasing\ningredients, and a rich set of fine-grained embodied actions for physical\ninteraction, enabling researchers to focus on high-level planning and\ndecision-making. Furthermore, we present an in-depth analysis of\nstate-of-the-art, closed-source Large Language Model and Vision-Language Model,\nrevealing their major shortcomings and challenges posed by complex,\nlong-horizon tasks. The full benchmark will be open-sourced to facilitate\nfuture research.",
      "authors": [
        "Muzhen Cai",
        "Xiubo Chen",
        "Yining An",
        "Jiaxin Zhang",
        "Xuesong Wang",
        "Wang Xu",
        "Weinan Zhang",
        "Ting Liu"
      ],
      "published": "2025-08-05T09:01:47+00:00",
      "updated": "2025-08-05T09:01:47+00:00",
      "arxiv_id": "2508.03232v1",
      "url": "http://arxiv.org/pdf/2508.03232v1",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "Navigation Pixie: Implementation and Empirical Study Toward On-demand Navigation Agents in Commercial Metaverse",
      "abstract": "While commercial metaverse platforms offer diverse user-generated content,\nthey lack effective navigation assistance that can dynamically adapt to users'\ninterests and intentions. Although previous research has investigated on-demand\nagents in controlled environments, implementation in commercial settings with\ndiverse world configurations and platform constraints remains challenging.\n  We present Navigation Pixie, an on-demand navigation agent employing a\nloosely coupled architecture that integrates structured spatial metadata with\nLLM-based natural language processing while minimizing platform dependencies,\nwhich enables experiments on the extensive user base of commercial metaverse\nplatforms. Our cross-platform experiments on commercial metaverse platform\nCluster with 99 PC client and 94 VR-HMD participants demonstrated that\nNavigation Pixie significantly increased dwell time and free exploration\ncompared to fixed-route and no-agent conditions across both platforms.\nSubjective evaluations revealed consistent on-demand preferences in PC\nenvironments versus context-dependent social perception advantages in VR-HMD.\nThis research contributes to advancing VR interaction design through\nconversational spatial navigation agents, establishes cross-platform evaluation\nmethodologies revealing environment-dependent effectiveness, and demonstrates\nempirical experimentation frameworks for commercial metaverse platforms.",
      "authors": [
        "Hikari Yanagawa",
        "Yuichi Hiroi",
        "Satomi Tokida",
        "Yuji Hatada",
        "Takefumi Hiraki"
      ],
      "published": "2025-08-05T08:45:34+00:00",
      "updated": "2025-08-05T08:45:34+00:00",
      "arxiv_id": "2508.03216v1",
      "url": "http://arxiv.org/pdf/2508.03216v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "A System Model Generation Benchmark from Natural Language Requirements",
      "abstract": "System models, a critical artifact in software development, provide a formal\nabstraction of both the structural and behavioral aspects of software systems,\nwhich can facilitate the early requirements analysis and architecture design.\nHowever, developing system models remains challenging due to the specific\nsyntax of model description languages and the relative scarcity of public model\nexamples. While large language models (LLMs) have shown promise in generating\ncode with programming languages and could potentially aid in system model\ndevelopment, no benchmarks currently exist for evaluating their ability to\ngenerate system models with specific description languages. We present\nSysMBench, which comprises 151 human-curated scenarios spanning a wide range of\npopular domains and varying difficulty levels. Each scenario mainly comprises a\nnatural language requirements description, a system model expressed in a\nspecific model description language, and a visualized system model diagram. The\nrequirements description is fed as user input to the LLM, the system model with\ndescription language is used to verify if the generated system model conforms\nto the requirements, and the visualized diagram serves to support manual\nvalidation. We introduce SysMEval, a semantic-aware evaluation metric to\nevaluate the quality of generated system models. We evaluate 17 popular LLMs on\nthis task with three traditional metrics and SysMEval, from directly prompting\nto three commonly used enhancement strategies. Our in-depth evaluation shows\nthat LLMs perform poorly on SysMBench, with the highest BLEU of 4% and\nSysMEval-F1 of 62%. We release the SysMBench and its evaluation framework to\nenable future research on LLM-based system model generation.",
      "authors": [
        "Dongming Jin",
        "Zhi Jin",
        "Linyu Li",
        "Zheng Fang",
        "Jia Li",
        "Xiaohong Chen"
      ],
      "published": "2025-08-05T08:45:19+00:00",
      "updated": "2025-08-05T08:45:19+00:00",
      "arxiv_id": "2508.03215v1",
      "url": "http://arxiv.org/pdf/2508.03215v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Probing Syntax in Large Language Models: Successes and Remaining Challenges",
      "abstract": "The syntactic structures of sentences can be readily read-out from the\nactivations of large language models (LLMs). However, the ``structural probes''\nthat have been developed to reveal this phenomenon are typically evaluated on\nan indiscriminate set of sentences. Consequently, it remains unclear whether\nstructural and/or statistical factors systematically affect these syntactic\nrepresentations. To address this issue, we conduct an in-depth analysis of\nstructural probes on three controlled benchmarks. Our results are three-fold.\nFirst, structural probes are biased by a superficial property: the closer two\nwords are in a sentence, the more likely structural probes will consider them\nas syntactically linked. Second, structural probes are challenged by linguistic\nproperties: they poorly represent deep syntactic structures, and get interfered\nby interacting nouns or ungrammatical verb forms. Third, structural probes do\nnot appear to be affected by the predictability of individual words. Overall,\nthis work sheds light on the current challenges faced by structural probes.\nProviding a benchmark made of controlled stimuli to better evaluate their\nperformance.",
      "authors": [
        "Pablo J. Diego-SimÃ³n",
        "Emmanuel Chemla",
        "Jean-RÃ©mi King",
        "Yair Lakretz"
      ],
      "published": "2025-08-05T08:41:14+00:00",
      "updated": "2025-08-05T08:41:14+00:00",
      "arxiv_id": "2508.03211v1",
      "url": "http://arxiv.org/pdf/2508.03211v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP",
      "abstract": "Privacy is a fundamental human right. Data privacy is protected by different\nregulations, such as GDPR. However, modern large language models require a huge\namount of data to learn linguistic variations, and the data often contains\nprivate information. Research has shown that it is possible to extract private\ninformation from such language models. Thus, anonymizing such private and\nsensitive information is of utmost importance. While complete anonymization may\nnot be possible, a number of different pre-processing approaches exist for\nmasking or pseudonymizing private information in textual data. This report\nfocuses on a few of such approaches for domain-agnostic NLP tasks.",
      "authors": [
        "Abhirup Sinha",
        "Pritilata Saha",
        "Tithi Saha"
      ],
      "published": "2025-08-05T08:26:45+00:00",
      "updated": "2025-08-05T08:26:45+00:00",
      "arxiv_id": "2508.03204v1",
      "url": "http://arxiv.org/pdf/2508.03204v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "GTPO: Trajectory-Based Policy Optimization in Large Language Models",
      "abstract": "Policy-based optimizations are widely adopted today for the training and\nalignment of language models, where one of the most recent and effective\napproaches is Group-relative Policy Optimization (GRPO). In this paper, we\nreveals and analyze two major limitations of GRPO: (i) tokens frequently appear\nin completions with both positive and negative rewards, leading to conflicting\ngradient updates that can reduce their output probability, even though can be\nessential for maintaining proper structure; (ii) negatively rewarded\ncompletions may penalize confident responses and shift model decisions toward\nunlikely tokens, progressively flattening the output distribution and degrading\nlearning. To address these issues and provide a more stable and effective\npolicy optimization strategy, we introduce GTPO (Group-relative\nTrajectory-based Policy Optimization), which identifies conflict tokens, tokens\nappearing in the same position across completions with opposite rewards,\nprotects them by skipping negative updates, while amplifying positive ones. To\nfurther prevent policy collapse, GTPO filters out completions whose entropy\nexceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence\nregularization, eliminating the need for a reference model during training,\nwhile still ensuring greater training stability and improved performance,\nvalidated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.",
      "authors": [
        "Marco Simoni",
        "Aleksandar Fontana",
        "Giulio Rossolini",
        "Andrea Saracino"
      ],
      "published": "2025-08-05T08:15:01+00:00",
      "updated": "2025-08-05T08:15:01+00:00",
      "arxiv_id": "2508.03772v1",
      "url": "http://arxiv.org/pdf/2508.03772v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies",
      "abstract": "In recent years, the expansion of neural network models and training data has\ndriven remarkable progress in deep learning, particularly in computer vision\nand natural language processing. This advancement is underpinned by the concept\nof Scaling Laws, which demonstrates that scaling model parameters and training\ndata enhances learning performance. While these fields have witnessed\nbreakthroughs, such as the development of large language models like GPT-4 and\nadvanced vision models like Midjourney, the application of scaling laws in deep\nreinforcement learning (DRL) remains relatively unexplored. Despite its\npotential to improve performance, the integration of scaling laws into DRL for\ndecision making has not been fully realized. This review addresses this gap by\nsystematically analyzing scaling strategies in three dimensions: data, network,\nand training budget. In data scaling, we explore methods to optimize data\nefficiency through parallel sampling and data generation, examining the\nrelationship between data volume and learning outcomes. For network scaling, we\ninvestigate architectural enhancements, including monolithic expansions,\nensemble and MoE methods, and agent number scaling techniques, which\ncollectively enhance model expressivity while posing unique computational\nchallenges. Lastly, in training budget scaling, we evaluate the impact of\ndistributed training, high replay ratios, large batch sizes, and auxiliary\ntraining on training efficiency and convergence. By synthesizing these\nstrategies, this review not only highlights their synergistic roles in\nadvancing DRL for decision making but also provides a roadmap for future\nresearch. We emphasize the importance of balancing scalability with\ncomputational efficiency and outline promising directions for leveraging\nscaling to unlock the full potential of DRL in various tasks such as robot\ncontrol, autonomous driving and LLM training.",
      "authors": [
        "Yi Ma",
        "Hongyao Tang",
        "Chenjun Xiao",
        "Yaodong Yang",
        "Wei Wei",
        "Jianye Hao",
        "Jiye Liang"
      ],
      "published": "2025-08-05T08:03:12+00:00",
      "updated": "2025-08-05T08:03:12+00:00",
      "arxiv_id": "2508.03194v1",
      "url": "http://arxiv.org/pdf/2508.03194v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Trustworthiness of Legal Considerations for the Use of LLMs in Education",
      "abstract": "As Artificial Intelligence (AI), particularly Large Language Models (LLMs),\nbecomes increasingly embedded in education systems worldwide, ensuring their\nethical, legal, and contextually appropriate deployment has become a critical\npolicy concern. This paper offers a comparative analysis of AI-related\nregulatory and ethical frameworks across key global regions, including the\nEuropean Union, United Kingdom, United States, China, and Gulf Cooperation\nCouncil (GCC) countries. It maps how core trustworthiness principles, such as\ntransparency, fairness, accountability, data privacy, and human oversight are\nembedded in regional legislation and AI governance structures. Special emphasis\nis placed on the evolving landscape in the GCC, where countries are rapidly\nadvancing national AI strategies and education-sector innovation. To support\nthis development, the paper introduces a Compliance-Centered AI Governance\nFramework tailored to the GCC context. This includes a tiered typology and\ninstitutional checklist designed to help regulators, educators, and developers\nalign AI adoption with both international norms and local values. By\nsynthesizing global best practices with region-specific challenges, the paper\ncontributes practical guidance for building legally sound, ethically grounded,\nand culturally sensitive AI systems in education. These insights are intended\nto inform future regulatory harmonization and promote responsible AI\nintegration across diverse educational environments.",
      "authors": [
        "Sara Alaswad",
        "Tatiana Kalganova",
        "Wasan Awad"
      ],
      "published": "2025-08-05T07:44:33+00:00",
      "updated": "2025-08-05T07:44:33+00:00",
      "arxiv_id": "2508.03771v1",
      "url": "http://arxiv.org/pdf/2508.03771v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following",
      "abstract": "While advancements in the reasoning abilities of LLMs have significantly\nenhanced their performance in solving mathematical problems, coding tasks, and\ngeneral puzzles, their effectiveness in accurately adhering to instructions\nremains inconsistent, particularly with more complex directives. Our\ninvestigation identifies lazy reasoning during the thinking stage as the\nprimary factor contributing to poor instruction adherence. To mitigate this\nissue, we propose a comprehensive framework designed to enable rigorous\nreasoning processes involving preview and self-checking, essential for\nsatisfying strict instruction constraints. Specifically, we first generate\ninstructions with complex constraints and apply a filtering process to obtain\nvalid prompts, resulting in three distinct prompt datasets categorized as hard,\neasy, and pass. Then, we employ rejection sampling on the pass prompts to\ncurate a small yet high-quality dataset, enabling a cold-start initialization\nof the model and facilitating its adaptation to effective reasoning patterns.\nSubsequently, we employ an entropy-preserving supervised fine-tuning\n(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)\nreinforcement learning guided by rule-based dense rewards. This approach\nencourages the model to transform its reasoning mechanism, ultimately fostering\ngeneralizable reasoning abilities that encompass preview and self-checking.\nExtensive experiments conducted on instruction-following benchmarks demonstrate\nremarkable performance improvements across various model scales. Notably, our\nLight-IF-32B model surpasses both larger open-source models such as DeepSeek-R1\nand closed-source models like Doubao-1.6.",
      "authors": [
        "Chenyang Wang",
        "Liang Wen",
        "Shousheng Jia",
        "Xiangzheng Zhang",
        "Liang Xu"
      ],
      "published": "2025-08-05T07:42:00+00:00",
      "updated": "2025-08-05T07:42:00+00:00",
      "arxiv_id": "2508.03178v1",
      "url": "http://arxiv.org/pdf/2508.03178v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Toward Low-Latency End-to-End Voice Agents for Telecommunications Using Streaming ASR, Quantized LLMs, and Real-Time TTS",
      "abstract": "We introduce a low-latency telecom AI voice agent pipeline for real-time,\ninteractive telecommunications use, enabling advanced voice AI for call center\nautomation, intelligent IVR (Interactive Voice Response), and AI-driven\ncustomer support. The solution is built for telecom, combining four specialized\nmodels by NetoAI: TSLAM, a 4-bit quantized Telecom-Specific Large Language\nModel (LLM); T-VEC, a Telecom-Specific Embedding Model; TTE, a Telecom-Specific\nAutomatic Speech Recognition (ASR) model; and T-Synth, a Telecom-Specific\nText-to-Speech (TTS) model. These models enable highly responsive,\ndomain-adapted voice AI agents supporting knowledge-grounded spoken\ninteractions with low latency. The pipeline integrates streaming ASR (TTE),\nconversational intelligence (TSLAM), retrieval augmented generation (RAG) over\ntelecom documents, and real-time TTS (T-Synth), setting a new benchmark for\ntelecom voice assistants. To evaluate the system, we built a dataset of 500\nhuman-recorded telecom questions from RFCs, simulating real telecom agent\nqueries. This framework allows analysis of latency, domain relevance, and\nreal-time performance across the stack. Results show that TSLAM, TTE, and\nT-Synth deliver real-time factors (RTF) below 1.0, supporting enterprise,\nlow-latency telecom deployments. These AI agents -- powered by TSLAM, TTE, and\nT-Synth -- provide a foundation for next-generation telecom AI, enabling\nautomated customer support, diagnostics, and more.",
      "authors": [
        "Vignesh Ethiraj",
        "Ashwath David",
        "Sidhanth Menon",
        "Divya Vijay"
      ],
      "published": "2025-08-05T07:39:35+00:00",
      "updated": "2025-08-05T07:39:35+00:00",
      "arxiv_id": "2508.04721v1",
      "url": "http://arxiv.org/pdf/2508.04721v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "68T50, 68T10, 94A12",
        "I.2.7; H.3.3; C.2.2"
      ],
      "primary_category": "cs.SD"
    },
    {
      "title": "InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation",
      "abstract": "Collaborative partnership matters in inquiry-oriented education. However,\nmost study partners are selected either rely on experience-based assignments\nwith little scientific planning or build on rule-based machine assistants,\nencountering difficulties in knowledge expansion and inadequate flexibility.\nThis paper proposes an LLM-empowered agent model for simulating and selecting\nlearning partners tailored to inquiry-oriented learning, named InqEduAgent.\nGenerative agents are designed to capture cognitive and evaluative features of\nlearners in real-world scenarios. Then, an adaptive matching algorithm with\nGaussian process augmentation is formulated to identify patterns within prior\nknowledge. Optimal learning-partner matches are provided for learners facing\ndifferent exercises. The experimental results show the optimal performance of\nInqEduAgent in most knowledge-learning scenarios and LLM environment with\ndifferent levels of capabilities. This study promotes the intelligent\nallocation of human-based learning partners and the formulation of AI-based\nlearning partners. The code, data, and appendix are publicly available at\nhttps://github.com/InqEduAgent/InqEduAgent.",
      "authors": [
        "Tian-Fang Zhao",
        "Wen-Xi Yang",
        "Guan Liu",
        "Liang Yang"
      ],
      "published": "2025-08-05T07:33:48+00:00",
      "updated": "2025-08-06T15:28:04+00:00",
      "arxiv_id": "2508.03174v2",
      "url": "http://arxiv.org/pdf/2508.03174v2",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions",
      "abstract": "Mathematical geometric reasoning is essential for scientific discovery and\neducational development, requiring precise logic and rigorous formal\nverification. While recent advances in Multimodal Large Language Models (MLLMs)\nhave improved reasoning tasks, existing models typically struggle with formal\ngeometric reasoning, particularly when dynamically constructing and verifying\nauxiliary geometric elements. To address these challenges, we introduce\nGeoint-R1, a multimodal reasoning framework designed to generate formally\nverifiable geometric solutions from textual descriptions and visual diagrams.\nGeoint-R1 uniquely integrates auxiliary elements construction, formal reasoning\nrepresented via Lean4, and interactive visualization. To systematically\nevaluate and advance formal geometric reasoning, we propose the Geoint\nbenchmark, comprising 1,885 rigorously annotated geometry problems across\ndiverse topics such as plane, spatial, and solid geometry. Each problem\nincludes structured textual annotations, precise Lean4 code for auxiliary\nconstructions, and detailed solution steps verified by experts. Extensive\nexperiments demonstrate that Geoint-R1 significantly surpasses existing\nmultimodal and math-specific reasoning models, particularly on challenging\nproblems requiring explicit auxiliary element constructions.",
      "authors": [
        "Jingxuan Wei",
        "Caijun Jia",
        "Qi Chen",
        "Honghao He",
        "Linzhuang Sun",
        "Conghui He",
        "Lijun Wu",
        "Bihui Yu",
        "Cheng Tan"
      ],
      "published": "2025-08-05T07:29:58+00:00",
      "updated": "2025-08-05T07:29:58+00:00",
      "arxiv_id": "2508.03173v1",
      "url": "http://arxiv.org/pdf/2508.03173v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction",
      "abstract": "Drug toxicity remains a major challenge in pharmaceutical development. Recent\nmachine learning models have improved in silico toxicity prediction, but their\nreliance on annotated data and lack of interpretability limit their\napplicability. This limits their ability to capture organ-specific toxicities\ndriven by complex biological mechanisms. Large language models (LLMs) offer a\npromising alternative through step-by-step reasoning and integration of textual\ndata, yet prior approaches lack biological context and transparent rationale.\nTo address this issue, we propose CoTox, a novel framework that integrates LLM\nwith chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox\ncombines chemical structure data, biological pathways, and gene ontology (GO)\nterms to generate interpretable toxicity predictions through step-by-step\nreasoning. Using GPT-4o, we show that CoTox outperforms both traditional\nmachine learning and deep learning model. We further examine its performance\nacross various LLMs to identify where CoTox is most effective. Additionally, we\nfind that representing chemical structures with IUPAC names, which are easier\nfor LLMs to understand than SMILES, enhances the model's reasoning ability and\nimproves predictive performance. To demonstrate its practical utility in drug\ndevelopment, we simulate the treatment of relevant cell types with drug and\nincorporated the resulting biological context into the CoTox framework. This\napproach allow CoTox to generate toxicity predictions aligned with\nphysiological responses, as shown in case study. This result highlights the\npotential of LLM-based frameworks to improve interpretability and support\nearly-stage drug safety assessment. The code and prompt used in this work are\navailable at https://github.com/dmis-lab/CoTox.",
      "authors": [
        "Jueon Park",
        "Yein Park",
        "Minju Song",
        "Soyon Park",
        "Donghyeon Lee",
        "Seungheun Baek",
        "Jaewoo Kang"
      ],
      "published": "2025-08-05T07:04:44+00:00",
      "updated": "2025-08-05T07:04:44+00:00",
      "arxiv_id": "2508.03159v1",
      "url": "http://arxiv.org/pdf/2508.03159v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Estimating Worst-Case Frontier Risks of Open-Weight LLMs",
      "abstract": "In this paper, we study the worst-case frontier risks of releasing gpt-oss.\nWe introduce malicious fine-tuning (MFT), where we attempt to elicit maximum\ncapabilities by fine-tuning gpt-oss to be as capable as possible in two\ndomains: biology and cybersecurity. To maximize biological risk (biorisk), we\ncurate tasks related to threat creation and train gpt-oss in an RL environment\nwith web browsing. To maximize cybersecurity risk, we train gpt-oss in an\nagentic coding environment to solve capture-the-flag (CTF) challenges. We\ncompare these MFT models against open- and closed-weight LLMs on frontier risk\nevaluations. Compared to frontier closed-weight models, MFT gpt-oss\nunderperforms OpenAI o3, a model that is below Preparedness High capability\nlevel for biorisk and cybersecurity. Compared to open-weight models, gpt-oss\nmay marginally increase biological capabilities but does not substantially\nadvance the frontier. Taken together, these results contributed to our decision\nto release the model, and we hope that our MFT approach can serve as useful\nguidance for estimating harm from future open-weight releases.",
      "authors": [
        "Eric Wallace",
        "Olivia Watkins",
        "Miles Wang",
        "Kai Chen",
        "Chris Koch"
      ],
      "published": "2025-08-05T06:57:53+00:00",
      "updated": "2025-08-05T06:57:53+00:00",
      "arxiv_id": "2508.03153v1",
      "url": "http://arxiv.org/pdf/2508.03153v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Can Large Language Models Bridge the Gap in Environmental Knowledge?",
      "abstract": "This research investigates the potential of Artificial Intelligence (AI)\nmodels to bridge the knowledge gap in environmental education among university\nstudents. By focusing on prominent large language models (LLMs) such as\nGPT-3.5, GPT-4, GPT-4o, Gemini, Claude Sonnet, and Llama 2, the study assesses\ntheir effectiveness in conveying environmental concepts and, consequently,\nfacilitating environmental education. The investigation employs a standardized\ntool, the Environmental Knowledge Test (EKT-19), supplemented by targeted\nquestions, to evaluate the environmental knowledge of university students in\ncomparison to the responses generated by the AI models. The results of this\nstudy suggest that while AI models possess a vast, readily accessible, and\nvalid knowledge base with the potential to empower both students and academic\nstaff, a human discipline specialist in environmental sciences may still be\nnecessary to validate the accuracy of the information provided.",
      "authors": [
        "Linda Smail",
        "David Santandreu Calonge",
        "Firuz Kamalov",
        "Nur H. Orak"
      ],
      "published": "2025-08-05T06:55:07+00:00",
      "updated": "2025-08-05T06:55:07+00:00",
      "arxiv_id": "2508.03149v1",
      "url": "http://arxiv.org/pdf/2508.03149v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Frontier: Simulating the Next Generation of LLM Inference Systems",
      "abstract": "Large Language Model (LLM) inference is growing increasingly complex with the\nrise of Mixture-of-Experts (MoE) models and disaggregated architectures that\ndecouple components like prefill/decode (PD) or attention/FFN (AF) for\nheterogeneous scaling. Existing simulators, architected for co-located, dense\nmodels, are unable to capture the intricate system dynamics of these emerging\nparadigms. We present Frontier, a high-fidelity simulator designed from the\nground up for this new landscape. Frontier introduces a unified framework to\nmodel both co-located and disaggregated systems, providing native support for\nMoE inference with expert parallelism (EP). It enables the simulation of\ncomplex workflows like cross-cluster expert routing and advanced pipelining\nstrategies for latency hiding. To ensure fidelity and usability, Frontier\nincorporates refined operator models for improved accuracy. Frontier empowers\nthe community to design and optimize the future of LLM inference at scale.",
      "authors": [
        "Yicheng Feng",
        "Xin Tan",
        "Kin Hang Sew",
        "Yimin Jiang",
        "Yibo Zhu",
        "Hong Xu"
      ],
      "published": "2025-08-05T06:53:28+00:00",
      "updated": "2025-08-05T06:53:28+00:00",
      "arxiv_id": "2508.03148v1",
      "url": "http://arxiv.org/pdf/2508.03148v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Who is a Better Player: LLM against LLM",
      "abstract": "Adversarial board games, as a paradigmatic domain of strategic reasoning and\nintelligence, have long served as both a popular competitive activity and a\nbenchmark for evaluating artificial intelligence (AI) systems. Building on this\nfoundation, we propose an adversarial benchmarking framework to assess the\ncomprehensive performance of Large Language Models (LLMs) through board games\ncompetition, compensating the limitation of data dependency of the mainstream\nQuestion-and-Answer (Q&A) based benchmark method. We introduce Qi Town, a\nspecialized evaluation platform that supports 5 widely played games and\ninvolves 20 LLM-driven players. The platform employs both the Elo rating system\nand a novel Performance Loop Graph (PLG) to quantitatively evaluate the\ntechnical capabilities of LLMs, while also capturing Positive Sentiment Score\n(PSS) throughout gameplay to assess mental fitness. The evaluation is\nstructured as a round-robin tournament, enabling systematic comparison across\nplayers. Experimental results indicate that, despite technical differences,\nmost LLMs remain optimistic about winning and losing, demonstrating greater\nadaptability to high-stress adversarial environments than humans. On the other\nhand, the complex relationship between cyclic wins and losses in PLGs exposes\nthe instability of LLMs' skill play during games, warranting further\nexplanation and exploration.",
      "authors": [
        "Yingjie Zhou",
        "Jiezhang Cao",
        "Farong Wen",
        "Li Xu",
        "Yanwei Jiang",
        "Jun Jia",
        "Ronghui Li",
        "Xiaohong Liu",
        "Yu Zhou",
        "Xiongkuo Min",
        "Jie Guo",
        "Zicheng Zhang",
        "Guangtao Zhai"
      ],
      "published": "2025-08-05T06:41:47+00:00",
      "updated": "2025-08-05T06:41:47+00:00",
      "arxiv_id": "2508.04720v1",
      "url": "http://arxiv.org/pdf/2508.04720v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior",
      "abstract": "Large Language Models (LLMs) with long chain-of-thought (CoT) capability,\ntermed Reasoning Models, demonstrate superior intricate problem-solving\nabilities through multi-step long CoT reasoning. To create a dual-capability\nmodel with long CoT capability and domain-specific knowledge without\nsubstantial computational and data costs, model merging emerges as a highly\nresource-efficient method. However, significant challenges lie in merging\ndomain-specific LLMs with long CoT ones since nowadays merging methods suffer\nfrom reasoning capability degradation, even gibberish output and output\ncollapse. To overcome this, we introduce RCP-Merging: Merging Long\nChain-of-Thought Models with Domain-Specific Models by Considering Reasoning\nCapability as Prior, a novel merging framework designed to integrate\ndomain-specific LLMs with long CoT capability, meanwhile maintaining model\nperformance in the original domain. Treating reasoning model weights as\nfoundational prior, our method utilizes a reasoning capability indicator to\npreserve core long CoT capability model weights while selectively merging\nessential domain-specific weights. We conducted extensive experiments on\nQwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance\ndomains. Our results show that RCP-Merging successfully merges a reasoning\nmodel with domain-specific ones, improving domain task performance by 9.5% and\n9.2% over state-of-the-art methods, without significantly harming the original\nlong CoT reasoning capability.",
      "authors": [
        "Junyao Yang",
        "Jianwei Wang",
        "Huiping Zhuang",
        "Cen Chen",
        "Ziqian Zeng"
      ],
      "published": "2025-08-05T06:38:18+00:00",
      "updated": "2025-08-05T06:38:18+00:00",
      "arxiv_id": "2508.03140v1",
      "url": "http://arxiv.org/pdf/2508.03140v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation",
      "abstract": "Robots operating in human-centric or hazardous environments must proactively\nanticipate and mitigate dangers beyond basic obstacle detection. Traditional\nnavigation systems often depend on static maps, which struggle to account for\ndynamic risks, such as a person emerging from a suddenly opening door. As a\nresult, these systems tend to be reactive rather than anticipatory when\nhandling dynamic hazards. Recent advancements in pre-trained large language\nmodels and vision-language models (VLMs) create new opportunities for proactive\nhazard avoidance. In this work, we propose a zero-shot language-as-cost mapping\nframework that leverages VLMs to interpret visual scenes, assess potential\ndynamic risks, and assign risk-aware navigation costs preemptively, enabling\nrobots to anticipate hazards before they materialize. By integrating this\nlanguage-based cost map with a geometric obstacle map, the robot not only\nidentifies existing obstacles but also anticipates and proactively plans around\npotential hazards arising from environmental dynamics. Experiments in simulated\nand diverse dynamic environments demonstrate that the proposed method\nsignificantly improves navigation success rates and reduces hazard encounters,\ncompared to reactive baseline planners. Code and supplementary materials are\navailable at https://github.com/Taekmino/LaC.",
      "authors": [
        "Mintaek Oh",
        "Chan Kim",
        "Seung-Woo Seo",
        "Seong-Woo Kim"
      ],
      "published": "2025-08-05T06:35:37+00:00",
      "updated": "2025-08-05T06:35:37+00:00",
      "arxiv_id": "2508.03138v1",
      "url": "http://arxiv.org/pdf/2508.03138v1",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "Long Story Generation via Knowledge Graph and Literary Theory",
      "abstract": "The generation of a long story consisting of several thousand words is a\nsub-task in the field of long text generation~(LTG). Previous research has\naddressed this challenge through outline-based generation, which employs a\nmulti-stage method for generating outlines into stories. However, this approach\nsuffers from two common issues: almost inevitable theme drift caused by the\nloss of memory of previous outlines, and tedious plots with incoherent logic\nthat are less appealing to human readers.\n  In this paper, we propose the multi-agent Story Generator structure to\nimprove the multi-stage method, using large language models~(LLMs) as the core\ncomponents of agents. To avoid theme drift, we introduce a memory storage model\ncomprising two components: a long-term memory storage that identifies the most\nimportant memories, thereby preventing theme drift; and a short-term memory\nstorage that retains the latest outlines from each generation round. To\nincorporate engaging elements into the story, we design a story theme obstacle\nframework based on literary narratology theory that introduces uncertain\nfactors and evaluation criteria to generate outline. This framework calculates\nthe similarity of the former storyline and enhances the appeal of the story by\nbuilding a knowledge graph and integrating new node content. Additionally, we\nestablish a multi-agent interaction stage to simulate writer-reader interaction\nthrough dialogue and revise the story text according to feedback, to ensure it\nremains consistent and logical. Evaluations against previous methods\ndemonstrate that our approach can generate higher-quality long stories.",
      "authors": [
        "Ge Shi",
        "Kaiyu Huang",
        "Guochen Feng"
      ],
      "published": "2025-08-05T06:35:14+00:00",
      "updated": "2025-08-05T06:35:14+00:00",
      "arxiv_id": "2508.03137v1",
      "url": "http://arxiv.org/pdf/2508.03137v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Attack the Messages, Not the Agents: A Multi-round Adaptive Stealthy Tampering Framework for LLM-MAS",
      "abstract": "Large language model-based multi-agent systems (LLM-MAS) effectively\naccomplish complex and dynamic tasks through inter-agent communication, but\nthis reliance introduces substantial safety vulnerabilities. Existing attack\nmethods targeting LLM-MAS either compromise agent internals or rely on direct\nand overt persuasion, which limit their effectiveness, adaptability, and\nstealthiness. In this paper, we propose MAST, a Multi-round Adaptive Stealthy\nTampering framework designed to exploit communication vulnerabilities within\nthe system. MAST integrates Monte Carlo Tree Search with Direct Preference\nOptimization to train an attack policy model that adaptively generates\neffective multi-round tampering strategies. Furthermore, to preserve\nstealthiness, we impose dual semantic and embedding similarity constraints\nduring the tampering process. Comprehensive experiments across diverse tasks,\ncommunication architectures, and LLMs demonstrate that MAST consistently\nachieves high attack success rates while significantly enhancing stealthiness\ncompared to baselines. These findings highlight the effectiveness,\nstealthiness, and adaptability of MAST, underscoring the need for robust\ncommunication safeguards in LLM-MAS.",
      "authors": [
        "Bingyu Yan",
        "Ziyi Zhou",
        "Xiaoming Zhang",
        "Chaozhuo Li",
        "Ruilin Zeng",
        "Yirui Qi",
        "Tianbo Wang",
        "Litian Zhang"
      ],
      "published": "2025-08-05T06:14:53+00:00",
      "updated": "2025-08-05T06:14:53+00:00",
      "arxiv_id": "2508.03125v1",
      "url": "http://arxiv.org/pdf/2508.03125v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Can Large Language Models Identify Materials from Radar Signals?",
      "abstract": "Accurately identifying the material composition of objects is a critical\ncapability for AI robots powered by large language models (LLMs) to perform\ncontext-aware manipulation. Radar technologies offer a promising sensing\nmodality for material recognition task. When combined with deep learning, radar\ntechnologies have demonstrated strong potential in identifying the material of\nvarious objects. However, existing radar-based solutions are often constrained\nto closed-set object categories and typically require task-specific data\ncollection to train deep learning models, largely limiting their practical\napplicability. This raises an important question: Can we leverage the powerful\nreasoning capabilities of pre-trained LLMs to directly infer material\ncomposition from raw radar signals? Answering this question is non-trivial due\nto the inherent redundancy of radar signals and the fact that pre-trained LLMs\nhave no prior exposure to raw radar data during training. To address this, we\nintroduce LLMaterial, the first study to investigate the feasibility of using\nLLM to identify materials directly from radar signals. First, we introduce a\nphysics-informed signal processing pipeline that distills high-redundancy radar\nraw data into a set of compact intermediate parameters that encapsulate the\nmaterial's intrinsic characteristics. Second, we adopt a retrieval-augmented\ngeneration (RAG) strategy to provide the LLM with domain-specific knowledge,\nenabling it to interpret and reason over the extracted intermediate parameters.\nLeveraging this integration, the LLM is empowered to perform step-by-step\nreasoning on the condensed radar features, achieving open-set material\nrecognition directly from raw radar signals. Preliminary results show that\nLLMaterial can effectively distinguish among a variety of common materials,\nhighlighting its strong potential for real-world material identification\napplications.",
      "authors": [
        "Jiangyou Zhu",
        "Hongyu Deng",
        "He Chen"
      ],
      "published": "2025-08-05T06:07:49+00:00",
      "updated": "2025-08-05T06:07:49+00:00",
      "arxiv_id": "2508.03120v1",
      "url": "http://arxiv.org/pdf/2508.03120v1",
      "categories": [
        "eess.SP",
        "cs.ET",
        "cs.RO"
      ],
      "primary_category": "eess.SP"
    },
    {
      "title": "Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation",
      "abstract": "We present a framework for training trustworthy large language model (LLM)\nagents for optimization modeling via a verifiable synthetic data generation\npipeline. Focusing on linear and mixed-integer linear programming, our approach\nbegins with structured symbolic representations and systematically produces\nnatural language descriptions, mathematical formulations, and solver-executable\ncode. By programmatically constructing each instance with known optimal\nsolutions, the pipeline ensures full verifiability and enables automatic\nfiltering of low-quality demonstrations generated by teacher models. Each\ndataset instance includes a structured representation of the optimization\nproblem, a corresponding natural language description, the verified optimal\nsolution, and step-by-step demonstrations - generated by a teacher model - that\nshow how to model and solve the problem across multiple optimization modeling\nlanguages. This enables supervised fine-tuning of open-source LLMs specifically\ntailored to optimization tasks. To operationalize this pipeline, we introduce\nOptiTrust, a modular LLM agent that performs multi-stage translation from\nnatural language to solver-ready code, leveraging stepwise demonstrations,\nmulti-language inference, and majority-vote cross-validation. Our agent\nachieves state-of-the-art performance on standard benchmarks. Out of 7\ndatasets, it achieves the highest accuracy on six and outperforms the next-best\nalgorithm by at least 8 percentage on three of them. Our approach provides a\nscalable, verifiable, and principled path toward building reliable LLM agents\nfor real-world optimization applications.",
      "authors": [
        "Vinicius Lima",
        "Dzung T. Phan",
        "Jayant Kalagnanam",
        "Dhaval Patel",
        "Nianjun Zhou"
      ],
      "published": "2025-08-05T05:54:20+00:00",
      "updated": "2025-08-05T05:54:20+00:00",
      "arxiv_id": "2508.03117v1",
      "url": "http://arxiv.org/pdf/2508.03117v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation",
      "abstract": "While large language models (LLMs) have achieved remarkable success in\nproviding trustworthy responses for knowledge-intensive tasks, they still face\ncritical limitations such as hallucinations and outdated knowledge. To address\nthese issues, the retrieval-augmented generation (RAG) framework enhances LLMs\nwith access to external knowledge via a retriever, enabling more accurate and\nreal-time outputs about the latest events. However, this integration brings new\nsecurity vulnerabilities: the risk that malicious content in the external\ndatabase can be retrieved and used to manipulate model outputs. Although prior\nwork has explored attacks on RAG systems, existing approaches either rely\nheavily on access to the retriever or fail to jointly consider both retrieval\nand generation stages, limiting their effectiveness, particularly in black-box\nscenarios. To overcome these limitations, we propose Token-level Precise Attack\non the RAG (TPARAG), a novel framework that targets both white-box and\nblack-box RAG systems. TPARAG leverages a lightweight white-box LLM as an\nattacker to generate and iteratively optimize malicious passages at the token\nlevel, ensuring both retrievability and high attack success in generation.\nExtensive experiments on open-domain QA datasets demonstrate that TPARAG\nconsistently outperforms previous approaches in retrieval-stage and end-to-end\nattack effectiveness. These results further reveal critical vulnerabilities in\nRAG pipelines and offer new insights into improving their robustness.",
      "authors": [
        "Zizhong Li",
        "Haopeng Zhang",
        "Jiawei Zhang"
      ],
      "published": "2025-08-05T05:44:19+00:00",
      "updated": "2025-08-05T05:44:19+00:00",
      "arxiv_id": "2508.03110v1",
      "url": "http://arxiv.org/pdf/2508.03110v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "AgentSME for Simulating Diverse Communication Modes in Smart Education",
      "abstract": "Generative agent models specifically tailored for smart education are\ncritical, yet remain relatively underdeveloped. A key challenge stems from the\ninherent complexity of educational contexts: learners are human beings with\nvarious cognitive behaviors, and pedagogy is fundamentally centered on\npersonalized human-to-human communication. To address this issue, this paper\nproposes AgentSME, a unified generative agent framework powered by LLM. Three\ndirectional communication modes are considered in the models, namely Solo,\nMono, and Echo, reflecting different types of agency autonomy and communicative\nreciprocity. Accuracy is adopted as the primary evaluation metric, complemented\nby three diversity indices designed to assess the diversity of reasoning\ncontents. Six widely used LLMs are tested to validate the robustness of\ncommunication modes across different model tiers, which are equally divided\ninto base-capacity and high-capacity configurations. The results show that\ngenerative agents that employ the Echo communication mode achieve the highest\naccuracy scores, while DeepSeek exhibits the greatest diversity. This study\nprovides valuable information to improve agent learning capabilities and\ninspire smart education models.",
      "authors": [
        "Wen-Xi Yang",
        "Tian-Fang Zhao"
      ],
      "published": "2025-08-05T05:40:40+00:00",
      "updated": "2025-08-05T05:40:40+00:00",
      "arxiv_id": "2508.03109v1",
      "url": "http://arxiv.org/pdf/2508.03109v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping",
      "abstract": "We propose Point2Act, which directly retrieves the 3D action point relevant\nfor a contextually described task, leveraging Multimodal Large Language Models\n(MLLMs). Foundation models opened the possibility for generalist robots that\ncan perform a zero-shot task following natural language descriptions within an\nunseen environment. While the semantics obtained from large-scale image and\nlanguage datasets provide contextual understanding in 2D images, the rich yet\nnuanced features deduce blurry 2D regions and struggle to find precise 3D\nlocations for actions. Our proposed 3D relevancy fields bypass the\nhigh-dimensional features and instead efficiently imbue lightweight 2D\npoint-level guidance tailored to the task-specific action. The multi-view\naggregation effectively compensates for misalignments due to geometric\nambiguities, such as occlusion, or semantic uncertainties inherent in the\nlanguage descriptions. The output region is highly localized, reasoning\nfine-grained 3D spatial context that can directly transfer to an explicit\nposition for physical action at the on-the-fly reconstruction of the scene. Our\nfull-stack pipeline, which includes capturing, MLLM querying, 3D\nreconstruction, and grasp pose extraction, generates spatially grounded\nresponses in under 20 seconds, facilitating practical manipulation tasks.\nProject page: https://sangminkim-99.github.io/point2act/",
      "authors": [
        "Sang Min Kim",
        "Hyeongjun Heo",
        "Junho Kim",
        "Yonghyeon Lee",
        "Young Min Kim"
      ],
      "published": "2025-08-05T05:23:19+00:00",
      "updated": "2025-08-05T05:23:19+00:00",
      "arxiv_id": "2508.03099v1",
      "url": "http://arxiv.org/pdf/2508.03099v1",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large\nlanguage models (LLMs) by conditioning outputs on external knowledge sources.\nHowever, when retrieval involves private or sensitive data, RAG systems are\nsusceptible to extraction attacks that can leak confidential information\nthrough generated responses. We propose Privacy-Aware Decoding (PAD), a\nlightweight, inference-time defense that adaptively injects calibrated Gaussian\nnoise into token logits during generation. PAD integrates confidence-based\nscreening to selectively protect high-risk tokens, efficient sensitivity\nestimation to minimize unnecessary noise, and context-aware noise calibration\nto balance privacy with generation quality. A \\renyi Differential Privacy (RDP)\naccountant rigorously tracks cumulative privacy loss, enabling explicit\nper-response $(\\varepsilon, \\delta)$-DP guarantees for sensitive outputs.\nUnlike prior approaches requiring retraining or corpus-level filtering, PAD is\nmodel-agnostic and operates entirely at decoding time with minimal\ncomputational overhead. Experiments on three real-world datasets demonstrate\nthat PAD substantially reduces private information leakage while preserving\nresponse utility, outperforming existing retrieval- and post-processing-based\ndefenses. Our work takes an important step toward mitigating privacy risks in\nRAG via decoding strategies, paving the way for universal and scalable privacy\nsolutions in sensitive domains. Our code is available:\nhttps://github.com/wang2226/PAD.",
      "authors": [
        "Haoran Wang",
        "Xiongxiao Xu",
        "Baixiang Huang",
        "Kai Shu"
      ],
      "published": "2025-08-05T05:22:13+00:00",
      "updated": "2025-08-05T05:22:13+00:00",
      "arxiv_id": "2508.03098v1",
      "url": "http://arxiv.org/pdf/2508.03098v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs",
      "abstract": "With the advancement of Large Language Models (LLMs), LLM applications have\nexpanded into a growing number of fields. However, users with data privacy\nconcerns face limitations in directly utilizing LLM APIs, while private\ndeployments incur significant computational demands. This creates a substantial\nchallenge in achieving secure LLM adaptation under constrained local resources.\nTo address this issue, collaborative learning methods, such as Split Learning\n(SL), offer a resource-efficient and privacy-preserving solution for adapting\nLLMs to private domains. In this study, we introduce VFLAIR-LLM (available at\nhttps://github.com/FLAIR-THU/VFLAIR-LLM), an extensible and lightweight split\nlearning framework for LLMs, enabling privacy-preserving LLM inference and\nfine-tuning in resource-constrained environments. Our library provides two LLM\npartition settings, supporting three task types and 18 datasets. In addition,\nwe provide standard modules for implementing and evaluating attacks and\ndefenses. We benchmark 5 attacks and 9 defenses under various Split Learning\nfor LLM(SL-LLM) settings, offering concrete insights and recommendations on the\nchoice of model partition configurations, defense strategies, and relevant\nhyperparameters for real-world applications.",
      "authors": [
        "Zixuan Gu",
        "Qiufeng Fan",
        "Long Sun",
        "Yang Liu",
        "Xiaojun Ye"
      ],
      "published": "2025-08-05T05:20:33+00:00",
      "updated": "2025-08-05T05:20:33+00:00",
      "arxiv_id": "2508.03097v1",
      "url": "http://arxiv.org/pdf/2508.03097v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "I.2.11"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts",
      "abstract": "Continual learning is essential for medical image classification systems to\nadapt to dynamically evolving clinical environments. The integration of\nmultimodal information can significantly enhance continual learning of image\nclasses. However, while existing approaches do utilize textual modality\ninformation, they solely rely on simplistic templates with a class name,\nthereby neglecting richer semantic information. To address these limitations,\nwe propose a novel framework that harnesses visual concepts generated by large\nlanguage models (LLMs) as discriminative semantic guidance. Our method\ndynamically constructs a visual concept pool with a similarity-based filtering\nmechanism to prevent redundancy. Then, to integrate the concepts into the\ncontinual learning process, we employ a cross-modal image-concept attention\nmodule, coupled with an attention loss. Through attention, the module can\nleverage the semantic knowledge from relevant visual concepts and produce\nclass-representative fused features for classification. Experiments on medical\nand natural image datasets show our method achieves state-of-the-art\nperformance, demonstrating the effectiveness and superiority of our method. We\nwill release the code publicly.",
      "authors": [
        "Jiantao Tan",
        "Peixian Ma",
        "Kanghao Chen",
        "Zhiming Dai",
        "Ruixuan Wang"
      ],
      "published": "2025-08-05T05:15:54+00:00",
      "updated": "2025-08-05T05:15:54+00:00",
      "arxiv_id": "2508.03094v1",
      "url": "http://arxiv.org/pdf/2508.03094v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework",
      "abstract": "With the proliferation of Large Language Models (LLMs), the detection of\nmisinformation has become increasingly important and complex. This research\nproposes an innovative verifiable misinformation detection LLM agent that goes\nbeyond traditional true/false binary judgments. The agent actively verifies\nclaims through dynamic interaction with diverse web sources, assesses\ninformation source credibility, synthesizes evidence, and provides a complete\nverifiable reasoning process. Our designed agent architecture includes three\ncore tools: precise web search tool, source credibility assessment tool and\nnumerical claim verification tool. These tools enable the agent to execute\nmulti-step verification strategies, maintain evidence logs, and form\ncomprehensive assessment conclusions. We evaluate using standard misinformation\ndatasets such as FakeNewsNet, comparing with traditional machine learning\nmodels and LLMs. Evaluation metrics include standard classification metrics,\nquality assessment of reasoning processes, and robustness testing against\nrewritten content. Experimental results show that our agent outperforms\nbaseline methods in misinformation detection accuracy, reasoning transparency,\nand resistance to information rewriting, providing a new paradigm for\ntrustworthy AI-assisted fact-checking.",
      "authors": [
        "Zikun Cui",
        "Tianyi Huang",
        "Chia-En Chiang",
        "Cuiqianhe Du"
      ],
      "published": "2025-08-05T05:15:03+00:00",
      "updated": "2025-08-05T05:15:03+00:00",
      "arxiv_id": "2508.03092v1",
      "url": "http://arxiv.org/pdf/2508.03092v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "ADSeeker: A Knowledge-Infused Framework for Anomaly Detection and Reasoning",
      "abstract": "Automatic vision inspection holds significant importance in industry\ninspection. While multimodal large language models (MLLMs) exhibit strong\nlanguage understanding capabilities and hold promise for this task, their\nperformance remains significantly inferior to that of human experts. In this\ncontext, we identify two key challenges: (i) insufficient integration of\nanomaly detection (AD) knowledge during pre-training, and (ii) the lack of\ntechnically precise and conte-aware language generation for anomaly reasoning.\nTo address these issues, we propose ADSeeker, an anomaly task assistant\ndesigned to enhance inspection performance through knowledge-grounded\nreasoning. ADSeeker leverages a curated visual document knowledge base,\nSEEK-MVTec&VisA (SEEK-M&V), which we construct to address the limitations of\nexisting resources that rely solely on unstructured text. SEEK-M&V includes\nsemantic-rich descriptions and image-document pairs, enabling more\ncomprehensive anomaly understanding. To effectively retrieve and utilize this\nknowledge, we introduce the Query Image-Knowledge Retrieval-Augmented\nGeneration (Q2K RAG) framework. To further enhance the performance in zero-shot\nanomaly detection (ZSAD), ADSeeker leverages the Hierarchical Sparse Prompt\nmechanism and type-level features to efficiently extract anomaly patterns.\nFurthermore, to tackle the challenge of limited in industry anomaly detection\n(IAD) data, we introduce the largest-scale AD dataset, Multi-type Anomaly\n(MulA), encompassing 72 multi-scale defect types across 26 Categories.\nExtensive experiments show that our plug-and-play framework, ADSeeker, achieves\nstate-of-the-art zero-shot performance on several benchmark datasets.",
      "authors": [
        "Kai Zhang",
        "Zekai Zhang",
        "Xihe Sun",
        "Jingmeng Nie",
        "Qinghui Chen",
        "Han Hao",
        "Jianyuan Guo",
        "Jinglin Zhang"
      ],
      "published": "2025-08-05T05:05:06+00:00",
      "updated": "2025-08-05T05:05:06+00:00",
      "arxiv_id": "2508.03088v1",
      "url": "http://arxiv.org/pdf/2508.03088v1",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "When AI Evaluates Its Own Work: Validating Learner-Initiated, AI-Generated Physics Practice Problems",
      "abstract": "Large language models (LLMs) can now generate physics practice problems in\nreal time, yet the educational value of these items hinges on rapid, reliable\npost-generation vetting. We investigated which automated checks are both\ntechnically feasible and pedagogically meaningful when exercises are produced\non demand within a chatbot interface. A cohort of 34 introductory-physics\nstudents generated and attempted 543 problems during exam preparation. Each\nitem was labeled by an expert on a wide range of quality attributes and\npresented to the learners in pairs to record their preference. We then (i)\nbenchmarked three commodity LLMs as \"judges\" against the expert labels, (ii)\nquantified which attributes predict student choice via random-forest models,\nand (iii) triangulated these results with free-form exit surveys. Only a small\nsubset of the original rubric proved necessary to reliably address student\npreferences either directly or by proxy. The study demonstrates that scalable\nformative assessment does not require exhaustive scoring: a carefully curated\ncore of structural and learner-visible checks is sufficient to ensure both\ntechnical soundness and user appeal. The findings provide a practical blueprint\nfor deploying real-time, AI-generated practice in physics and other\nquantitative disciplines.",
      "authors": [
        "Tobias Geisler",
        "Gerd Kortemeyer"
      ],
      "published": "2025-08-05T04:58:16+00:00",
      "updated": "2025-08-05T04:58:16+00:00",
      "arxiv_id": "2508.03085v1",
      "url": "http://arxiv.org/pdf/2508.03085v1",
      "categories": [
        "physics.ed-ph"
      ],
      "primary_category": "physics.ed-ph"
    },
    {
      "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design",
      "abstract": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has\nachieved notable success in recent years. Despite the effectiveness of existing\napproaches, they only design a single heuristic to serve all problem instances,\noften inducing poor generalization across different distributions or settings.\nTo address this issue, we propose Automated Heuristic Set Design (AHSD), a new\nformulation for LLM-driven AHD. The aim of AHSD is to automatically generate a\nsmall-sized complementary heuristic set to serve diverse problem instances,\nsuch that each problem instance could be optimized by at least one heuristic in\nthis set. We show that the objective function of AHSD is monotone and\nsupermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the\nAHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary\npopulation management and complementary-aware memetic search, EoH-S could\neffectively generate a set of high-quality and complementary heuristics.\nComprehensive experimental results on three AHD tasks with diverse instances\nspanning various sizes and distributions demonstrate that EoH-S consistently\noutperforms existing state-of-the-art AHD methods and achieves up to 60\\%\nperformance improvements.",
      "authors": [
        "Fei Liu",
        "Yilu Liu",
        "Qingfu Zhang",
        "Xialiang Tong",
        "Mingxuan Yuan"
      ],
      "published": "2025-08-05T04:55:03+00:00",
      "updated": "2025-08-05T04:55:03+00:00",
      "arxiv_id": "2508.03082v1",
      "url": "http://arxiv.org/pdf/2508.03082v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification in Commercial Contracts",
      "abstract": "The potential of large language models (LLMs) in specialized domains such as\nlegal risk analysis remains underexplored. In response to growing interest in\nlocally deploying open-source LLMs for legal tasks while preserving data\nconfidentiality, this paper introduces ContractEval, the first benchmark to\nthoroughly evaluate whether open-source LLMs could match proprietary LLMs in\nidentifying clause-level legal risks in commercial contracts. Using the\nContract Understanding Atticus Dataset (CUAD), we assess 4 proprietary and 15\nopen-source LLMs. Our results highlight five key findings: (1) Proprietary\nmodels outperform open-source models in both correctness and output\neffectiveness, though some open-source models are competitive in certain\nspecific dimensions. (2) Larger open-source models generally perform better,\nthough the improvement slows down as models get bigger. (3) Reasoning\n(\"thinking\") mode improves output effectiveness but reduces correctness, likely\ndue to over-complicating simpler tasks. (4) Open-source models generate \"no\nrelated clause\" responses more frequently even when relevant clauses are\npresent. This suggests \"laziness\" in thinking or low confidence in extracting\nrelevant content. (5) Model quantization speeds up inference but at the cost of\nperformance drop, showing the tradeoff between efficiency and accuracy. These\nfindings suggest that while most LLMs perform at a level comparable to junior\nlegal assistants, open-source models require targeted fine-tuning to ensure\ncorrectness and effectiveness in high-stakes legal settings. ContractEval\noffers a solid benchmark to guide future development of legal-domain LLMs.",
      "authors": [
        "Shuang Liu",
        "Zelong Li",
        "Ruoyun Ma",
        "Haiyan Zhao",
        "Mengnan Du"
      ],
      "published": "2025-08-05T04:53:05+00:00",
      "updated": "2025-08-05T04:53:05+00:00",
      "arxiv_id": "2508.03080v1",
      "url": "http://arxiv.org/pdf/2508.03080v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Exploring Fairness across Fine-Grained Attributes in Large Vision-Language Models",
      "abstract": "The rapid expansion of applications using Large Vision-Language Models\n(LVLMs), such as GPT-4o, has raised significant concerns about their fairness.\nWhile existing studies primarily focus on demographic attributes such as race\nand gender, fairness across a broader range of attributes remains largely\nunexplored. In this study, we construct an open-set knowledge base of bias\nattributes leveraging Large Language Models (LLMs) and evaluate the fairness of\nLVLMs across finer-grained attributes. Our experimental results reveal that\nLVLMs exhibit biased outputs across a diverse set of attributes and further\ndemonstrate that cultural, environmental, and behavioral factors have a more\npronounced impact on LVLM decision-making than traditional demographic\nattributes.",
      "authors": [
        "Zaiying Zhao",
        "Toshihiko Yamasaki"
      ],
      "published": "2025-08-05T04:52:32+00:00",
      "updated": "2025-08-05T04:52:32+00:00",
      "arxiv_id": "2508.03079v1",
      "url": "http://arxiv.org/pdf/2508.03079v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning",
      "abstract": "Defending large language models (LLMs) against jailbreak attacks is essential\nfor their safe and reliable deployment. Existing defenses often rely on shallow\npattern matching, which struggles to generalize to novel and unseen attack\nstrategies. To address this challenge, we propose the Cognitive-Driven Defense\n(CDD) framework, which targets the underlying structure of jailbreak prompts by\napplying meta-operations, defined as basic manipulations that conceal harmful\nintent.CDD emulates human cognitive reasoning through a structured reasoning\nchain. It begins with a global perception of the prompt and follows with a\nlocalized analysis to uncover hidden manipulations. By applying supervised\nfine-tuning on this structured chain, the model learns to identify and reason\nabout known manipulation patterns. To enhance generalization to unseen threats,\nan entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to\nencourage exploration of new types and variants of meta-operations. Experiments\ndemonstrate that CDD can achieve state-of-the-art defense performance and\nexhibit strong generalization to unseen jailbreak attacks.",
      "authors": [
        "Rui Pu",
        "Chaozhuo Li",
        "Rui Ha",
        "Litian Zhang",
        "Lirong Qiu",
        "Xi Zhang"
      ],
      "published": "2025-08-05T03:58:15+00:00",
      "updated": "2025-08-05T03:58:15+00:00",
      "arxiv_id": "2508.03054v1",
      "url": "http://arxiv.org/pdf/2508.03054v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree",
      "abstract": "Large language models (LLMs) have shown great potential in the medical\ndomain. However, existing models still fall short when faced with complex\nmedical diagnosis task in the real world. This is mainly because they lack\nsufficient reasoning depth, which leads to information loss or logical jumps\nwhen processing a large amount of specialized medical data, leading to\ndiagnostic errors. To address these challenges, we propose Tree-of-Reasoning\n(ToR), a novel multi-agent framework designed to handle complex scenarios.\nSpecifically, ToR introduces a tree structure that can clearly record the\nreasoning path of LLMs and the corresponding clinical evidence. At the same\ntime, we propose a cross-validation mechanism to ensure the consistency of\nmulti-agent decision-making, thereby improving the clinical reasoning ability\nof multi-agents in complex medical scenarios. Experimental results on\nreal-world medical data show that our framework can achieve better performance\nthan existing baseline methods.",
      "authors": [
        "Qi Peng",
        "Jialin Cui",
        "Jiayuan Xie",
        "Yi Cai",
        "Qing Li"
      ],
      "published": "2025-08-05T03:31:28+00:00",
      "updated": "2025-08-05T03:31:28+00:00",
      "arxiv_id": "2508.03038v1",
      "url": "http://arxiv.org/pdf/2508.03038v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "From Text to Trajectories: GPT-2 as an ODE Solver via In-Context",
      "abstract": "In-Context Learning (ICL) has emerged as a new paradigm in large language\nmodels (LLMs), enabling them to perform novel tasks by conditioning on a few\nexamples embedded in the prompt. Yet, the highly nonlinear behavior of ICL for\nNLP tasks remains poorly understood. To shed light on its underlying\nmechanisms, this paper investigates whether LLMs can solve ordinary\ndifferential equations (ODEs) under the ICL setting. We formulate standard ODE\nproblems and their solutions as sequential prompts and evaluate GPT-2 models on\nthese tasks. Experiments on two types of ODEs show that GPT-2 can effectively\nlearn a meta-ODE algorithm, with convergence behavior comparable to, or better\nthan, the Euler method, and achieve exponential accuracy gains with increasing\nnumbers of demonstrations. Moreover, the model generalizes to\nout-of-distribution (OOD) problems, demonstrating robust extrapolation\ncapabilities. These empirical findings provide new insights into the mechanisms\nof ICL in NLP and its potential for solving nonlinear numerical problems.",
      "authors": [
        "Ziyang Ma",
        "Baojian Zhou",
        "Deqing Yang",
        "Yanghua Xiao"
      ],
      "published": "2025-08-05T03:16:37+00:00",
      "updated": "2025-08-05T03:16:37+00:00",
      "arxiv_id": "2508.03031v1",
      "url": "http://arxiv.org/pdf/2508.03031v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "KBest: Efficient Vector Search on Kunpeng CPU",
      "abstract": "Vector search, which returns the vectors most similar to a given query vector\nfrom a large vector dataset, underlies many important applications such as\nsearch, recommendation, and LLMs. To be economic, vector search needs to be\nefficient to reduce the resources required by a given query workload. However,\nexisting vector search libraries (e.g., Faiss and DiskANN) are optimized for\nx86 CPU architectures (i.e., Intel and AMD CPUs) while Huawei Kunpeng CPUs are\nbased on the ARM architecture and competitive in compute power. In this paper,\nwe present KBest as a vector search library tailored for the latest Kunpeng 920\nCPUs. To be efficient, KBest incorporates extensive hardware-aware and\nalgorithmic optimizations, which include single-instruction-multiple-data\n(SIMD) accelerated distance computation, data prefetch, index refinement, early\ntermination, and vector quantization. Experiment results show that KBest\noutperforms SOTA vector search libraries running on x86 CPUs, and our\noptimizations can improve the query throughput by over 2x. Currently, KBest\nserves applications from both our internal business and external enterprise\nclients with tens of millions of queries on a daily basis.",
      "authors": [
        "Kaihao Ma",
        "Meiling Wang",
        "Senkevich Oleg",
        "Zijian Li",
        "Daihao Xue",
        "Dmitriy Malyshev",
        "Yangming Lv",
        "Shihai Xiao",
        "Xiao Yan",
        "Radionov Alexander",
        "Weidi Zeng",
        "Yuanzhan Gao",
        "Zhiyu Zou",
        "Xin Yao",
        "Lin Liu",
        "Junhao Wu",
        "Yiding Liu",
        "Yaoyao Fu",
        "Gongyi Wang",
        "Gong Zhang",
        "Fei Yi",
        "Yingfan Liu"
      ],
      "published": "2025-08-05T02:52:15+00:00",
      "updated": "2025-08-07T03:48:34+00:00",
      "arxiv_id": "2508.03016v2",
      "url": "http://arxiv.org/pdf/2508.03016v2",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Survey of Large Language Models in Extended Reality: Technical Paradigms and Application Frontiers",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation, and their integration with\nExtended Reality (XR) is poised to transform how users interact with immersive\nenvironments. This survey provides a comprehensive review of recent\ndevelopments at the intersection of LLMs and XR, offering a structured\norganization of research along both technical and application dimensions. We\npropose a taxonomy of LLM-enhanced XR systems centered on key technical\nparadigms -- such as interactive agent control, XR development toolkits, and\ngenerative scene synthesis -- and discuss how these paradigms enable novel\ncapabilities in XR. In parallel, we examine how LLM-driven techniques support\npractical XR applications across diverse domains, including immersive\neducation, clinical healthcare, and industrial manufacturing. By connecting\nthese technical paradigms with application frontiers, our survey highlights\ncurrent trends, delineates design considerations, and identifies open\nchallenges in building LLM-augmented XR systems. This work provides insights\nthat can guide researchers and practitioners in advancing the state of the art\nin intelligent XR experiences.",
      "authors": [
        "Jingyan Wang",
        "Yang Zhao",
        "Haotian Mao",
        "Xubo Yang"
      ],
      "published": "2025-08-05T02:46:34+00:00",
      "updated": "2025-08-05T02:46:34+00:00",
      "arxiv_id": "2508.03014v1",
      "url": "http://arxiv.org/pdf/2508.03014v1",
      "categories": [
        "cs.HC",
        "I.2, H.5"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Tool-integrated Reinforcement Learning for Repo Deep Search",
      "abstract": "Issue localization, the process of identifying code locations that need\nmodification to resolve software issues, is a critical yet challenging task in\nsoftware development. The semantic gap between natural language issue\ndescriptions and faulty code requires complex multi-hop reasoning through code\ndependencies. Existing LLM-based agents attempt to address this by integrating\nrepository retrieval tools. However, this transforms issue localization into a\ndemanding task we call Repo Deep Search, which requires the LLM to effectively\nutilize various repository retrieval tools throughout a multi-step reasoning\nand navigation process. To tackle this challenge, we present ToolTrain, a\ntwo-stage tool-integrated training framework combining rejection-sampled\nsupervised fine-tuning and tool-integrated reinforcement learning to enhance\nLLMs' ability to use retrieval tools for issue localization. Experimental\nresults show that ToolTrain-trained models achieve state-of-the-art\nperformance, with our 32B model even surpassing Claude-3.7 on function-level\nlocalization. The results also show that improved localization performance\ntranslates to better end-to-end issue resolution performance. This further\ndemonstrates that training for issue localization is a viable and effective\nstrategy for improving automated software development.",
      "authors": [
        "Zexiong Ma",
        "Chao Peng",
        "Qunhong Zeng",
        "Pengfei Gao",
        "Yanzhen Zou",
        "Bing Xie"
      ],
      "published": "2025-08-05T02:44:21+00:00",
      "updated": "2025-08-06T03:43:30+00:00",
      "arxiv_id": "2508.03012v2",
      "url": "http://arxiv.org/pdf/2508.03012v2",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Enhancing Long Video Question Answering with Scene-Localized Frame Grouping",
      "abstract": "Current Multimodal Large Language Models (MLLMs) often perform poorly in long\nvideo understanding, primarily due to resource limitations that prevent them\nfrom processing all video frames and their associated information. Efficiently\nextracting relevant information becomes a challenging task. Existing frameworks\nand evaluation tasks focus on identifying specific frames containing core\nobjects from a large number of irrelevant frames, which does not align with the\npractical needs of real-world applications. To address this issue, we propose a\nnew scenario under the video question-answering task, SceneQA, which emphasizes\nscene-based detail perception and reasoning abilities. And we develop the LVSQA\ndataset to support the SceneQA task, which is built upon carefully selected\nvideos from LVBench and contains a new collection of question-answer pairs to\npromote a more fair evaluation of MLLMs' scene perception abilities in long\nvideos. Inspired by human cognition, we introduce a novel method called SLFG.\nThe core idea of SLFG is to combine individual frames into semantically\ncoherent scene frames. By leveraging scene localization methods and dynamic\nframe reassembly mechanisms, SLFG significantly enhances the understanding\ncapabilities of existing MLLMs in long videos. SLFG requires no modification to\nthe original model architecture and boasts excellent plug-and-play usability.\nExperimental results show that this method performs exceptionally well in\nseveral long video benchmark tests. Code and dataset will be released at\nhttp://www.slfg.pkuzwh.cn.",
      "authors": [
        "Xuyi Yang",
        "Wenhao Zhang",
        "Hongbo Jin",
        "Lin Liu",
        "Hongbo Xu",
        "Yongwei Nie",
        "Fei Yu",
        "Fei Ma"
      ],
      "published": "2025-08-05T02:28:58+00:00",
      "updated": "2025-08-05T02:28:58+00:00",
      "arxiv_id": "2508.03009v1",
      "url": "http://arxiv.org/pdf/2508.03009v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "GeoFlow: Agentic Workflow Automation for Geospatial Tasks",
      "abstract": "We present GeoFlow, a method that automatically generates agentic workflows\nfor geospatial tasks. Unlike prior work that focuses on reasoning decomposition\nand leaves API selection implicit, our method provides each agent with detailed\ntool-calling objectives to guide geospatial API invocation at runtime. GeoFlow\nincreases agentic success by 6.8% and reduces token usage by up to fourfold\nacross major LLM families compared to state-of-the-art approaches.",
      "authors": [
        "Amulya Bhattaram",
        "Justin Chung",
        "Stanley Chung",
        "Ranit Gupta",
        "Janani Ramamoorthy",
        "Kartikeya Gullapalli",
        "Diana Marculescu",
        "Dimitrios Stamoulis"
      ],
      "published": "2025-08-05T02:14:58+00:00",
      "updated": "2025-08-05T02:14:58+00:00",
      "arxiv_id": "2508.04719v1",
      "url": "http://arxiv.org/pdf/2508.04719v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "SustainableQA: A Comprehensive Question Answering Dataset for Corporate Sustainability and EU Taxonomy Reporting",
      "abstract": "The growing demand for corporate sustainability transparency, particularly\nunder new regulations like the EU Taxonomy, necessitates precise data\nextraction from large, unstructured corporate reports. Large Language Models\n(LLMs) and Retrieval-Augmented Generation (RAG) systems, requires high-quality,\ndomain-specific question-answering (QA) datasets to excel at particular\ndomains. To address this, we introduce SustainableQA, a novel dataset and a\nscalable pipeline for generating a comprehensive QA datasets from corporate\nsustainability reports and annual reports. Our approach integrates semantic\nchunk classification, a hybrid span extraction pipeline combining fine-tuned\nNamed Entity Recognition (NER), rule-based methods, and LLM-driven refinement,\nalongside a specialized table-to-paragraph transformation. With over 195,000\ndiverse factoid and non-factoid QA pairs, SustainableQA is an effective\nresource for developing and benchmarking advanced knowledge assistants capable\nof navigating complex sustainability compliance",
      "authors": [
        "Mohammed Ali",
        "Abdelrahman Abdallah",
        "Adam Jatowt"
      ],
      "published": "2025-08-05T02:03:59+00:00",
      "updated": "2025-08-05T02:03:59+00:00",
      "arxiv_id": "2508.03000v1",
      "url": "http://arxiv.org/pdf/2508.03000v1",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots",
      "abstract": "AGENTiGraph is a user-friendly, agent-driven system that enables intuitive\ninteraction and management of domain-specific data through the manipulation of\nknowledge graphs in natural language. It gives non-technical users a complete,\nvisual solution to incrementally build and refine their knowledge bases,\nallowing multi-round dialogues and dynamic updates without specialized query\nlanguages. The flexible design of AGENTiGraph, including intent classification,\ntask planning, and automatic knowledge integration, ensures seamless reasoning\nbetween diverse tasks. Evaluated on a 3,500-query benchmark within an\neducational scenario, the system outperforms strong zero-shot baselines\n(achieving 95.12% classification accuracy, 90.45% execution success),\nindicating potential scalability to compliance-critical or multi-step queries\nin legal and medical domains, e.g., incorporating new statutes or research on\nthe fly. Our open-source demo offers a powerful new paradigm for multi-turn\nenterprise knowledge management that bridges LLMs and structured graphs.",
      "authors": [
        "Xinjie Zhao",
        "Moritz Blum",
        "Fan Gao",
        "Yingjian Chen",
        "Boming Yang",
        "Luis Marquez-Carpintero",
        "MÃ³nica Pina-Navarro",
        "Yanran Fu",
        "So Morikawa",
        "Yusuke Iwasawa",
        "Yutaka Matsuo",
        "Chanjun Park",
        "Irene Li"
      ],
      "published": "2025-08-05T01:55:06+00:00",
      "updated": "2025-08-05T01:55:06+00:00",
      "arxiv_id": "2508.02999v1",
      "url": "http://arxiv.org/pdf/2508.02999v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "MRG-Bench: Evaluating and Exploring the Requirements of Context for Repository-Level Code Generation",
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode generation. However, current evaluation datasets suffer from issues such\nas the lack of runnable test cases, deviation from the distribution of\nreal-world code, and the ability to evaluate only the Python language. These\nlimitations undermine the credibility of the evaluation results.\n  To address these limitations, we introduce \\textbf{MRG-Bench} (Multi-language\nRepository-level Code Generation Benchmark), a novel dataset that provides a\nmore accurate evaluation of LLMs in practical repository-level code generation\ntasks. MRG-Bench has three main features: (1) practical data sourced from\nreal-world code repositories that align to the practical distribution, (2)\nmultiple programming languages support, including Python, Java, and Go, and (3)\nproject-level runnable test cases to assess the quality of the generated code.\n  Based on MRG-Bench, we conducted extensive experiments including large\nlanguage models, long-context models, and RAG-related methods. These evaluation\nresults demonstrate that \\textbf{current repository-level code generation\ntechniques suffer from significant performance deficiencies}. To further\ninvestigate why models fail, we designed novel experiments to annotate the\nunderlying causes of generation errors. The results explicitly show that the\nmajority of methods suffer from \"\\textbf{difficulty in understanding user\nrequirements},\" failing to comprehend their assigned tasks accurately.\nMoreover, the impact of different repository-level contexts on this issue\nexhibits significant disparities across different programming languages,\nsuggesting that, in practice, specialized contextual information needs to be\ndesigned for different languages.",
      "authors": [
        "Haiyang Li"
      ],
      "published": "2025-08-05T01:53:45+00:00",
      "updated": "2025-08-05T01:53:45+00:00",
      "arxiv_id": "2508.02998v1",
      "url": "http://arxiv.org/pdf/2508.02998v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors",
      "abstract": "The widespread use of Large Language Models (LLMs) in many applications marks\na significant advance in research and practice. However, their complexity and\nhard-to-understand nature make them vulnerable to attacks, especially\njailbreaks designed to produce harmful responses. To counter these threats,\ndeveloping strong detection methods is essential for the safe and reliable use\nof LLMs. This paper studies this detection problem using the Contextual\nCo-occurrence Matrix, a structure recognized for its efficacy in data-scarce\nenvironments. We propose a novel method leveraging the latent space\ncharacteristics of Contextual Co-occurrence Matrices and Tensors for the\neffective identification of adversarial and jailbreak prompts. Our evaluations\nshow that this approach achieves a notable F1 score of 0.83 using only 0.5% of\nlabeled prompts, which is a 96.6% improvement over baselines. This result\nhighlights the strength of our learned patterns, especially when labeled data\nis scarce. Our method is also significantly faster, speedup ranging from 2.3 to\n128.4 times compared to the baseline models. To support future research and\nreproducibility, we have made our implementation publicly available.",
      "authors": [
        "Sri Durga Sai Sowmya Kadali",
        "Evangelos E. Papalexakis"
      ],
      "published": "2025-08-05T01:53:32+00:00",
      "updated": "2025-08-06T00:30:43+00:00",
      "arxiv_id": "2508.02997v2",
      "url": "http://arxiv.org/pdf/2508.02997v2",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation",
      "abstract": "The specification of prior distributions is fundamental in Bayesian\ninference, yet it remains a significant bottleneck. The prior elicitation\nprocess is often a manual, subjective, and unscalable task. We propose a novel\nframework which leverages Large Language Models (LLMs) to automate and scale\nthis process. We introduce \\texttt{LLMPrior}, a principled operator that\ntranslates rich, unstructured contexts such as natural language descriptions,\ndata or figures into valid, tractable probability distributions. We formalize\nthis operator by architecturally coupling an LLM with an explicit, tractable\ngenerative model, such as a Gaussian Mixture Model (forming a LLM based Mixture\nDensity Network), ensuring the resulting prior satisfies essential mathematical\nproperties. We further extend this framework to multi-agent systems where\nLogarithmic Opinion Pooling is employed to aggregate prior distributions\ninduced by decentralized knowledge. We present the federated prior aggregation\nalgorithm, \\texttt{Fed-LLMPrior}, for aggregating distributed,\ncontext-dependent priors in a manner robust to agent heterogeneity. This work\nprovides the foundation for a new class of tools that can potentially lower the\nbarrier to entry for sophisticated Bayesian modeling.",
      "authors": [
        "Yongchao Huang"
      ],
      "published": "2025-08-05T01:43:29+00:00",
      "updated": "2025-08-05T01:43:29+00:00",
      "arxiv_id": "2508.03766v1",
      "url": "http://arxiv.org/pdf/2508.03766v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs",
      "abstract": "As large language models (LLMs) grow in capability and autonomy, evaluating\ntheir outputs-especially in open-ended and complex tasks-has become a critical\nbottleneck. A new paradigm is emerging: using AI agents as the evaluators\nthemselves. This \"agent-as-a-judge\" approach leverages the reasoning and\nperspective-taking abilities of LLMs to assess the quality and safety of other\nmodels, promising calable and nuanced alternatives to human evaluation. In this\nreview, we define the agent-as-a-judge concept, trace its evolution from\nsingle-model judges to dynamic multi-agent debate frameworks, and critically\nexamine their strengths and shortcomings. We compare these approaches across\nreliability, cost, and human alignment, and survey real-world deployments in\ndomains such as medicine, law, finance, and education. Finally, we highlight\npressing challenges-including bias, robustness, and meta evaluation-and outline\nfuture research directions. By bringing together these strands, our review\ndemonstrates how agent-based judging can complement (but not replace) human\noversight, marking a step toward trustworthy, scalable evaluation for\nnext-generation LLMs.",
      "authors": [
        "Fangyi Yu"
      ],
      "published": "2025-08-05T01:42:25+00:00",
      "updated": "2025-08-05T01:42:25+00:00",
      "arxiv_id": "2508.02994v1",
      "url": "http://arxiv.org/pdf/2508.02994v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling",
      "abstract": "The proliferation of tool-augmented Large Language Models (LLMs) has created\na fragmented ecosystem where developers must navigate multiple protocols,\nmanual schema definitions, and complex execution workflows. We address this\nchallenge by proposing a unified approach to tool integration that abstracts\nprotocol differences while optimizing execution performance. Our solution\ndemonstrates how protocol-agnostic design principles can significantly reduce\ndevelopment overhead through automated schema generation, dual-mode concurrent\nexecution, and seamless multi-source tool management. Experimental results show\n60-80% code reduction across integration scenarios, performance improvements up\nto 3.1x through optimized concurrency, and full compatibility with existing\nfunction calling standards. This work contributes both theoretical insights\ninto tool integration architecture and practical solutions for real-world LLM\napplication development.",
      "authors": [
        "Peng Ding",
        "Rick Stevens"
      ],
      "published": "2025-08-05T01:06:49+00:00",
      "updated": "2025-08-05T01:06:49+00:00",
      "arxiv_id": "2508.02979v1",
      "url": "http://arxiv.org/pdf/2508.02979v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Robot builds a robot's brain: AI generated drone command and control station hosted in the sky",
      "abstract": "Advances in artificial intelligence (AI) including large language models\n(LLMs) and hybrid reasoning models present an opportunity to reimagine how\nautonomous robots such as drones are designed, developed, and validated. Here,\nwe demonstrate a fully AI-generated drone control system: with minimal human\ninput, an artificial intelligence (AI) model authored all the code for a\nreal-time, self-hosted drone command and control platform, which was deployed\nand demonstrated on a real drone in flight as well as a simulated virtual drone\nin the cloud. The system enables real-time mapping, flight telemetry,\nautonomous mission planning and execution, and safety protocolsall orchestrated\nthrough a web interface hosted directly on the drone itself. Not a single line\nof code was written by a human. We quantitatively benchmark system performance,\ncode complexity, and development speed against prior, human-coded\narchitectures, finding that AI-generated code can deliver functionally complete\ncommand-and-control stacks at orders-of-magnitude faster development cycles,\nthough with identifiable current limitations related to specific model context\nwindow and reasoning depth. Our analysis uncovers the practical boundaries of\nAI-driven robot control code generation at current model scales, as well as\nemergent strengths and failure modes in AI-generated robotics code. This work\nsets a precedent for the autonomous creation of robot control systems and, more\nbroadly, suggests a new paradigm for robotics engineeringone in which future\nrobots may be largely co-designed, developed, and verified by artificial\nintelligence. In this initial work, a robot built a robot's brain.",
      "authors": [
        "Peter Burke"
      ],
      "published": "2025-08-04T23:53:01+00:00",
      "updated": "2025-08-04T23:53:01+00:00",
      "arxiv_id": "2508.02962v1",
      "url": "http://arxiv.org/pdf/2508.02962v1",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "Defend LLMs Through Self-Consciousness",
      "abstract": "This paper introduces a novel self-consciousness defense mechanism for Large\nLanguage Models (LLMs) to combat prompt injection attacks. Unlike traditional\napproaches that rely on external classifiers, our method leverages the LLM's\ninherent reasoning capabilities to perform self-protection. We propose a\nframework that incorporates Meta-Cognitive and Arbitration Modules, enabling\nLLMs to evaluate and regulate their own outputs autonomously. Our approach is\nevaluated on seven state-of-the-art LLMs using two datasets: AdvBench and\nPrompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate\nsignificant improvements in defense success rates across models and datasets,\nwith some achieving perfect and near-perfect defense in Enhanced Mode. We also\nanalyze the trade-off between defense success rate improvement and\ncomputational overhead. This self-consciousness method offers a lightweight,\ncost-effective solution for enhancing LLM ethics, particularly beneficial for\nGenAI use cases across various platforms.",
      "authors": [
        "Boshi Huang",
        "Fabio Nonato de Paula"
      ],
      "published": "2025-08-04T23:52:15+00:00",
      "updated": "2025-08-04T23:52:15+00:00",
      "arxiv_id": "2508.02961v1",
      "url": "http://arxiv.org/pdf/2508.02961v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow",
      "abstract": "Large language models (LLMs) excel at solving complex tasks by executing\nagentic workflows composed of detailed instructions and structured operations.\nYet, building general-purpose agents by manually embedding foundation models\ninto agentic systems such as Chain-of-Thought, Self-Reflection, and ReACT\nthrough text interfaces limits scalability and efficiency. Recently, many\nresearchers have sought to automate the generation and optimization of these\nworkflows through code-based representations. However, existing methods often\nrely on labeled datasets to train and optimize workflows, making them\nineffective and inflexible for solving real-world, dynamic problems where\nlabeled data is unavailable. To address this challenge, we introduce Polymath,\na self-optimizing agent with dynamic hierarchical workflow that leverages the\nflexibility of task flow graphs and the expressiveness of code-represented\nworkflows to solve a wide range of real-world, dynamic problems. The proposed\noptimization methodology integrates multi-grid-inspired graph optimization with\na self-reflection-guided evolutionary algorithm to refine workflows without\nlabeled data. Experimental results on six benchmark datasets across coding,\nmath, and multi-turn QA tasks show that Polymath achieves 8.1% average\nimprovement over state-of-the-art baselines.",
      "authors": [
        "Chia-Tung Ho",
        "Jing Gong",
        "Xufeng Yao",
        "Yunsheng Bai",
        "Abhishek B Akkur",
        "Haoxing Ren"
      ],
      "published": "2025-08-04T23:50:02+00:00",
      "updated": "2025-08-07T01:30:51+00:00",
      "arxiv_id": "2508.02959v2",
      "url": "http://arxiv.org/pdf/2508.02959v2",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People",
      "abstract": "Virtual Reality (VR) is inaccessible to blind people. While research has\ninvestigated many techniques to enhance VR accessibility, they require\nadditional developer effort to integrate. As such, most mainstream VR apps\nremain inaccessible as the industry de-prioritizes accessibility. We present\nVRSight, an end-to-end system that recognizes VR scenes post hoc through a set\nof AI models (e.g., object detection, depth estimation, LLM-based atmosphere\ninterpretation) and generates tone-based, spatial audio feedback, empowering\nblind users to interact in VR without developer intervention. To enable virtual\nelement detection, we further contribute DISCOVR, a VR dataset consisting of 30\nvirtual object classes from 17 social VR apps, substituting real-world datasets\nthat remain not applicable to VR contexts. Nine participants used VRSight to\nexplore an off-the-shelf VR app (Rec Room), demonstrating its effectiveness in\nfacilitating social tasks like avatar awareness and available seat\nidentification.",
      "authors": [
        "Daniel Killough",
        "Justin Feng",
        "Zheng Xue \"ZX\" Ching",
        "Daniel Wang",
        "Rithvik Dyava",
        "Yapeng Tian",
        "Yuhang Zhao"
      ],
      "published": "2025-08-04T23:50:00+00:00",
      "updated": "2025-08-04T23:50:00+00:00",
      "arxiv_id": "2508.02958v1",
      "url": "http://arxiv.org/pdf/2508.02958v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "LLM-based IR-system for Bank Supervisors",
      "abstract": "Bank supervisors face the complex task of ensuring that new measures are\nconsistently aligned with historical precedents. To address this challenge, we\nintroduce a novel Information Retrieval (IR) System tailored to assist\nsupervisors in drafting both consistent and effective measures. This system\ningests findings from on-site investigations. It then retrieves the most\nrelevant historical findings and their associated measures from a comprehensive\ndatabase, providing a solid basis for supervisors to write well-informed\nmeasures for new findings. Utilizing a blend of lexical, semantic, and Capital\nRequirements Regulation (CRR) fuzzy set matching techniques, the IR system\nensures the retrieval of findings that closely align with current cases. The\nperformance of this system, particularly in scenarios with partially labeled\ndata, is validated through a Monte Carlo methodology, showcasing its robustness\nand accuracy. Enhanced by a Transformer-based Denoising AutoEncoder for\nfine-tuning, the final model achieves a Mean Average Precision (MAP@100) of\n0.83 and a Mean Reciprocal Rank (MRR@100) of 0.92. These scores surpass those\nof both standalone lexical models such as BM25 and semantic BERT-like models.",
      "authors": [
        "Ilias Aarab"
      ],
      "published": "2025-08-04T23:02:01+00:00",
      "updated": "2025-08-04T23:02:01+00:00",
      "arxiv_id": "2508.02945v1",
      "url": "http://arxiv.org/pdf/2508.02945v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "stat.AP",
        "stat.CO",
        "68P20, 68T50, 68T05, 62P20, 91G80",
        "H.3.3; I.2.6; I.2.7; J.1"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "AQUAH: Automatic Quantification and Unified Agent in Hydrology",
      "abstract": "We introduce AQUAH, the first end-to-end language-based agent designed\nspecifically for hydrologic modeling. Starting from a simple natural-language\nprompt (e.g., 'simulate floods for the Little Bighorn basin from 2020 to\n2022'), AQUAH autonomously retrieves the required terrain, forcing, and gauge\ndata; configures a hydrologic model; runs the simulation; and generates a\nself-contained PDF report. The workflow is driven by vision-enabled large\nlanguage models, which interpret maps and rasters on the fly and steer key\ndecisions such as outlet selection, parameter initialization, and uncertainty\ncommentary. Initial experiments across a range of U.S. basins show that AQUAH\ncan complete cold-start simulations and produce analyst-ready documentation\nwithout manual intervention. The results are judged by hydrologists as clear,\ntransparent, and physically plausible. While further calibration and validation\nare still needed for operational deployment, these early outcomes highlight the\npromise of LLM-centered, vision-grounded agents to streamline complex\nenvironmental modeling and lower the barrier between Earth observation data,\nphysics-based tools, and decision makers.",
      "authors": [
        "Songkun Yan",
        "Zhi Li",
        "Siyu Zhu",
        "Yixin Wen",
        "Mofan Zhang",
        "Mengye Chen",
        "Jie Cao",
        "Yang Hong"
      ],
      "published": "2025-08-04T22:26:50+00:00",
      "updated": "2025-08-04T22:26:50+00:00",
      "arxiv_id": "2508.02936v1",
      "url": "http://arxiv.org/pdf/2508.02936v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "PLoRA: Efficient LoRA Hyperparameter Tuning for Large Models",
      "abstract": "Low-rank Adaptation (LoRA) has gained popularity as a fine-tuning approach\nfor Large Language Models (LLMs) due to its low resource requirements and good\nperformance. While a plethora of work has investigated improving LoRA serving\nefficiency by serving multiple LoRAs concurrently, existing methods assume that\na wide range of LoRA adapters are available for serving. In our work, we\nconduct extensive empirical studies to identify that current training paradigms\ndo not utilize hardware resources efficiently and require high overhead to\nobtain a performant LoRA. Leveraging these insights, we propose PLoRA, which\nautomatically orchestrates concurrent LoRA fine-tuning jobs under given\nhardware and model constraints and develops performant kernels to improve\ntraining efficiency. Our experimental studies show that PLoRA reduces the\nmakespan of LoRA fine-tuning over a given hyperparameter search space by up to\n7.52x and improves training throughput by up to 12.8x across a range of\nstate-of-the-art LLMs.",
      "authors": [
        "Minghao Yan",
        "Zhuang Wang",
        "Zhen Jia",
        "Shivaram Venkataraman",
        "Yida Wang"
      ],
      "published": "2025-08-04T22:11:34+00:00",
      "updated": "2025-08-04T22:11:34+00:00",
      "arxiv_id": "2508.02932v1",
      "url": "http://arxiv.org/pdf/2508.02932v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Can LLMs Generate High-Quality Task-Specific Conversations?",
      "abstract": "This paper introduces a parameterization framework for controlling\nconversation quality in large language models. We explore nine key parameters\nacross six dimensions that enable precise specification of dialogue properties.\nThrough experiments with state-of-the-art LLMs, we demonstrate that\nparameter-based control produces statistically significant differences in\ngenerated conversation properties. Our approach addresses challenges in\nconversation generation, including topic coherence, knowledge progression,\ncharacter consistency, and control granularity. The framework provides a\nstandardized method for conversation quality control with applications in\neducation, therapy, customer service, and entertainment. Future work will focus\non implementing additional parameters through architectural modifications and\ndeveloping benchmark datasets for evaluation.",
      "authors": [
        "Shengqi Li",
        "Amarnath Gupta"
      ],
      "published": "2025-08-04T22:07:08+00:00",
      "updated": "2025-08-04T22:07:08+00:00",
      "arxiv_id": "2508.02931v1",
      "url": "http://arxiv.org/pdf/2508.02931v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "GrandJury: A Collaborative Machine Learning Model Evaluation Protocol for Dynamic Quality Rubrics",
      "abstract": "Generative Machine Learning models have become central to modern systems,\npowering applications in creative writing, summarization, multi-hop reasoning,\nand context-aware dialogue. These models underpin large-scale AI assistants,\nworkflow automation, and autonomous decision-making. In such domains,\nacceptable response is rarely absolute or static, but plural and highly\ncontext-dependent. Yet standard evaluation regimes still rely on static,\nbenchmark-style tests, incentivizing optimization toward leaderboard scores\nrather than alignment with dynamic user needs or evolving realities. GrandJury\nintroduces a formal evaluation protocol combining time-decayed aggregation,\ncomplete traceability, with the support of dynamic, transparent task rubric\nattribution, and multi-rater human judgment. Together, these elements enable\npluralistic, accountable evaluation that captures evolving consensus and\nsurfaces disagreement. We provide an open-source implementation (grandjury PyPI\npackage) and a public collection of Large Language Model (LLM) inference\noutputs to illustrate the need and method. GrandJury provides a new paradigm\nfor AI practitioners when evaluating machine learning outputs without absolute\nground truth.",
      "authors": [
        "Arthur Cho"
      ],
      "published": "2025-08-04T22:00:44+00:00",
      "updated": "2025-08-06T19:57:38+00:00",
      "arxiv_id": "2508.02926v2",
      "url": "http://arxiv.org/pdf/2508.02926v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "I.2.6; I.2.7"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "PentestJudge: Judging Agent Behavior Against Operational Requirements",
      "abstract": "We introduce PentestJudge, a system for evaluating the operations of\npenetration testing agents. PentestJudge is a large language model\n(LLM)-as-judge with access to tools that allow it to consume arbitrary\ntrajectories of agent states and tool call history to determine whether a\nsecurity agent's actions meet certain operating criteria that would be\nimpractical to evaluate programmatically. We develop rubrics that use a tree\nstructure to hierarchically collapse the penetration testing task for a\nparticular environment into smaller, simpler, and more manageable sub-tasks and\ncriteria until each leaf node represents simple yes-or-no criteria for\nPentestJudge to evaluate. Task nodes are broken down into different categories\nrelated to operational objectives, operational security, and tradecraft.\nLLM-as-judge scores are compared to human domain experts as a ground-truth\nreference, allowing us to compare their relative performance with standard\nbinary classification metrics, such as F1 scores. We evaluate several frontier\nand open-source models acting as judge agents, with the best model reaching an\nF1 score of 0.83. We find models that are better at tool-use perform more\nclosely to human experts. By stratifying the F1 scores by requirement type, we\nfind even models with similar overall scores struggle with different types of\nquestions, suggesting certain models may be better judges of particular\noperating criteria. We find that weaker and cheaper models can judge the\ntrajectories of pentests performed by stronger and more expensive models,\nsuggesting verification may be easier than generation for the penetration\ntesting task. We share this methodology to facilitate future research in\nunderstanding the ability of judges to holistically and scalably evaluate the\nprocess quality of AI-based information security agents so that they may be\nconfidently used in sensitive production environments.",
      "authors": [
        "Shane Caldwell",
        "Max Harley",
        "Michael Kouremetis",
        "Vincent Abruzzo",
        "Will Pearce"
      ],
      "published": "2025-08-04T21:52:50+00:00",
      "updated": "2025-08-04T21:52:50+00:00",
      "arxiv_id": "2508.02921v1",
      "url": "http://arxiv.org/pdf/2508.02921v1",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Enhancing Japanese Large Language Models with Reasoning Vectors",
      "abstract": "Post-training methods have improved the performance and enhanced the\nreasoning capability for mainstream large language models (LLMs), but the same\nis challenging for Japanese LLMs to achieve due to the amount of resources\nrequired. Inspired by task vectors that extract the change of weights before\nand after training, specifically for a certain task, we obtain reasoning\nvectors from reasoning LLMs and apply them to Japanese LLMs to boost their\nperformance. While the resources available present a challenge to improve\nJapanese LLMs, we present a simple and effective way to obtain high improvement\nand hope to inspire for other languages.",
      "authors": [
        "Carolina Minami Oguchi",
        "Leo Wei",
        "Koyo Kobayashi",
        "Hsin-Tai Wu",
        "Dipak Ghosal"
      ],
      "published": "2025-08-04T21:31:20+00:00",
      "updated": "2025-08-04T21:31:20+00:00",
      "arxiv_id": "2508.02913v1",
      "url": "http://arxiv.org/pdf/2508.02913v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations",
      "abstract": "Sensorial language -- the language connected to our senses including vision,\nsound, touch, taste, smell, and interoception, plays a fundamental role in how\nwe communicate experiences and perceptions. We explore the relationship between\nsensorial language and traditional stylistic features, like those measured by\nLIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate\nthat low-dimensional latent representations of LIWC features r = 24 effectively\ncapture stylistic information for sensorial language prediction compared to the\nfull feature set (r = 74). We introduce Stylometrically Lean Interpretable\nModels (SLIM-LLMs), which model non-linear relationships between these style\ndimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features\nmatch the performance of full-scale language models while reducing parameters\nby up to 80%.",
      "authors": [
        "Osama Khalid",
        "Sanvesh Srivastava",
        "Padmini Srinivasan"
      ],
      "published": "2025-08-04T21:02:12+00:00",
      "updated": "2025-08-04T21:02:12+00:00",
      "arxiv_id": "2508.02901v1",
      "url": "http://arxiv.org/pdf/2508.02901v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Seemingly Simple Planning Problems are Computationally Challenging: The Countdown Game",
      "abstract": "There is a broad consensus that the inability to form long-term plans is one\nof the key limitations of current foundational models and agents. However, the\nexisting planning benchmarks remain woefully inadequate to truly measure their\nplanning capabilities. Most existing benchmarks either focus on loosely defined\ntasks like travel planning or end up leveraging existing domains and problems\nfrom international planning competitions. While the former tasks are hard to\nformalize and verify, the latter were specifically designed to test and\nchallenge the weaknesses of existing automated planners. To address these\nshortcomings, we propose a procedure for creating a planning benchmark centered\naround the game called Countdown, where a player is expected to form a target\nnumber from a list of input numbers through arithmetic operations. We discuss\nhow this problem meets many of the desiderata associated with an ideal\nbenchmark for planning capabilities evaluation. Specifically, the domain allows\nfor an intuitive, natural language description for each problem instance, it is\ncomputationally challenging (NP-complete), and the instance space is rich\nenough that we do not have to worry about memorization. We perform an extensive\ntheoretical analysis, establishing the computational complexity result and\ndemonstrate the advantage of our instance generation procedure over public\nbenchmarks. We evaluate a variety of existing LLM-assisted planning methods on\ninstances generated using our procedure. Our results show that, unlike other\ndomains like 24 Game (a special case of Countdown), our proposed dynamic\nbenchmark remains extremely challenging for existing LLM-based approaches.",
      "authors": [
        "Michael Katz",
        "Harsha Kokel",
        "Sarath Sreedharan"
      ],
      "published": "2025-08-04T21:01:03+00:00",
      "updated": "2025-08-04T21:01:03+00:00",
      "arxiv_id": "2508.02900v1",
      "url": "http://arxiv.org/pdf/2508.02900v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models",
      "abstract": "Despite significant advancements, current large language models (LLMs) and\nvision-language models (LVLMs) continue to struggle with complex, multi-step,\ncross-modal common sense reasoning tasks, often exhibiting a lack of\n\"deliberative thinking.\" They tend to rely on superficial associations rather\nthan deep, chained inference, particularly when integrating visual information\nwith abstract concepts. To address this, we propose the Coherent Multimodal\nReasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense\nreasoning capabilities through an iterative, self-evaluating inference\nmechanism. CMRF mimics human problem-solving by decomposing complex queries,\ngenerating step-by-step inferences, and self-correcting errors. Our framework\nintegrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking\ndown problems into sub-questions, a Contextual Inference Engine (CIE) for\ncontextual inference, and a Coherence Assessment Module (CAM) for evaluating\nlogical consistency and confidence. Coupled with an Adaptive Iterative\nRefinement strategy, CMRF systematically refines its reasoning paths. Built\nupon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning\n(MDAR) dataset, CMRF achieves state-of-the-art performance among open-source\nLVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It\nattains an average accuracy of 69.4%, surpassing the best open-source baseline\nby +2.4 percentage points, with particular strength in complex reasoning\nscenarios. Extensive ablation studies and human evaluations confirm the\ncritical contributions of each module and the effectiveness of iterative\nrefinement in fostering more coherent and accurate reasoning.",
      "authors": [
        "Wenjie Luo",
        "Ruocheng Li",
        "Shanshan Zhu",
        "Julian Perry"
      ],
      "published": "2025-08-04T20:33:58+00:00",
      "updated": "2025-08-04T20:33:58+00:00",
      "arxiv_id": "2508.02886v1",
      "url": "http://arxiv.org/pdf/2508.02886v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Highlight & Summarize: RAG without the jailbreaks",
      "abstract": "Preventing jailbreaking and model hijacking of Large Language Models (LLMs)\nis an important yet challenging task. For example, when interacting with a\nchatbot, malicious users can input specially crafted prompts to cause the LLM\nto generate undesirable content or perform a completely different task from its\nintended purpose. Existing mitigations for such attacks typically rely on\nhardening the LLM's system prompt or using a content classifier trained to\ndetect undesirable content or off-topic conversations. However, these\nprobabilistic approaches are relatively easy to bypass due to the very large\nspace of possible inputs and undesirable outputs. In this paper, we present and\nevaluate Highlight & Summarize (H&S), a new design pattern for\nretrieval-augmented generation (RAG) systems that prevents these attacks by\ndesign. The core idea is to perform the same task as a standard RAG pipeline\n(i.e., to provide natural language answers to questions, based on relevant\nsources) without ever revealing the user's question to the generative LLM. This\nis achieved by splitting the pipeline into two components: a highlighter, which\ntakes the user's question and extracts relevant passages (\"highlights\") from\nthe retrieved documents, and a summarizer, which takes the highlighted passages\nand summarizes them into a cohesive answer. We describe several possible\ninstantiations of H&S and evaluate their generated responses in terms of\ncorrectness, relevance, and response quality. Surprisingly, when using an\nLLM-based highlighter, the majority of H&S responses are judged to be better\nthan those of a standard RAG pipeline.",
      "authors": [
        "Giovanni Cherubin",
        "Andrew Paverd"
      ],
      "published": "2025-08-04T20:01:00+00:00",
      "updated": "2025-08-04T20:01:00+00:00",
      "arxiv_id": "2508.02872v1",
      "url": "http://arxiv.org/pdf/2508.02872v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows",
      "abstract": "Foundation models, such as Large Language Models (LLMs), are increasingly\nused as core components of AI agents in complex, large-scale workflows across\nfederated and heterogeneous environments. In agentic workflows, autonomous\nagents plan tasks, interact with humans and peers, and shape scientific\noutcomes. This makes transparency, traceability, reproducibility, and\nreliability essential. However, AI-based agents can hallucinate or reason\nincorrectly, and their decisions may propagate errors through the workflow,\nespecially when one agent's output feeds into another's input. Therefore,\nfine-grained provenance is essential to link agent decisions, their end-to-end\ncontext, and downstream impacts. While provenance techniques have long\nsupported reproducibility and workflow data understanding, they fail to capture\nand relate agent-centric metadata (prompts, responses, and decisions) with the\nrest of the workflow. In this paper, we introduce PROV-AGENT, a provenance\nmodel that extends W3C PROV and leverages the Model Context Protocol (MCP) to\nintegrate agent interactions into end-to-end workflow provenance. Our\ncontributions include: (1) a provenance model tailored for agentic workflows,\n(2) a near real-time, open-source system for capturing agentic provenance, and\n(3) a cross-facility evaluation spanning edge, cloud, and HPC environments,\ndemonstrating support for critical provenance queries and agent reliability\nanalysis.",
      "authors": [
        "Renan Souza",
        "Amal Gueroudji",
        "Stephen DeWitt",
        "Daniel Rosendo",
        "Tirthankar Ghosal",
        "Robert Ross",
        "Prasanna Balaprakash",
        "Rafael Ferreira da Silva"
      ],
      "published": "2025-08-04T19:54:40+00:00",
      "updated": "2025-08-04T19:54:40+00:00",
      "arxiv_id": "2508.02866v1",
      "url": "http://arxiv.org/pdf/2508.02866v1",
      "categories": [
        "cs.DC",
        "cs.DB",
        "68T42, 68T30, 68P20, 68Q85, 68M14,",
        "D.2.12; H.2.4; I.2.11; C.2.4; H.3.4"
      ],
      "primary_category": "cs.DC"
    },
    {
      "title": "Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives",
      "abstract": "We present an approach to modeling annotator disagreement in subjective NLP\ntasks through both architectural and data-centric innovations. Our model,\nDEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert\nsubnetworks based on annotator demographics, enabling it to better represent\nstructured, group-level variation compared to prior models. DEM-MoE\nconsistently performs competitively across demographic groups, and shows\nespecially strong results on datasets with high annotator disagreement. To\naddress sparse demographic coverage, we test whether LLM-generated synthetic\nannotations via zero-shot persona prompting can be used for data imputation. We\nshow these synthetic judgments align moderately well with human annotations on\nour data and offer a scalable way to potentially enrich training data. We then\npropose and evaluate approaches for blending real and synthetic data using\nstrategies tailored to dataset structure. We find that the optimal strategies\ndepend on dataset structure. Together, these contributions improve the\nrepresentation of diverse perspectives.",
      "authors": [
        "Yinuo Xu",
        "Veronica Derricks",
        "Allison Earl",
        "David Jurgens"
      ],
      "published": "2025-08-04T19:27:17+00:00",
      "updated": "2025-08-04T19:27:17+00:00",
      "arxiv_id": "2508.02853v1",
      "url": "http://arxiv.org/pdf/2508.02853v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering",
      "abstract": "Radiology visual question answering (RVQA) provides precise answers to\nquestions about chest X-ray images, alleviating radiologists' workload. While\nrecent methods based on multimodal large language models (MLLMs) and\nretrieval-augmented generation (RAG) have shown promising progress in RVQA,\nthey still face challenges in factual accuracy, hallucinations, and cross-modal\nmisalignment. We introduce a multi-agent system (MAS) designed to support\ncomplex reasoning in RVQA, with specialized agents for context understanding,\nmultimodal reasoning, and answer validation. We evaluate our system on a\nchallenging RVQA set curated via model disagreement filtering, comprising\nconsistently hard cases across multiple MLLMs. Extensive experiments\ndemonstrate the superiority and effectiveness of our system over strong MLLM\nbaselines, with a case study illustrating its reliability and interpretability.\nThis work highlights the potential of multi-agent approaches to support\nexplainable and trustworthy clinical AI applications that require complex\nreasoning.",
      "authors": [
        "Ziruo Yi",
        "Jinyu Liu",
        "Ting Xiao",
        "Mark V. Albert"
      ],
      "published": "2025-08-04T19:09:52+00:00",
      "updated": "2025-08-04T19:09:52+00:00",
      "arxiv_id": "2508.02841v1",
      "url": "http://arxiv.org/pdf/2508.02841v1",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Defending Against Knowledge Poisoning Attacks During Retrieval-Augmented Generation",
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to\nboost the capabilities of large language models (LLMs) by incorporating\nexternal, up-to-date knowledge sources. However, this introduces a potential\nvulnerability to knowledge poisoning attacks, where attackers can compromise\nthe knowledge source to mislead the generation model. One such attack is the\nPoisonedRAG in which the injected adversarial texts steer the model to generate\nan attacker-chosen response to a target question. In this work, we propose\nnovel defense methods, FilterRAG and ML-FilterRAG, to mitigate the PoisonedRAG\nattack. First, we propose a new property to uncover distinct properties to\ndifferentiate between adversarial and clean texts in the knowledge data source.\nNext, we employ this property to filter out adversarial texts from clean ones\nin the design of our proposed approaches. Evaluation of these methods using\nbenchmark datasets demonstrate their effectiveness, with performances close to\nthose of the original RAG systems.",
      "authors": [
        "Kennedy Edemacu",
        "Vinay M. Shashidhar",
        "Micheal Tuape",
        "Dan Abudu",
        "Beakcheol Jang",
        "Jong Wook Kim"
      ],
      "published": "2025-08-04T19:03:52+00:00",
      "updated": "2025-08-04T19:03:52+00:00",
      "arxiv_id": "2508.02835v1",
      "url": "http://arxiv.org/pdf/2508.02835v1",
      "categories": [
        "cs.LG",
        "cs.IR"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "On the Theory and Practice of GRPO: A Trajectory-Corrected Approach with Fast Convergence",
      "abstract": "Group Relative Policy Optimization (GRPO), recently proposed by DeepSeek, is\na critic-free reinforcement learning algorithm for fine tuning large language\nmodels. It replaces the value function in Proximal Policy Optimization (PPO)\nwith group normalized rewards, while retaining PPO style token level importance\nsampling based on an old policy. We show that GRPO update rule in fact\nestimates the policy gradient at the old policy rather than the current one.\nHowever, since the old policy is refreshed every few steps, the discrepancy\nbetween the two remains small limiting the impact of this bias in practice. We\nvalidate this through an ablation study in which importance sampling is\nentirely removed, and updates are instead performed using the gradient\nestimated at a fixed old policy across multiple optimization steps. Remarkably,\nthis simplification results in performance comparable to standard GRPO.\n  Motivated by these findings, we propose a new algorithm: Trajectory level\nImportance Corrected GRPO (TIC GRPO). TIC GRPO replaces token level importance\nratios with a single trajectory level probability ratio, yielding an unbiased\nestimate of the current policy gradient while preserving the critic free\nstructure. Furthermore, we present the first theoretical convergence analysis\nfor GRPO style methods, covering both the original GRPO and our proposed\nvariant.",
      "authors": [
        "Lei Pang",
        "Ruinan Jin"
      ],
      "published": "2025-08-04T19:01:19+00:00",
      "updated": "2025-08-07T06:46:48+00:00",
      "arxiv_id": "2508.02833v2",
      "url": "http://arxiv.org/pdf/2508.02833v2",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Automated Validation of LLM-based Evaluators for Software Engineering Artifacts",
      "abstract": "Automation in software engineering increasingly relies on large language\nmodels (LLMs) to generate, review, and assess code artifacts. However,\nestablishing LLMs as reliable evaluators remains an open challenge: human\nevaluations are costly, subjective and non scalable, while existing automated\nmethods fail to discern fine grained variations in artifact quality.\n  We introduce REFINE (Ranking Evaluators for FIne grained Nuanced Evaluation),\nan automated framework for benchmarking LLM based evaluators across software\nengineering tasks. REFINE comprises of two modules: Hierarchy Dataset Builder\napplies novel generation techniques to automatically synthesize artifacts with\nprogressively reduced quality, and Evaluator Tester quantifies each candidate\nevaluator configuration by measuring how closely its rankings align with\nexpected ordering.\n  A key feature of REFINE is controllability: users can tune the granularity of\ndegradation to progressively refine evaluator configurations, from coarse\nfiltering to stress testing on subtle quality gaps.\n  While the methodology is general, we focus on coding tasks reflecting the\npractical demands in our production setting. REFINE was integrated into IBM's\ninternal development workflows and applied to code generation, translation, and\nsummarization for COBOL, an enterprise critical programming language, using\nindustrial data. It was used to identify LLM as a Judge configurations that\nlifted alignment scores from below $0.7$ to above $0.9$ in some coding tasks.\nThese nuance sensitive evaluators are now actively used by model training teams\nto support model release decisions.",
      "authors": [
        "Ora Nova Fandina",
        "Eitan Farchi",
        "Shmulik Froimovich",
        "Rami Katan",
        "Alice Podolsky",
        "Orna Raz",
        "Avi Ziv"
      ],
      "published": "2025-08-04T18:52:01+00:00",
      "updated": "2025-08-04T18:52:01+00:00",
      "arxiv_id": "2508.02827v1",
      "url": "http://arxiv.org/pdf/2508.02827v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation",
      "abstract": "Radiological imaging is central to diagnosis, treatment planning, and\nclinical decision-making. Vision-language foundation models have spurred\ninterest in automated radiology report generation (RRG), but safe deployment\nrequires reliable clinical evaluation of generated reports. Existing metrics\noften rely on surface-level similarity or behave as black boxes, lacking\ninterpretability. We introduce ICARE (Interpretable and Clinically-grounded\nAgent-based Report Evaluation), an interpretable evaluation framework\nleveraging large language model agents and dynamic multiple-choice question\nanswering (MCQA). Two agents, each with either the ground-truth or generated\nreport, generate clinically meaningful questions and quiz each other. Agreement\non answers captures preservation and consistency of findings, serving as\ninterpretable proxies for clinical precision and recall. By linking scores to\nquestion-answer pairs, ICARE enables transparent, and interpretable assessment.\nClinician studies show ICARE aligns significantly more with expert judgment\nthan prior metrics. Perturbation analyses confirm sensitivity to clinical\ncontent and reproducibility, while model comparisons reveal interpretable error\npatterns.",
      "authors": [
        "Radhika Dua",
        "Young Joon",
        "Kwon",
        "Siddhant Dogra",
        "Daniel Freedman",
        "Diana Ruan",
        "Motaz Nashawaty",
        "Danielle Rigau",
        "Daniel Alexander Alber",
        "Kang Zhang",
        "Kyunghyun Cho",
        "Eric Karl Oermann"
      ],
      "published": "2025-08-04T18:28:03+00:00",
      "updated": "2025-08-04T18:28:03+00:00",
      "arxiv_id": "2508.02808v1",
      "url": "http://arxiv.org/pdf/2508.02808v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science",
      "abstract": "The capacity for artificial intelligence (AI) to formulate, evolve, and test\naltered thought patterns under dynamic conditions indicates advanced cognition\nthat is crucial for scientific discovery. The existing AI development landscape\nfalls into two categories: 1) frameworks over non-reasoning models that\nnatively incorporate opinions on how humans think, and 2) reasoning models that\nabstract precise control of the reasoning intuition away from end users. While\npowerful, for scientists to maximize utility of AI in scientific discovery,\nthey not only require accuracy and transparency in reasoning, but also\nsteerability. Hence, we introduce an alternative approach that enables deep and\nprecise control over the reasoning process called: a cognitive loop via in-situ\noptimization (CLIO). CLIO enables large language models (LLMs) to\nself-formulate ways of approaching a problem, adapt behavior when\nself-confidence is low, and ultimately provide scientists with a final belief\nor answer. Through CLIO's open design, scientists can observe uncertainty\nlevels, understand how final belief states are formulated using graph\nstructures, and interject corrections. Without any further post-training,\nOpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\\% in text-based biology\nand medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\\% net\nor 161.64\\% relative increase when compared to the base GPT-4.1 model and\nsurpasses OpenAI's o3 performance in high and low reasoning effort modes. We\nfurther discovered that oscillations within internal uncertainty measures are\nkey in determining the accuracy of CLIO's results, revealing how its open\ndesign and internal mechanisms can provide insight and control into scientific\ndecision-making processes.",
      "authors": [
        "Newman Cheng",
        "Gordon Broadbent",
        "William Chappell"
      ],
      "published": "2025-08-04T18:01:35+00:00",
      "updated": "2025-08-04T18:01:35+00:00",
      "arxiv_id": "2508.02789v1",
      "url": "http://arxiv.org/pdf/2508.02789v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on Vision-Language Models",
      "abstract": "For CLIP-based prompt tuning, introducing more data as additional knowledge\nfor enhancing fine-tuning process is proved to be an effective approach.\nExisting data amplification strategies for prompt tuning typically rely on\nexternal knowledge (e.g., large language models or pre-structured knowledge\nbases), resulting in higher costs for data collection and processing, while\ngenerally ignoring further utilization of features in image modality. To\naddress this, we propose Augmentation-driven Prompt Tuning (AugPT), a\nself-contained distillation-based prompt tuning approach using only internal\naugmentation on raw dataset to better exploit known features. Specifically,\nAugPT employs self-supervised augmentation on unlabeled images in the training\nset, and introduces a novel gating mechanism based on consensus test, reusing\nthe pre-trained prompt tuning backbone model to spontaneously filter noisy\nsamples, further enhancing the quality of augmented views. Extensive\nexperiments validate that AugPT simultaneously enhances model performance and\ngeneralization capability without using appended external knowledge. The code\nof AugPT is available at: https://github.com/JREion/AugPT .",
      "authors": [
        "Haoyang Li",
        "Liang Wang",
        "Chao Wang",
        "Siyu Zhou",
        "Jing Jiang",
        "Yan Peng",
        "Guodong Long"
      ],
      "published": "2025-08-04T17:59:56+00:00",
      "updated": "2025-08-04T17:59:56+00:00",
      "arxiv_id": "2508.02671v1",
      "url": "http://arxiv.org/pdf/2508.02671v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "LOST: Low-rank and Sparse Pre-training for Large Language Models",
      "abstract": "While large language models (LLMs) have achieved remarkable performance\nacross a wide range of tasks, their massive scale incurs prohibitive\ncomputational and memory costs for pre-training from scratch. Recent studies\nhave investigated the use of low-rank parameterization as a means of reducing\nmodel size and training cost. In this context, sparsity is often employed as a\ncomplementary technique to recover important information lost in low-rank\ncompression by capturing salient features in the residual space. However,\nexisting approaches typically combine low-rank and sparse components in a\nsimplistic or ad hoc manner, often resulting in undesirable performance\ndegradation compared to full-rank training. In this paper, we propose\n\\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for\nLLMs, a novel method that ingeniously integrates low-rank and sparse structures\nto enable effective training of LLMs from scratch under strict efficiency\nconstraints. LOST applies singular value decomposition to weight matrices,\npreserving the dominant low-rank components, while allocating the remaining\nsingular values to construct channel-wise sparse components to complement the\nexpressiveness of low-rank training. We evaluate LOST on LLM pretraining\nranging from 60M to 7B parameters. Our experiments show that LOST achieves\ncompetitive or superior performance compared to full-rank models, while\nsignificantly reducing both memory and compute overhead. Moreover, Code is\navailable at\n\\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST\nRepo}",
      "authors": [
        "Jiaxi Li",
        "Lu Yin",
        "Li Shen",
        "Jinjin Xu",
        "Liwu Xu",
        "Tianjin Huang",
        "Wenwu Wang",
        "Shiwei Liu",
        "Xilu Wang"
      ],
      "published": "2025-08-04T17:58:22+00:00",
      "updated": "2025-08-04T17:58:22+00:00",
      "arxiv_id": "2508.02668v1",
      "url": "http://arxiv.org/pdf/2508.02668v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Evaluating Variance in Visual Question Answering Benchmarks",
      "abstract": "Multimodal large language models (MLLMs) have emerged as powerful tools for\nvisual question answering (VQA), enabling reasoning and contextual\nunderstanding across visual and textual modalities. Despite their advancements,\nthe evaluation of MLLMs on VQA benchmarks often relies on point estimates,\noverlooking the significant variance in performance caused by factors such as\nstochastic model outputs, training seed sensitivity, and hyperparameter\nconfigurations. This paper critically examines these issues by analyzing\nvariance across 14 widely used VQA benchmarks, covering diverse tasks such as\nvisual reasoning, text understanding, and commonsense reasoning. We\nsystematically study the impact of training seed, framework non-determinism,\nmodel scale, and extended instruction finetuning on performance variability.\nAdditionally, we explore Cloze-style evaluation as an alternate assessment\nstrategy, studying its effectiveness in reducing stochasticity and improving\nreliability across benchmarks. Our findings highlight the limitations of\ncurrent evaluation practices and advocate for variance-aware methodologies to\nfoster more robust and reliable development of MLLMs.",
      "authors": [
        "Nikitha SR"
      ],
      "published": "2025-08-04T17:37:13+00:00",
      "updated": "2025-08-04T17:37:13+00:00",
      "arxiv_id": "2508.02645v1",
      "url": "http://arxiv.org/pdf/2508.02645v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Test Set Quality in Multilingual LLM Evaluation",
      "abstract": "Several multilingual benchmark datasets have been developed in a\nsemi-automatic manner in the recent past to measure progress and understand the\nstate-of-the-art in the multilingual capabilities of Large Language Models.\nHowever, there is not a lot of attention paid to the quality of the datasets\nthemselves, despite the existence of previous work in identifying errors in\neven fully human-annotated test sets. In this paper, we manually analyze recent\nmultilingual evaluation sets in two languages - French and Telugu, identifying\nseveral errors in the process. We compare the performance difference across\nseveral LLMs with the original and revised versions of the datasets and\nidentify large differences (almost 10% in some cases) in both languages). Based\non these results, we argue that test sets should not be considered immutable\nand should be revisited, checked for correctness, and potentially versioned. We\nend with some recommendations for both the dataset creators as well as\nconsumers on addressing the dataset quality issues.",
      "authors": [
        "Kranti Chalamalasetti",
        "Gabriel Bernier-Colborne",
        "Yvan Gauthier",
        "Sowmya Vajjala"
      ],
      "published": "2025-08-04T17:22:08+00:00",
      "updated": "2025-08-04T17:22:08+00:00",
      "arxiv_id": "2508.02635v1",
      "url": "http://arxiv.org/pdf/2508.02635v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents",
      "abstract": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.",
      "authors": [
        "Yibin Liu",
        "Zhixuan Liang",
        "Zanxin Chen",
        "Tianxing Chen",
        "Mengkang Hu",
        "Wanxi Dong",
        "Congsheng Xu",
        "Zhaoming Han",
        "Yusen Qin",
        "Yao Mu"
      ],
      "published": "2025-08-04T17:18:14+00:00",
      "updated": "2025-08-06T07:24:55+00:00",
      "arxiv_id": "2508.02629v2",
      "url": "http://arxiv.org/pdf/2508.02629v2",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction",
      "abstract": "This paper introduces and formalizes Noosemia, a novel\ncognitive-phenomenological phenomenon emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological, and social\nimplications of noosemic dynamics and directions for future research.",
      "authors": [
        "Enrico De Santis",
        "Antonello Rizzi"
      ],
      "published": "2025-08-04T17:10:08+00:00",
      "updated": "2025-08-04T17:10:08+00:00",
      "arxiv_id": "2508.02622v1",
      "url": "http://arxiv.org/pdf/2508.02622v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation",
      "abstract": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM.",
      "authors": [
        "Jianxiang Zang",
        "Meiling Ning",
        "Shihan Dou",
        "Jiazheng Zhang",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "published": "2025-08-04T17:06:23+00:00",
      "updated": "2025-08-04T17:06:23+00:00",
      "arxiv_id": "2508.02618v1",
      "url": "http://arxiv.org/pdf/2508.02618v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Meta-RAG on Large Codebases Using Code Summarization",
      "abstract": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance.",
      "authors": [
        "Vali Tawosia",
        "Salwa Alamir",
        "Xiaomo Liu",
        "Manuela Veloso"
      ],
      "published": "2025-08-04T17:01:10+00:00",
      "updated": "2025-08-04T17:01:10+00:00",
      "arxiv_id": "2508.02611v1",
      "url": "http://arxiv.org/pdf/2508.02611v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis in Low-Data Regimes",
      "abstract": "The application of machine learning on tabular data in specialized domains is\nseverely limited by data scarcity. While generative models offer a solution,\ntraditional methods falter in low-data regimes, and recent Large Language\nModels (LLMs) often ignore the explicit dependency structure of tabular data,\nleading to low-fidelity synthetics. To address these limitations, we introduce\nStructSynth, a novel framework that integrates the generative power of LLMs\nwith robust structural control. StructSynth employs a two-stage architecture.\nFirst, it performs explicit structure discovery to learn a Directed Acyclic\nGraph (DAG) from the available data. Second, this learned structure serves as a\nhigh-fidelity blueprint to steer the LLM's generation process, forcing it to\nadhere to the learned feature dependencies and thereby ensuring the generated\ndata respects the underlying structure by design. Our extensive experiments\ndemonstrate that StructSynth produces synthetic data with significantly higher\nstructural integrity and downstream utility than state-of-the-art methods. It\nproves especially effective in challenging low-data scenarios, successfully\nnavigating the trade-off between privacy preservation and statistical fidelity.",
      "authors": [
        "Siyi Liu",
        "Yujia Zheng",
        "Yongqi Zhang"
      ],
      "published": "2025-08-04T16:55:02+00:00",
      "updated": "2025-08-04T16:55:02+00:00",
      "arxiv_id": "2508.02601v1",
      "url": "http://arxiv.org/pdf/2508.02601v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks",
      "abstract": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks.",
      "authors": [
        "Omri Uzan",
        "Yuval Pinter"
      ],
      "published": "2025-08-04T16:46:15+00:00",
      "updated": "2025-08-06T09:06:34+00:00",
      "arxiv_id": "2508.02591v2",
      "url": "http://arxiv.org/pdf/2508.02591v2",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification",
      "abstract": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs.",
      "authors": [
        "Ming Pok Ng",
        "Junqi Jiang",
        "Gabriel Freedman",
        "Antonio Rago",
        "Francesca Toni"
      ],
      "published": "2025-08-04T16:40:02+00:00",
      "updated": "2025-08-04T16:40:02+00:00",
      "arxiv_id": "2508.02584v1",
      "url": "http://arxiv.org/pdf/2508.02584v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge",
      "abstract": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone.",
      "authors": [
        "Lei Zan",
        "Keli Zhang",
        "Ruichu Cai",
        "Lujia Pan"
      ],
      "published": "2025-08-04T16:39:24+00:00",
      "updated": "2025-08-07T15:57:23+00:00",
      "arxiv_id": "2508.02583v2",
      "url": "http://arxiv.org/pdf/2508.02583v2",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare",
      "abstract": "Arabic-language patient feedback remains under-analysed because dialect\ndiversity and scarce aspect-level sentiment labels hinder automated assessment.\nTo address this gap, we introduce EHSAN, a data-centric hybrid pipeline that\nmerges ChatGPT pseudo-labelling with targeted human review to build the first\nexplainable Arabic aspect-based sentiment dataset for healthcare. Each sentence\nis annotated with an aspect and sentiment label (positive, negative, or\nneutral), forming a pioneering Arabic dataset aligned with healthcare themes,\nwith ChatGPT-generated rationales provided for each label to enhance\ntransparency. To evaluate the impact of annotation quality on model\nperformance, we created three versions of the training data: a fully supervised\nset with all labels reviewed by humans, a semi-supervised set with 50% human\nreview, and an unsupervised set with only machine-generated labels. We\nfine-tuned two transformer models on these datasets for both aspect and\nsentiment classification. Experimental results show that our Arabic-specific\nmodel achieved high accuracy even with minimal human supervision, reflecting\nonly a minor performance drop when using ChatGPT-only labels. Reducing the\nnumber of aspect classes notably improved classification metrics across the\nboard. These findings demonstrate an effective, scalable approach to Arabic\naspect-based sentiment analysis (SA) in healthcare, combining large language\nmodel annotation with human expertise to produce a robust and explainable\ndataset. Future directions include generalisation across hospitals, prompt\nrefinement, and interpretable data-driven modelling.",
      "authors": [
        "Eman Alamoudi",
        "Ellis Solaiman"
      ],
      "published": "2025-08-04T16:28:58+00:00",
      "updated": "2025-08-04T16:28:58+00:00",
      "arxiv_id": "2508.02574v1",
      "url": "http://arxiv.org/pdf/2508.02574v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs",
      "abstract": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization.",
      "authors": [
        "JÃ©rÃ©mie Dentan",
        "Davide Buscaldi",
        "Sonia Vanier"
      ],
      "published": "2025-08-04T16:27:56+00:00",
      "updated": "2025-08-04T16:27:56+00:00",
      "arxiv_id": "2508.02573v1",
      "url": "http://arxiv.org/pdf/2508.02573v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ASINT: Learning AS-to-Organization Mapping from Internet Metadata",
      "abstract": "Accurately mapping Autonomous Systems (ASNs) to their owning or operating\norganizations underpins Internet measurement research and security\napplications. Yet existing approaches commonly rely solely on WHOIS or\nPeeringDB, missing important relationships (e.g., cross-regional aliases,\nparent-child ownership) and failing to unify organizations scattered across\ndifferent RIR identifiers. We introduce ASINT, an end-to-end pipeline that\nfuses bulk registry data with unstructured Web sources, then employs\nretrieval-augmented generation (RAG) to guide large language model (LLM)\ninference. Through a multi-stage procedure, ASINT merges ASNs into\n\"organization families,\" capturing nuanced ties beyond the scope of simpler\nheuristics.\n  ASINT maps 111,470 ASNs to 81,233 organization families; compared to both\nAS2ORG+ and AS-Sibling, ASINT identifies more cross-regional groupings (e.g.,\noperator aliases, rebrands) that other datasets overlook. Moreover, our refined\nmappings enhance multiple security and measurement tasks: ASINT exposes 27.5%\nmore intra-organizational RPKI misconfigurations, cuts false-positive hijack\nalarms by 9.4%, and lowers erroneous IP leasing inferences by 5.9%.\n  Finally, ASINT supports periodic updates and cost-sensitive LLM selection,\ndemonstrating that broader Web evidence can provide a more accurate, evolving\nview of the Internet's organizational structure.",
      "authors": [
        "Yongzhe Xu",
        "Weitong Li",
        "Eeshan Umrani",
        "Taejoong Chung"
      ],
      "published": "2025-08-04T16:26:17+00:00",
      "updated": "2025-08-04T16:26:17+00:00",
      "arxiv_id": "2508.02571v1",
      "url": "http://arxiv.org/pdf/2508.02571v1",
      "categories": [
        "cs.NI"
      ],
      "primary_category": "cs.NI"
    },
    {
      "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
      "abstract": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
      "authors": [
        "Yuerong Song",
        "Xiaoran Liu",
        "Ruixiao Li",
        "Zhigeng Liu",
        "Zengfeng Huang",
        "Qipeng Guo",
        "Ziwei He",
        "Xipeng Qiu"
      ],
      "published": "2025-08-04T16:14:03+00:00",
      "updated": "2025-08-04T16:14:03+00:00",
      "arxiv_id": "2508.02558v1",
      "url": "http://arxiv.org/pdf/2508.02558v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2",
      "abstract": "Large language models demonstrate proficiency on phonetic tasks, such as\nrhyming, without explicit phonetic or auditory grounding. In this work, we\ninvestigate how \\verb|Llama-3.2-1B-Instruct| represents token-level phonetic\ninformation. Our results suggest that Llama uses a rich internal model of\nphonemes to complete phonetic tasks. We provide evidence for high-level\norganization of phoneme representations in its latent space. In doing so, we\nalso identify a ``phoneme mover head\" which promotes phonetic information\nduring rhyming tasks. We visualize the output space of this head and find that,\nwhile notable differences exist, Llama learns a model of vowels similar to the\nstandard IPA vowel chart for humans, despite receiving no direct supervision to\ndo so.",
      "authors": [
        "Jack Merullo",
        "Arjun Khurana",
        "Oliver McLaughlin"
      ],
      "published": "2025-08-04T15:36:51+00:00",
      "updated": "2025-08-04T15:36:51+00:00",
      "arxiv_id": "2508.02527v1",
      "url": "http://arxiv.org/pdf/2508.02527v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Accurate and Interpretable Postmenstrual Age Prediction via Multimodal Large Language Model",
      "abstract": "Accurate estimation of postmenstrual age (PMA) at scan is crucial for\nassessing neonatal development and health. While deep learning models have\nachieved high accuracy in predicting PMA from brain MRI, they often function as\nblack boxes, offering limited transparency and interpretability in clinical\ndecision support. In this work, we address the dual challenge of accuracy and\ninterpretability by adapting a multimodal large language model (MLLM) to\nperform both precise PMA prediction and clinically relevant explanation\ngeneration. We introduce a parameter-efficient fine-tuning (PEFT) strategy\nusing instruction tuning and Low-Rank Adaptation (LoRA) applied to the\nQwen2.5-VL-7B model. The model is trained on four 2D cortical surface\nprojection maps derived from neonatal MRI scans. By employing distinct prompts\nfor training and inference, our approach enables the MLLM to handle a\nregression task during training and generate clinically relevant explanations\nduring inference. The fine-tuned model achieves a low prediction error with a\n95 percent confidence interval of 0.78 to 1.52 weeks, while producing\ninterpretable outputs grounded in developmental features, marking a significant\nstep toward transparent and trustworthy AI systems in perinatal neuroscience.",
      "authors": [
        "Qifan Chen",
        "Jin Cui",
        "Cindy Duan",
        "Yushuo Han",
        "Yifei Shi"
      ],
      "published": "2025-08-04T15:35:36+00:00",
      "updated": "2025-08-04T15:35:36+00:00",
      "arxiv_id": "2508.02525v1",
      "url": "http://arxiv.org/pdf/2508.02525v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Transportation Cyber Incident Awareness through Generative AI-Based Incident Analysis and Retrieval-Augmented Question-Answering Systems",
      "abstract": "Technological advancements have revolutionized numerous industries, including\ntransportation. While digitalization, automation, and connectivity have\nenhanced safety and efficiency, they have also introduced new vulnerabilities.\nWith 95% of data breaches attributed to human error, promoting cybersecurity\nawareness in transportation is increasingly critical. Despite numerous\ncyberattacks on transportation systems worldwide, comprehensive and centralized\nrecords of these incidents remain scarce. To address this gap and enhance cyber\nawareness, this paper presents a large language model (LLM) based approach to\nextract and organize transportation related cyber incidents from publicly\navailable datasets. A key contribution of this work is the use of generative AI\nto transform unstructured, heterogeneous cyber incident data into structured\nformats. Incidents were sourced from the Center for Strategic & International\nStudies (CSIS) List of Significant Cyber Incidents, the University of Maryland\nCyber Events Database (UMCED), the European Repository of Cyber Incidents\n(EuRepoC), the Maritime Cyber Attack Database (MCAD), and the U.S. DOT\nTransportation Cybersecurity and Resiliency (TraCR) Examples of Cyber Attacks\nin Transportation (2018 to 2022). These were classified by a fine tuned LLM\ninto five transportation modes: aviation, maritime, rail, road, and multimodal,\nforming a transportation specific cyber incident database. Another key\ncontribution of this work is the development of a Retrieval Augmented\nGeneration question answering system, designed to enhance accessibility and\npractical use by enabling users to query the curated database for specific\ndetails on transportation related cyber incidents. By leveraging LLMs for both\ndata extraction and user interaction, this study contributes a novel,\naccessible tool for improving cybersecurity awareness in the transportation\nsector.",
      "authors": [
        "Ostonya Thomas",
        "Muhaimin Bin Munir",
        "Jean-Michel Tine",
        "Mizanur Rahman",
        "Yuchen Cai",
        "Khandakar Ashrafi Akbar",
        "Md Nahiyan Uddin",
        "Latifur Khan",
        "Trayce Hockstad",
        "Mashrur Chowdhury"
      ],
      "published": "2025-08-04T15:34:25+00:00",
      "updated": "2025-08-04T15:34:25+00:00",
      "arxiv_id": "2508.02523v1",
      "url": "http://arxiv.org/pdf/2508.02523v1",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384",
      "abstract": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in\nlarge-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in\nrecent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is\nscaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s\nhigh-speed interconnects. Running large MoE models on SuperPod-scale hardware\nbrings new challenges. It requires new execution models, scalable scheduling,\nefficient expert load balancing, and elimination of single points of failure.\nThis paper presents xDeepServe, Huawei Cloud's LLM serving system designed for\nSuperPod-scale infrastructure. At its core is Transformerless, a disaggregated\narchitecture that decomposes transformer models into modular units--attention,\nfeedforward, and MoE--executed independently on NPUs connected via high-speed\nfabric. We implement this design in two forms: disaggregated prefill-decode and\ndisaggregated MoE-attention. This fully disaggregated setup enables independent\nscaling of compute and memory without sacrificing performance. To support this\narchitecture, we propose XCCL, a communication library that leverages\nCloudMatrix384's global shared memory to implement efficient point-to-point and\nall-to-all primitives. We also extend our serving engine FlowServe with\nsystem-level techniques, enabling scalable inference across hundreds of NPUs.",
      "authors": [
        "Ao Xiao",
        "Bangzheng He",
        "Baoquan Zhang",
        "Baoxing Huai",
        "Bingji Wang",
        "Bo Wang",
        "Bo Xu",
        "Boyi Hou",
        "Chan Yang",
        "Changhong Liu",
        "Cheng Cui",
        "Chenyu Zhu",
        "Cong Feng",
        "Daohui Wang",
        "Dayun Lin",
        "Duo Zhao",
        "Fengshao Zou",
        "Fu Wang",
        "Gangqiang Zhang",
        "Gengyuan Dan",
        "Guanjie Chen",
        "Guodong Guan",
        "Guodong Yang",
        "Haifeng Li",
        "Haipei Zhu",
        "Hao Feng",
        "Hao Huang",
        "Hao Xu",
        "Hengrui Ma",
        "Hengtao Fan",
        "Hui Liu",
        "Jia Li",
        "Jiang Liu",
        "Jiang Xu",
        "Jie Meng",
        "Jinhan Xin",
        "Junhao Hu",
        "Juwei Chen",
        "Lan Yu",
        "Lanxin Miao",
        "Liang Liu",
        "Linan Jing",
        "Lu Zhou",
        "Meina Han",
        "Mingkun Deng",
        "Mingyu Deng",
        "Naitian Deng",
        "Nizhong Lin",
        "Peihan Zhao",
        "Peng Pan",
        "Pengfei Shen",
        "Ping Li",
        "Qi Zhang",
        "Qin Zhang",
        "Qingrong Xia",
        "Qingyi Zhang",
        "Qunchao Fu",
        "Ren Guo",
        "Ruimin Gao",
        "Shaochun Li",
        "Sheng Long",
        "Shentian Li",
        "Shining Wan",
        "Shuai Shen",
        "Shuangfu Zeng",
        "Shuming Jing",
        "Siqi Yang",
        "Song Zhang",
        "Tao Xu",
        "Tianlin Du",
        "Ting Chen",
        "Wanxu Wu",
        "Wei Jiang",
        "Weinan Tong",
        "Weiwei Chen",
        "Wen Peng",
        "Wenli Zhou",
        "Wenquan Yang",
        "Wenxin Liang",
        "Xiang Liu",
        "Xiaoli Zhou",
        "Xin Jin",
        "Xinyu Duan",
        "Xu Li",
        "Xu Zhang",
        "Xusheng Chen",
        "Yalong Shan",
        "Yang Gan",
        "Yao Lu",
        "Yi Deng",
        "Yi Zheng",
        "Yingfei Zheng",
        "Yiyun Zheng",
        "Yizhou Shan",
        "Yong Gao",
        "Yongqiang Yang",
        "Yuanjin Gong",
        "Yue Yu",
        "Yuetao Chen",
        "Yukun Zhu",
        "Yulong He",
        "Yusu Zhao",
        "Yuyan Wu",
        "Zenan Zhang",
        "Zhaojin Zhuo",
        "Zhaoyang Ji",
        "Zhefeng Wang",
        "Zheng Wang",
        "Zhenhua Yang",
        "Zhenli Sheng",
        "Zhibin Yu",
        "Zhigang Ji",
        "Zhihao Ren",
        "Zhipeng Bian",
        "Zhixia Liu",
        "Zhiyu Dong",
        "Zhonghua Li",
        "Zhou Yu",
        "Zhuoming Shen",
        "Zhuwei Peng",
        "Zi Ye",
        "Zihao Xiang",
        "Zimin Fu",
        "Zixuan Zhang"
      ],
      "published": "2025-08-04T15:30:57+00:00",
      "updated": "2025-08-07T05:19:42+00:00",
      "arxiv_id": "2508.02520v4",
      "url": "http://arxiv.org/pdf/2508.02520v4",
      "categories": [
        "cs.DC"
      ],
      "primary_category": "cs.DC"
    },
    {
      "title": "AnalogCoder-Pro: Unifying Analog Circuit Generation and Optimization via Multi-modal LLMs",
      "abstract": "Despite advances in analog design automation, analog front-end design still\nheavily depends on expert intuition and iterative simulations, underscoring\ncritical gaps in fully automated optimization for performance-critical\napplications. Recently, the rapid development of Large Language Models (LLMs)\nhas brought new promise to analog design automation. However, existing work\nremains in its early stages, and holistic joint optimization for practical\nend-to-end solutions remains largely unexplored. We propose AnalogCoder-Pro, a\nunified multimodal LLM-based framework that integrates generative capabilities\nand optimization techniques to jointly explore circuit topologies and optimize\ndevice sizing, automatically generating performance-specific, fully sized\nschematic netlists. AnalogCoder-Pro employs rejection sampling for fine-tuning\nLLMs on high-quality synthesized circuit data and introduces a multimodal\ndiagnosis and repair workflow based on functional specifications and waveform\nimages. By leveraging LLMs to interpret generated circuit netlists,\nAnalogCoder-Pro automates the extraction of critical design parameters and the\nformulation of parameter spaces, establishing an end-to-end workflow for\nsimultaneous topology generation and device sizing optimization. Extensive\nexperiments demonstrate that these orthogonal approaches significantly improve\nthe success rate of analog circuit design and enhance circuit performance.",
      "authors": [
        "Yao Lai",
        "Souradip Poddar",
        "Sungyoung Lee",
        "Guojin Chen",
        "Mengkang Hu",
        "Bei Yu",
        "Ping Luo",
        "David Z. Pan"
      ],
      "published": "2025-08-04T15:25:48+00:00",
      "updated": "2025-08-04T15:25:48+00:00",
      "arxiv_id": "2508.02518v1",
      "url": "http://arxiv.org/pdf/2508.02518v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs",
      "abstract": "This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts.",
      "authors": [
        "Zhan Qu",
        "Shuzhou Yuan",
        "Michael FÃ¤rber"
      ],
      "published": "2025-08-04T15:19:22+00:00",
      "updated": "2025-08-04T15:19:22+00:00",
      "arxiv_id": "2508.02515v1",
      "url": "http://arxiv.org/pdf/2508.02515v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Modular Arithmetic: Language Models Solve Math Digit by Digit",
      "abstract": "While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks.",
      "authors": [
        "Tanja Baeumel",
        "Daniil Gurgurov",
        "Yusser al Ghussin",
        "Josef van Genabith",
        "Simon Ostermann"
      ],
      "published": "2025-08-04T15:18:41+00:00",
      "updated": "2025-08-04T15:18:41+00:00",
      "arxiv_id": "2508.02513v1",
      "url": "http://arxiv.org/pdf/2508.02513v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Test-time Prompt Intervention",
      "abstract": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.",
      "authors": [
        "Chenxu Yang",
        "Qingyi Si",
        "Mz Dai",
        "Dingyu Yao",
        "Mingyu Zheng",
        "Minghui Chen",
        "Zheng Lin",
        "Weiping Wang"
      ],
      "published": "2025-08-04T15:17:13+00:00",
      "updated": "2025-08-04T15:17:13+00:00",
      "arxiv_id": "2508.02511v1",
      "url": "http://arxiv.org/pdf/2508.02511v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms",
      "abstract": "Retrieval-augmented generation (RAG) plays a critical role in user-generated\ncontent (UGC) platforms, but its effectiveness depends heavily on accurate\nrelevance assessment of query-document pairs. Despite recent advances in\napplying large language models (LLMs) to relevance modeling, UGC platforms\npresent unique challenges: 1) ambiguous user intent due to sparse user feedback\nin RAG scenarios, and 2) substantial noise introduced by informal and\nunstructured language. To address these issues, we propose the Reinforced\nReasoning Model for Relevance Assessment (R3A), which introduces a decomposed\nreasoning framework over queries and candidate documents before scoring. R3A\nfirst leverages auxiliary high-ranked documents within the platform to infer\nlatent query intent. It then performs verbatim fragment extraction to justify\nrelevance decisions, thereby reducing errors caused by noisy UGC. Based on a\nreinforcement learning framework, R3A is optimized to mitigate distortions\narising from ambiguous queries and unstructured content. Experimental results\nshow that R3A significantly outperforms existing baseline methods in terms of\nrelevance accuracy, across both offline benchmarks and online experiments.",
      "authors": [
        "Xiaowei Yuan",
        "Lei Jin",
        "Haoxin Zhang",
        "Yan Gao",
        "Yi Wu",
        "Yao Hu",
        "Ziyang Huang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "published": "2025-08-04T15:14:09+00:00",
      "updated": "2025-08-04T15:14:09+00:00",
      "arxiv_id": "2508.02506v1",
      "url": "http://arxiv.org/pdf/2508.02506v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Would you let a humanoid play storytelling with your child? A usability study on LLM-powered narrative Humanoid-Robot Interaction",
      "abstract": "A key challenge in human-robot interaction research lies in developing\nrobotic systems that can effectively perceive and interpret social cues,\nfacilitating natural and adaptive interactions. In this work, we present a\nnovel framework for enhancing the attention of the iCub humanoid robot by\nintegrating advanced perceptual abilities to recognise social cues, understand\nsurroundings through generative models, such as ChatGPT, and respond with\ncontextually appropriate social behaviour. Specifically, we propose an\ninteraction task implementing a narrative protocol (storytelling task) in which\nthe human and the robot create a short imaginary story together, exchanging in\nturn cubes with creative images placed on them. To validate the protocol and\nthe framework, experiments were performed to quantify the degree of usability\nand the quality of experience perceived by participants interacting with the\nsystem. Such a system can be beneficial in promoting effective human robot\ncollaborations, especially in assistance, education and rehabilitation\nscenarios where the social awareness and the robot responsiveness play a\npivotal role.",
      "authors": [
        "Maria Lombardi",
        "Carmela Calabrese",
        "Davide Ghiglino",
        "Caterina Foglino",
        "Davide De Tommaso",
        "Giulia Da Lisca",
        "Lorenzo Natale",
        "Agnieszka Wykowska"
      ],
      "published": "2025-08-04T15:13:56+00:00",
      "updated": "2025-08-04T15:13:56+00:00",
      "arxiv_id": "2508.02505v1",
      "url": "http://arxiv.org/pdf/2508.02505v1",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling",
      "abstract": "LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems.",
      "authors": [
        "Maxime Bouscary",
        "Saurabh Amin"
      ],
      "published": "2025-08-04T15:11:51+00:00",
      "updated": "2025-08-04T15:11:51+00:00",
      "arxiv_id": "2508.02503v1",
      "url": "http://arxiv.org/pdf/2508.02503v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks",
      "abstract": "Large Language Models (LLMs) exhibit strong linguistic capabilities, but\nlittle is known about how they encode psycholinguistic knowledge across\nlanguages. We investigate whether and how LLMs exhibit human-like\npsycholinguistic responses under different linguistic identities using two\ntasks: sound symbolism and word valence. We evaluate two models,\nLlama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and\nbilingual prompting in English, Dutch, and Chinese. Behaviorally, both models\nadjust their outputs based on prompted language identity, with Qwen showing\ngreater sensitivity and sharper distinctions between Dutch and Chinese. Probing\nanalysis reveals that psycholinguistic signals become more decodable in deeper\nlayers, with Chinese prompts yielding stronger and more stable valence\nrepresentations than Dutch. Our results demonstrate that language identity\nconditions both output behavior and internal representations in LLMs, providing\nnew insights into their application as models of cross-linguistic cognition.",
      "authors": [
        "Shuzhou Yuan",
        "Zhan Qu",
        "Mario Tawfelis",
        "Michael FÃ¤rber"
      ],
      "published": "2025-08-04T15:10:44+00:00",
      "updated": "2025-08-04T15:10:44+00:00",
      "arxiv_id": "2508.02502v1",
      "url": "http://arxiv.org/pdf/2508.02502v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Bridging Language Gaps in Open-Source Documentation with Large-Language-Model Translation",
      "abstract": "While open source communities attract diverse contributors globally, few\nrepositories provide essential documentation in languages other than English.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nsoftware engineering tasks and translations across domains. However, little is\nknown about LLM capabilities in translating open-source technical\ndocumentation, which mixes natural language, code, URLs, and markdown\nformatting. To understand the need and potential for LLMs in technical\ndocumentation translation, we evaluated community translation activity and\nEnglish-to-German translations of 50 README files using OpenAI's ChatGPT 4 and\nAnthropic's Claude. We found scarce translation activity, mostly in larger\nrepositories and community-driven in nature. LLM performance comparison\nsuggests they can provide accurate translations. However, analysis revealed\nfidelity challenges: both models struggled to preserve structural components\n(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings\nhighlight both promise and challenges of LLM-assisted documentation\ninternationalization. As a first step toward translation-aware continuous\nintegration pipelines, we introduce TRIFID, an early-stage translation fidelity\nscoring framework that automatically checks how well translations preserve\ncode, links, and formatting. Our efforts provide a foundation for automated\nLLM-driven support for creating and maintaining open source documentation.",
      "authors": [
        "Elijah Kayode Adejumo",
        "Brittany Johnson",
        "Mariam Guizani"
      ],
      "published": "2025-08-04T15:07:35+00:00",
      "updated": "2025-08-04T15:07:35+00:00",
      "arxiv_id": "2508.02497v1",
      "url": "http://arxiv.org/pdf/2508.02497v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "PHM-Bench: A Domain-Specific Benchmarking Framework for Systematic Evaluation of Large Models in Prognostics and Health Management",
      "abstract": "With the rapid advancement of generative artificial intelligence, large\nlanguage models (LLMs) are increasingly adopted in industrial domains, offering\nnew opportunities for Prognostics and Health Management (PHM). These models\nhelp address challenges such as high development costs, long deployment cycles,\nand limited generalizability. However, despite the growing synergy between PHM\nand LLMs, existing evaluation methodologies often fall short in structural\ncompleteness, dimensional comprehensiveness, and evaluation granularity. This\nhampers the in-depth integration of LLMs into the PHM domain. To address these\nlimitations, this study proposes PHM-Bench, a novel three-dimensional\nevaluation framework for PHM-oriented large models. Grounded in the triadic\nstructure of fundamental capability, core task, and entire lifecycle, PHM-Bench\nis tailored to the unique demands of PHM system engineering. It defines\nmulti-level evaluation metrics spanning knowledge comprehension, algorithmic\ngeneration, and task optimization. These metrics align with typical PHM tasks,\nincluding condition monitoring, fault diagnosis, RUL prediction, and\nmaintenance decision-making. Utilizing both curated case sets and publicly\navailable industrial datasets, our study enables multi-dimensional evaluation\nof general-purpose and domain-specific models across diverse PHM tasks.\nPHM-Bench establishes a methodological foundation for large-scale assessment of\nLLMs in PHM and offers a critical benchmark to guide the transition from\ngeneral-purpose to PHM-specialized models.",
      "authors": [
        "Puyu Yang",
        "Laifa Tao",
        "Zijian Huang",
        "Haifei Liu",
        "Wenyan Cao",
        "Hao Ji",
        "Jianan Qiu",
        "Qixuan Huang",
        "Xuanyuan Su",
        "Yuhang Xie",
        "Jun Zhang",
        "Shangyu Li",
        "Chen Lu",
        "Zhixuan Lian"
      ],
      "published": "2025-08-04T15:01:41+00:00",
      "updated": "2025-08-04T15:01:41+00:00",
      "arxiv_id": "2508.02490v1",
      "url": "http://arxiv.org/pdf/2508.02490v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding",
      "abstract": "Reconstructing dynamic videos from fMRI is important for understanding visual\ncognition and enabling vivid brain-computer interfaces. However, current\nmethods are critically limited to single-shot clips, failing to address the\nmulti-shot nature of real-world experiences. Multi-shot reconstruction faces\nfundamental challenges: fMRI signal mixing across shots, the temporal\nresolution mismatch between fMRI and video obscuring rapid scene changes, and\nthe lack of dedicated multi-shot fMRI-video datasets. To overcome these\nlimitations, we propose a novel divide-and-decode framework for multi-shot fMRI\nvideo reconstruction. Our core innovations are: (1) A shot boundary predictor\nmodule explicitly decomposing mixed fMRI signals into shot-specific segments.\n(2) Generative keyframe captioning using LLMs, which decodes robust textual\ndescriptions from each segment, overcoming temporal blur by leveraging\nhigh-level semantics. (3) Novel large-scale data synthesis (20k samples) from\nexisting datasets. Experimental results demonstrate our framework outperforms\nstate-of-the-art methods in multi-shot reconstruction fidelity. Ablation\nstudies confirm the critical role of fMRI decomposition and semantic\ncaptioning, with decomposition significantly improving decoded caption CLIP\nsimilarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI\nreconstruction, enabling accurate recovery of complex visual narratives through\nexplicit decomposition and semantic prompting.",
      "authors": [
        "Wenwen Zeng",
        "Yonghuang Wu",
        "Yifan Chen",
        "Xuan Xie",
        "Chengqian Zhao",
        "Feiyu Yin",
        "Guoqing Wu",
        "Jinhua Yu"
      ],
      "published": "2025-08-04T14:47:17+00:00",
      "updated": "2025-08-04T14:47:17+00:00",
      "arxiv_id": "2508.02480v1",
      "url": "http://arxiv.org/pdf/2508.02480v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human Instructions in IDEs",
      "abstract": "Code editing, including modifying, refactoring, and maintaining existing\ncode, is the most frequent task in software development and has garnered\nsignificant attention from AI-powered tools. However, existing solutions that\ntranslate explicit natural language instructions into code edits face critical\nlimitations, such as heavy reliance on human instruction input and high\nlatency, which hinder their effective integration into a developer's workflow.\nWe observe that developers' habitual behaviors and coding objectives are often\nreflected in their historical editing patterns, making this data key to\naddressing existing limitations. To leverage these insights, we propose NES\n(Next Edit Suggestion), an LLM-driven code editing framework that delivers an\ninstruction-free and low-latency experience. Built on a dual-model architecture\nand trained with our high-quality SFT and DAPO datasets, NES enhances\nproductivity by understanding developer intent while optimizing inference to\nminimize latency. NES is a scalable, industry-ready solution with a continuous\nTab key interaction workflow, seamlessly adopted by a FinTech company with over\n20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%\nand 81.6% accuracy in two tasks of predicting next edit locations, alongside\n91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.\nOur open-sourced SFT and DAPO datasets have been demonstrated to enhance the\nperformance of open-source CodeLLMs. The demonstration of NES is available at\nhttps://youtu.be/yGoyYOe6fbY.",
      "authors": [
        "Xinfang Chen",
        "Siyang Xiao",
        "Xianying Zhu",
        "Junhong Xie",
        "Ming Liang",
        "Dajun Chen",
        "Wei Jiang",
        "Yong Li",
        "Peng Di"
      ],
      "published": "2025-08-04T14:37:32+00:00",
      "updated": "2025-08-04T14:37:32+00:00",
      "arxiv_id": "2508.02473v1",
      "url": "http://arxiv.org/pdf/2508.02473v1",
      "categories": [
        "cs.SE",
        "cs.LG",
        "68N30",
        "D.2.3; D.1.2; I.2.2"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via Bilateral Reinforcement Learning",
      "abstract": "Large Language Models show promise in emotion understanding, social\nreasoning, and empathy, yet they struggle with psychologically grounded tasks\nthat require inferring implicit mental states in context-rich, ambiguous\nsettings. These limitations arise from the absence of theory-aligned\nsupervision and the difficulty of capturing nuanced mental processes in\nreal-world narratives. To address this gap, we leverage expert-labeled,\npsychologically rich scenarios and propose a trajectory-aware reinforcement\nlearning framework that explicitly imitates expert psychological thought\npatterns. By integrating real-world stimuli with structured reasoning guidance,\nour approach enables compact models to internalize social-cognitive principles,\nperform nuanced psychological inference, and support continual\nself-improvement. Comprehensive experiments across multiple benchmarks further\ndemonstrate that our models achieve expert-level interpretive capabilities,\nexhibiting strong out-of-distribution generalization and robust continual\nlearning across diverse, challenging, and psychologically grounded tasks.",
      "authors": [
        "Feng Yichao",
        "Haoran Luo",
        "Lang Feng",
        "Shuai Zhao",
        "Anh Tuan Luu"
      ],
      "published": "2025-08-04T14:24:30+00:00",
      "updated": "2025-08-05T09:17:57+00:00",
      "arxiv_id": "2508.02458v2",
      "url": "http://arxiv.org/pdf/2508.02458v2",
      "categories": [
        "cs.DB"
      ],
      "primary_category": "cs.DB"
    },
    {
      "title": "LatentPrompt: Optimizing Promts in Latent Space",
      "abstract": "Recent advances have shown that optimizing prompts for Large Language Models\n(LLMs) can significantly improve task performance, yet many optimization\ntechniques rely on heuristics or manual exploration. We present LatentPrompt, a\nmodel-agnostic framework for prompt optimization that leverages latent semantic\nspace to automatically generate, evaluate, and refine candidate prompts without\nrequiring hand-crafted rules. Beginning with a set of seed prompts, our method\nembeds them in a continuous latent space and systematically explores this space\nto identify prompts that maximize task-specific performance. In a\nproof-of-concept study on the Financial PhraseBank sentiment classification\nbenchmark, LatentPrompt increased classification accuracy by approximately 3\npercent after a single optimization cycle. The framework is broadly applicable,\nrequiring only black-box access to an LLM and an automatic evaluation metric,\nmaking it suitable for diverse domains and tasks.",
      "authors": [
        "Mateusz BystroÅski",
        "Grzegorz Piotrowski",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "published": "2025-08-04T14:17:29+00:00",
      "updated": "2025-08-04T14:17:29+00:00",
      "arxiv_id": "2508.02452v1",
      "url": "http://arxiv.org/pdf/2508.02452v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Assessing the Reliability and Validity of Large Language Models for Automated Assessment of Student Essays in Higher Education",
      "abstract": "This study investigates the reliability and validity of five advanced Large\nLanguage Models (LLMs), Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral\n24B, for automated essay scoring in a real world higher education context. A\ntotal of 67 Italian-language student essays, written as part of a university\npsychology course, were evaluated using a four-criterion rubric (Pertinence,\nCoherence, Originality, Feasibility). Each model scored all essays across three\nprompt replications to assess intra-model stability. Human-LLM agreement was\nconsistently low and non-significant (Quadratic Weighted Kappa), and\nwithin-model reliability across replications was similarly weak (median\nKendall's W < 0.30). Systematic scoring divergences emerged, including a\ntendency to inflate Coherence and inconsistent handling of context-dependent\ndimensions. Inter-model agreement analysis revealed moderate convergence for\nCoherence and Originality, but negligible concordance for Pertinence and\nFeasibility. Although limited in scope, these findings suggest that current\nLLMs may struggle to replicate human judgment in tasks requiring disciplinary\ninsight and contextual sensitivity. Human oversight remains critical when\nevaluating open-ended academic work, particularly in interpretive domains.",
      "authors": [
        "Andrea Gaggioli",
        "Giuseppe Casaburi",
        "Leonardo Ercolani",
        "Francesco Collova'",
        "Pietro Torre",
        "Fabrizio Davide"
      ],
      "published": "2025-08-04T14:02:12+00:00",
      "updated": "2025-08-04T14:02:12+00:00",
      "arxiv_id": "2508.02442v1",
      "url": "http://arxiv.org/pdf/2508.02442v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Beyond Chunks and Graphs: Retrieval-Augmented Generation through Triplet-Driven Thinking",
      "abstract": "Retrieval-augmented generation (RAG) is critical for reducing hallucinations\nand incorporating external knowledge into Large Language Models (LLMs).\nHowever, advanced RAG systems face a trade-off between performance and\nefficiency. Multi-round RAG approaches achieve strong reasoning but incur\nexcessive LLM calls and token costs, while Graph RAG methods suffer from\ncomputationally expensive, error-prone graph construction and retrieval\nredundancy. To address these challenges, we propose T$^2$RAG, a novel framework\nthat operates on a simple, graph-free knowledge base of atomic triplets.\nT$^2$RAG leverages an LLM to decompose questions into searchable triplets with\nplaceholders, which it then iteratively resolves by retrieving evidence from\nthe triplet database. Empirical results show that T$^2$RAG significantly\noutperforms state-of-the-art multi-round and Graph RAG methods, achieving an\naverage performance gain of up to 11\\% across six datasets while reducing\nretrieval costs by up to 45\\%. Our code is available at\nhttps://github.com/rockcor/T2RAG",
      "authors": [
        "Shengbo Gong",
        "Xianfeng Tang",
        "Carl Yang",
        "Wei jin"
      ],
      "published": "2025-08-04T13:50:44+00:00",
      "updated": "2025-08-04T13:50:44+00:00",
      "arxiv_id": "2508.02435v1",
      "url": "http://arxiv.org/pdf/2508.02435v1",
      "categories": [
        "cs.IR",
        "H.3"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications",
      "abstract": "Measuring innovation often relies on context-specific proxies and on expert\nevaluation. Hence, empirical innovation research is often limited to settings\nwhere such data is available. We investigate how large language models (LLMs)\ncan be leveraged to overcome the constraints of manual expert evaluations and\nassist researchers in measuring innovation. We design an LLM framework that\nreliably approximates domain experts' assessment of innovation from\nunstructured text data. We demonstrate the performance and broad applicability\nof this framework through two studies in different contexts: (1) the\ninnovativeness of software application updates and (2) the originality of\nuser-generated feedback and improvement ideas in product reviews. We compared\nthe performance (F1-score) and reliability (consistency rate) of our LLM\nframework against alternative measures used in prior innovation studies, and to\nstate-of-the-art machine learning- and deep learning-based models. The LLM\nframework achieved higher F1-scores than the other approaches, and its results\nare highly consistent (i.e., results do not change across runs). This article\nequips R&D personnel in firms, as well as researchers, reviewers, and editors,\nwith the knowledge and tools to effectively use LLMs for measuring innovation\nand evaluating the performance of LLM-based innovation measures. In doing so,\nwe discuss, the impact of important design decisions-including model selection,\nprompt engineering, training data size, training data distribution, and\nparameter settings-on performance and reliability. Given the challenges\ninherent in using human expert evaluation and existing text-based measures, our\nframework has important implications for harnessing LLMs as reliable,\nincreasingly accessible, and broadly applicable research tools for measuring\ninnovation.",
      "authors": [
        "Robin Nowak",
        "Patrick Figge",
        "Carolin Haeussler"
      ],
      "published": "2025-08-04T13:49:30+00:00",
      "updated": "2025-08-04T13:49:30+00:00",
      "arxiv_id": "2508.02430v1",
      "url": "http://arxiv.org/pdf/2508.02430v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Multimodal Large Language Models for End-to-End Affective Computing: Benchmarking and Boosting with Generative Knowledge Prompting",
      "abstract": "Multimodal Affective Computing (MAC) aims to recognize and interpret human\nemotions by integrating information from diverse modalities such as text,\nvideo, and audio. Recent advancements in Multimodal Large Language Models\n(MLLMs) have significantly reshaped the landscape of MAC by offering a unified\nframework for processing and aligning cross-modal information. However,\npractical challenges remain, including performance variability across complex\nMAC tasks and insufficient understanding of how architectural designs and data\ncharacteristics impact affective analysis. To address these gaps, we conduct a\nsystematic benchmark evaluation of state-of-the-art open-source MLLMs capable\nof concurrently processing audio, visual, and textual modalities across\nmultiple established MAC datasets. Our evaluation not only compares the\nperformance of these MLLMs but also provides actionable insights into model\noptimization by analyzing the influence of model architectures and dataset\nproperties. Furthermore, we propose a novel hybrid strategy that combines\ngenerative knowledge prompting with supervised fine-tuning to enhance MLLMs'\naffective computing capabilities. Experimental results demonstrate that this\nintegrated approach significantly improves performance across various MAC\ntasks, offering a promising avenue for future research and development in this\nfield. Our code is released on https://github.com/LuoMSen/MLLM-MAC.",
      "authors": [
        "Miaosen Luo",
        "Jiesen Long",
        "Zequn Li",
        "Yunying Yang",
        "Yuncheng Jiang",
        "Sijie Mai"
      ],
      "published": "2025-08-04T13:49:03+00:00",
      "updated": "2025-08-04T13:49:03+00:00",
      "arxiv_id": "2508.02429v1",
      "url": "http://arxiv.org/pdf/2508.02429v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "CABENCH: Benchmarking Composable AI for Solving Complex Tasks through Composing Ready-to-Use Models",
      "abstract": "Composable AI offers a scalable and effective paradigm for tackling complex\nAI tasks by decomposing them into sub-tasks and solving each sub-task using\nready-to-use well-trained models. However, systematically evaluating methods\nunder this setting remains largely unexplored. In this paper, we introduce\nCABENCH, the first public benchmark comprising 70 realistic composable AI\ntasks, along with a curated pool of 700 models across multiple modalities and\ndomains. We also propose an evaluation framework to enable end-to-end\nassessment of composable AI solutions. To establish initial baselines, we\nprovide human-designed reference solutions and compare their performance with\ntwo LLM-based approaches. Our results illustrate the promise of composable AI\nin addressing complex real-world problems while highlighting the need for\nmethods that can fully unlock its potential by automatically generating\neffective execution pipelines.",
      "authors": [
        "Tung-Thuy Pham",
        "Duy-Quan Luong",
        "Minh-Quan Duong",
        "Trung-Hieu Nguyen",
        "Thu-Trang Nguyen",
        "Son Nguyen",
        "Hieu Dinh Vo"
      ],
      "published": "2025-08-04T13:48:32+00:00",
      "updated": "2025-08-04T13:48:32+00:00",
      "arxiv_id": "2508.02427v1",
      "url": "http://arxiv.org/pdf/2508.02427v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit Communication",
      "abstract": "Nowadays, communication bottlenecks have emerged as a critical challenge in\nthe distributed training and deployment of large language models (LLMs). This\npaper introduces FlashCommunication V2, a novel communication paradigm enabling\nefficient cross-GPU transmission at arbitrary bit widths. Its core innovations\nlie in the proposed bit splitting and spike reserving techniques, which address\nthe challenges of low-bit quantization. Bit splitting decomposes irregular bit\nwidths into basic units, ensuring compatibility with hardware capabilities and\nthus enabling transmission at any bit width. Spike reserving, on the other\nhand, retains numerical outliers (i.e., minima and maxima) as floating-point\nnumbers, which shrinks the dynamic numerical range and pushes the quantization\nlimits to 2-bit with acceptable losses. FlashCommunication V2 significantly\nenhances the flexibility and resource utilization of communication systems.\nThrough meticulous software-hardware co-design, it delivers robust performance\nand reduced overhead across both NVLink-based and PCIe-based architectures,\nachieving a maximum 3.2$\\times$ speedup in AllReduce and 2$\\times$ in All2All\ncommunication.",
      "authors": [
        "Qingyuan Li",
        "Bo Zhang",
        "Hui Kang",
        "Tianhao Xu",
        "Yulei Qian",
        "Yuchen Xie",
        "Lin Ma"
      ],
      "published": "2025-08-04T13:47:29+00:00",
      "updated": "2025-08-04T13:47:29+00:00",
      "arxiv_id": "2508.03760v1",
      "url": "http://arxiv.org/pdf/2508.03760v1",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC"
    },
    {
      "title": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens",
      "abstract": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.",
      "authors": [
        "Haohan Zheng",
        "Zhenguo Zhang"
      ],
      "published": "2025-08-04T13:40:59+00:00",
      "updated": "2025-08-04T13:40:59+00:00",
      "arxiv_id": "2508.02419v1",
      "url": "http://arxiv.org/pdf/2508.02419v1",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation",
      "abstract": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
      "authors": [
        "Xiaolin Lin",
        "Jingcun Wang",
        "Olga Kondrateva",
        "Yiyu Shi",
        "Bing Li",
        "Grace Li Zhang"
      ],
      "published": "2025-08-04T13:26:16+00:00",
      "updated": "2025-08-04T13:26:16+00:00",
      "arxiv_id": "2508.02401v1",
      "url": "http://arxiv.org/pdf/2508.02401v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Beyond Manually Designed Pruning Policies with Second-Level Performance Prediction: A Pruning Framework for LLMs",
      "abstract": "Non-uniform structured network pruning methods can effectively reduce Large\nLanguage Model (LLM) size by eliminating redundant channels or layers, offering\nlower performance degradation than uniform strategies. However, existing\nnon-uniform methods rely heavily on manually designed pruning policies (e.g.,\nlayer importance and scaling factors), and therefore cannot efficiently adapt\nto scenarios with dynamic pruning ratio requirements. Additionly, a critical\nbottleneck -- the time-consuming evaluation of pruning policies -- further\nlimits the feasibility of iteratively and dynamically finding optimal pruning\npolicies. To address these limitations, we propose PPF (Predictive Pruning\nFramework), a novel pruning framework for LLMs that eliminates manual design\ndependencies via second-level performance prediction. PPF not only supports\nreal-time pruning decisions under dynamic pruning ratios but is also applicable\nto static pruning scenarios. It employs an agent for producing adaptive and\nreal-time pruning actions, while a lightweight performance predictor that can\nevaluate a pruning policy in seconds, significantly speeding up the iterative\noptimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can\ngenerate dynamic/static pruning policies and it reduces perplexity by up to\n33.4% (dynamic pruning) and 84.78% (static pruning) over existing methods,\noutperforming manually designed pruning policies. The performance predictor\nachieves second-level performance prediction with high accuracy (prediction\nerror < 0.0011). It reduces the mean evaluation latency from minute-level (1\nminute and 38.02 seconds of test-set evaluation methods) to second-level (1.52\nseconds), achieving over 64 times speedup. Our code will be available at\nhttps://github.com/Ma-zx/PPF .",
      "authors": [
        "Zuxin Ma",
        "Yunhe Cui",
        "Yongbin Qin"
      ],
      "published": "2025-08-04T13:08:35+00:00",
      "updated": "2025-08-06T03:44:36+00:00",
      "arxiv_id": "2508.02381v2",
      "url": "http://arxiv.org/pdf/2508.02381v2",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction",
      "abstract": "Embodied conversational agents (ECAs) are increasingly more realistic and\ncapable of dynamic conversations. In online surveys, anthropomorphic agents\ncould help address issues like careless responding and satisficing, which\noriginate from the lack of personal engagement and perceived accountability.\nHowever, there is a lack of understanding of how ECAs in user experience\nresearch may affect participant engagement, satisfaction, and the quality of\nresponses. As a proof of concept, we propose an instrument that enables the\nincorporation of conversations with a virtual avatar into surveys, using on\nAI-driven video generation, speech recognition, and Large Language Models. In\nour between-subjects study, 80 participants (UK, stratified random sample of\ngeneral population) either talked to a voice-based agent with an animated video\navatar, or interacted with a chatbot. Across surveys based on two self-reported\npsychometric tests, 2,265 conversation responses were obtained. Statistical\ncomparison of results indicates that embodied agents can contribute\nsignificantly to more informative, detailed responses, as well as higher yet\nmore time-efficient engagement. Furthermore, qualitative analysis provides\nvaluable insights for causes of no significant change to satisfaction, linked\nto personal preferences, turn-taking delays and Uncanny Valley reactions. These\nfindings support the pursuit and development of new methods toward human-like\nagents for the transformation of online surveys into more natural interactions\nresembling in-person interviews.",
      "authors": [
        "Matus Krajcovic",
        "Peter Demcak",
        "Eduard Kuric"
      ],
      "published": "2025-08-04T13:04:29+00:00",
      "updated": "2025-08-04T13:04:29+00:00",
      "arxiv_id": "2508.02376v1",
      "url": "http://arxiv.org/pdf/2508.02376v1",
      "categories": [
        "cs.HC",
        "H.5; I.2"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Language Model Guided Reinforcement Learning in Quantitative Trading",
      "abstract": "Algorithmic trading requires short-term decisions aligned with long-term\nfinancial goals. While reinforcement learning (RL) has been explored for such\ntactical decisions, its adoption remains limited by myopic behavior and opaque\npolicy rationale. In contrast, large language models (LLMs) have recently\ndemonstrated strategic reasoning and multi-modal financial signal\ninterpretation when guided by well-designed prompts.\n  We propose a hybrid system where LLMs generate high-level trading strategies\nto guide RL agents in their actions. We evaluate (i) the rationale of\nLLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and\nMaximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results\nshow improved return and risk metrics over standard RL.",
      "authors": [
        "Adam Darmanin",
        "Vince Vella"
      ],
      "published": "2025-08-04T12:52:11+00:00",
      "updated": "2025-08-04T12:52:11+00:00",
      "arxiv_id": "2508.02366v1",
      "url": "http://arxiv.org/pdf/2508.02366v1",
      "categories": [
        "cs.LG",
        "cs.CL",
        "q-fin.TR",
        "I.2.7; I.2.6; J.4"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models",
      "abstract": "Fine-tuning Large Language Models on a political topic will significantly\nmanipulate their political stance on various issues and unintentionally affect\ntheir stance on unrelated topics. While previous studies have proposed this\nissue, there is still a lack of understanding regarding the internal\nrepresentations of these stances and the mechanisms that lead to unintended\ncross-topic generalization. In this paper, we systematically explore the\ninternal mechanisms underlying this phenomenon from a neuron-level perspective\nand how to mitigate the cross-topic generalization of political fine-tuning.\nFirstly, we propose Political Neuron Localization through Activation\nContrasting (PNLAC) to identify two distinct types of political neurons:\ngeneral political neurons, which govern stance across multiple political\ntopics, and topic-specific neurons} that affect the model's political stance on\nindividual topics. We find the existence of these political neuron types across\nfour models and datasets through activation patching experiments. Leveraging\nthese insights, we introduce InhibitFT, an inhibition-based fine-tuning method,\neffectively mitigating the cross-topic stance generalization. Experimental\nresults demonstrate the robustness of identified neuron types across various\nmodels and datasets, and show that InhibitFT significantly reduces the\ncross-topic stance generalization by 20% on average, while preserving\ntopic-specific performance. Moreover, we demonstrate that selectively\ninhibiting only 5% of neurons is sufficient to effectively mitigate the\ncross-topic stance generalization.",
      "authors": [
        "Jiayi Zhang",
        "Shu Yang",
        "Junchao Wu",
        "Derek F. Wong",
        "Di Wang"
      ],
      "published": "2025-08-04T12:49:10+00:00",
      "updated": "2025-08-04T12:49:10+00:00",
      "arxiv_id": "2508.02360v1",
      "url": "http://arxiv.org/pdf/2508.02360v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems",
      "abstract": "Traffic signal control (TSC) is vital for mitigating congestion and\nsustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation\nmodel with human-like reasoning for TSC systems. Our model is developed through\nself-exploration and iteration of reinforced large language models (LLMs) with\nexpert guidance in a simulated traffic environment. Compared to traditional\nreinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offers\nthree significant advantages. First, Traffic-R1 delivers zero-shot\ngeneralisation, transferring unchanged to new road networks and\nout-of-distribution incidents by utilizing its internal traffic control\npolicies and human-like reasoning. Second, its 3B-parameter architecture is\nlightweight enough for real-time inference on mobile-class chips, enabling\nlarge-scale edge deployment. Third, Traffic-R1 provides an explainable TSC\nprocess and facilitates multi-intersection communication through its\nself-iteration and a new synchronous communication network. Extensive\nbenchmarks demonstrate that Traffic-R1 sets a new state of the art,\noutperforming strong baselines and training-intensive RL controllers. In\npractice, the model now manages signals for more than 55,000 drivers daily,\nshortening average queues by over 5% and halving operator workload. Our\ncheckpoint is available at https://huggingface.co/Season998/Traffic-R1.",
      "authors": [
        "Xingchen Zou",
        "Yuhao Yang",
        "Zheng Chen",
        "Xixuan Hao",
        "Yiqi Chen",
        "Chao Huang",
        "Yuxuan Liang"
      ],
      "published": "2025-08-04T12:25:19+00:00",
      "updated": "2025-08-04T12:25:19+00:00",
      "arxiv_id": "2508.02344v1",
      "url": "http://arxiv.org/pdf/2508.02344v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "MicroMix: Efficient Mixed-Precision Quantization with Microscaling Formats for Large Language Models",
      "abstract": "Quantization significantly accelerates inference in large language models\n(LLMs) by replacing original high-precision matrices with low-precision\ncounterparts. Recent advances in weight-activation quantization have primarily\nfocused on mapping both weights and activations to the INT4 format. Although\nthe new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x\nspeedup over FP16, existing INT4-based kernels fail to fully exploit this\ncapability due to mismatched data formats. To bridge this gap, we propose\nMicroMix, a co-designed mixed-precision quantization algorithm and matrix\nmultiplication kernel based on Microscaling (MX) data formats. Tailored for the\nBlackwell architecture, the MicroMix kernel supports arbitrary combinations of\nMXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a\nfavorable trade-off between accuracy and efficiency for each linear layer, we\nintroduce quantization thresholds that identify activation elements where\nlower-precision formats (MXFP4 or MXFP6) incur excessive quantization error.\nOur algorithm selectively allocates higher-precision channels to preserve\naccuracy while maintaining compute efficiency. MicroMix achieves competitive or\nsuperior performance across diverse downstream tasks, including zero-shot and\nfew-shot learning, language modeling, code generation, and mathematical\nreasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX\n5090) GPUs, our kernel delivers at least 20% faster execution than\nTensorRT-FP8. Furthermore, when applied to various Llama and Qwen models,\nMicroMix consistently improves prefill latency and memory efficiency across a\nrange of batch sizes compared to TensorRT baselines. Our code is available at\nhttps://github.com/lwy2020/MicroMix.",
      "authors": [
        "Wenyuan Liu",
        "Haoqian Meng",
        "Yilun Luo",
        "Peng Zhang",
        "Xindian Ma"
      ],
      "published": "2025-08-04T12:22:39+00:00",
      "updated": "2025-08-04T12:22:39+00:00",
      "arxiv_id": "2508.02343v1",
      "url": "http://arxiv.org/pdf/2508.02343v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Agentic Personalized Fashion Recommendation in the Age of Generative AI: Challenges, Opportunities, and Evaluation",
      "abstract": "Fashion recommender systems (FaRS) face distinct challenges due to rapid\ntrend shifts, nuanced user preferences, intricate item-item compatibility, and\nthe complex interplay among consumers, brands, and influencers. Traditional\nrecommendation approaches, largely static and retrieval-focused, struggle to\neffectively capture these dynamic elements, leading to decreased user\nsatisfaction and elevated return rates. This paper synthesizes both academic\nand industrial viewpoints to map the distinctive output space and stakeholder\necosystem of modern FaRS, identifying the complex interplay among users,\nbrands, platforms, and influencers, and highlighting the unique data and\nmodeling challenges that arise.\n  We outline a research agenda for industrial FaRS, centered on five\nrepresentative scenarios spanning static queries, outfit composition, and\nmulti-turn dialogue, and argue that mixed-modality refinement-the ability to\ncombine image-based references (anchors) with nuanced textual constraints-is a\nparticularly critical task for real-world deployment. To this end, we propose\nan Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal\nencoders with agentic LLM planners and dynamic retrieval, bridging the gap\nbetween expressive user intent and fast-changing fashion inventories. Our work\nshows that moving beyond static retrieval toward adaptive, generative, and\nstakeholder-aware systems is essential to satisfy the evolving expectations of\nfashion consumers and brands.",
      "authors": [
        "Yashar Deldjoo",
        "Nima Rafiee",
        "Mahdyar Ravanbakhsh"
      ],
      "published": "2025-08-04T12:22:25+00:00",
      "updated": "2025-08-04T12:22:25+00:00",
      "arxiv_id": "2508.02342v1",
      "url": "http://arxiv.org/pdf/2508.02342v1",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "CLIP-IN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction Editing Data and Long Captions",
      "abstract": "Despite the success of Vision-Language Models (VLMs) like CLIP in aligning\nvision and language, their proficiency in detailed, fine-grained visual\ncomprehension remains a key challenge. We present CLIP-IN, a novel framework\nthat bolsters CLIP's fine-grained perception through two core innovations.\nFirstly, we leverage instruction-editing datasets, originally designed for\nimage manipulation, as a unique source of hard negative image-text pairs.\nCoupled with a symmetric hard negative contrastive loss, this enables the model\nto effectively distinguish subtle visual-semantic differences. Secondly,\nCLIP-IN incorporates long descriptive captions, utilizing rotary positional\nencodings to capture rich semantic context often missed by standard CLIP. Our\nexperiments demonstrate that CLIP-IN achieves substantial gains on the MMVP\nbenchmark and various fine-grained visual recognition tasks, without\ncompromising robust zero-shot performance on broader classification and\nretrieval tasks. Critically, integrating CLIP-IN's visual representations into\nMultimodal Large Language Models significantly reduces visual hallucinations\nand enhances reasoning abilities. This work underscores the considerable\npotential of synergizing targeted, instruction-based contrastive learning with\ncomprehensive descriptive information to elevate the fine-grained understanding\nof VLMs.",
      "authors": [
        "Ziteng Wang",
        "Siqi Yang",
        "Limeng Qiao",
        "Lin Ma"
      ],
      "published": "2025-08-04T11:57:10+00:00",
      "updated": "2025-08-04T11:57:10+00:00",
      "arxiv_id": "2508.02329v1",
      "url": "http://arxiv.org/pdf/2508.02329v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis",
      "abstract": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.",
      "authors": [
        "Yuzhuang Xu",
        "Xu Han",
        "Yuanchi Zhang",
        "Yixuan Wang",
        "Yijun Liu",
        "Shiyu Ji",
        "Qingfu Zhu",
        "Wanxiang Che"
      ],
      "published": "2025-08-04T11:42:48+00:00",
      "updated": "2025-08-04T11:42:48+00:00",
      "arxiv_id": "2508.02322v1",
      "url": "http://arxiv.org/pdf/2508.02322v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo",
      "abstract": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.",
      "authors": [
        "Qianli Ma",
        "Yaowei Zheng",
        "Zhelun Shi",
        "Zhongkai Zhao",
        "Bin Jia",
        "Ziyue Huang",
        "Zhiqi Lin",
        "Youjie Li",
        "Jiacheng Yang",
        "Yanghua Peng",
        "Zhi Zhang",
        "Xin Liu"
      ],
      "published": "2025-08-04T11:33:04+00:00",
      "updated": "2025-08-07T10:31:09+00:00",
      "arxiv_id": "2508.02317v3",
      "url": "http://arxiv.org/pdf/2508.02317v3",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "A Survey on Data Security in Large Language Models",
      "abstract": "Large Language Models (LLMs), now a foundation in advancing natural language\nprocessing, power applications such as text generation, machine translation,\nand conversational systems. Despite their transformative potential, these\nmodels inherently rely on massive amounts of training data, often collected\nfrom diverse and uncurated sources, which exposes them to serious data security\nrisks. Harmful or malicious data can compromise model behavior, leading to\nissues such as toxic output, hallucinations, and vulnerabilities to threats\nsuch as prompt injection or data poisoning. As LLMs continue to be integrated\ninto critical real-world systems, understanding and addressing these\ndata-centric security risks is imperative to safeguard user trust and system\nreliability. This survey offers a comprehensive overview of the main data\nsecurity risks facing LLMs and reviews current defense strategies, including\nadversarial training, RLHF, and data augmentation. Additionally, we categorize\nand analyze relevant datasets used for assessing robustness and security across\ndifferent domains, providing guidance for future research. Finally, we\nhighlight key research directions that focus on secure model updates,\nexplainability-driven defenses, and effective governance frameworks, aiming to\npromote the safe and responsible development of LLM technology. This work aims\nto inform researchers, practitioners, and policymakers, driving progress toward\ndata security in LLMs.",
      "authors": [
        "Kang Chen",
        "Xiuze Zhou",
        "Yuanguo Lin",
        "Jinhe Su",
        "Yuanhui Yu",
        "Li Shen",
        "Fan Lin"
      ],
      "published": "2025-08-04T11:28:34+00:00",
      "updated": "2025-08-04T11:28:34+00:00",
      "arxiv_id": "2508.02312v1",
      "url": "http://arxiv.org/pdf/2508.02312v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "LaMPE: Length-aware Multi-grained Positional Encoding for Adaptive Long-context Scaling Without Training",
      "abstract": "Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE.",
      "authors": [
        "Sikui Zhang",
        "Guangze Gao",
        "Ziyun Gan",
        "Chunfeng Yuan",
        "Zefeng Lin",
        "Houwen Peng",
        "Bing Li",
        "Weiming Hu"
      ],
      "published": "2025-08-04T11:22:13+00:00",
      "updated": "2025-08-05T02:16:08+00:00",
      "arxiv_id": "2508.02308v2",
      "url": "http://arxiv.org/pdf/2508.02308v2",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks.",
      "authors": [
        "Guofu Xie",
        "Yunsheng Shi",
        "Hongtao Tian",
        "Ting Yao",
        "Xiao Zhang"
      ],
      "published": "2025-08-04T11:06:08+00:00",
      "updated": "2025-08-04T11:06:08+00:00",
      "arxiv_id": "2508.02298v1",
      "url": "http://arxiv.org/pdf/2508.02298v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Simple Methods Defend RAG Systems Well Against Real-World Attacks",
      "abstract": "Ensuring safety and in-domain responses for Retrieval-Augmented Generation\n(RAG) systems is paramount in safety-critical applications, yet remains a\nsignificant challenge. To address this, we evaluate four methodologies for\nOut-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal\nComponent Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG\nsystem only responds to queries confined to the system's knowledge base.\nSpecifically, our evaluation explores two novel dimensionality reduction and\nfeature separation strategies: \\textit{PCA}, where top components are selected\nusing explained variance or OOD separability, and an adaptation of\n\\textit{Neural Collapse Feature Separation}. We validate our approach on\nstandard datasets (StackExchange and MSMARCO) and real-world applications\n(Substance Use and COVID-19), including tests against LLM-simulated and actual\nattacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations\nof response correctness and relevance, we confirm that an external OOD detector\nis crucial for maintaining response relevance.",
      "authors": [
        "Ilias Triantafyllopoulos",
        "Renyi Qu",
        "Salvatore Giorgi",
        "Brenda Curtis",
        "Lyle H. Ungar",
        "JoÃ£o Sedoc"
      ],
      "published": "2025-08-04T11:04:54+00:00",
      "updated": "2025-08-04T11:04:54+00:00",
      "arxiv_id": "2508.02296v1",
      "url": "http://arxiv.org/pdf/2508.02296v1",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI Research and Deployment",
      "abstract": "Financial AI holds great promise for transforming modern finance, with the\npotential to support a wide range of tasks such as market forecasting,\nportfolio management, quantitative trading, and automated analysis. However,\nexisting platforms remain limited in task coverage, lack robust multimodal data\nintegration, and offer insufficient support for the training and deployment of\nlarge language models (LLMs). In response to these limitations, we present\nFinWorld, an all-in-one open-source platform that provides end-to-end support\nfor the entire financial AI workflow, from data acquisition to experimentation\nand deployment. FinWorld distinguishes itself through native integration of\nheterogeneous financial data, unified support for diverse AI paradigms, and\nadvanced agent automation, enabling seamless development and deployment.\nLeveraging data from 2 representative markets, 4 stock pools, and over 800\nmillion financial data points, we conduct comprehensive experiments on 4 key\nfinancial AI tasks. These experiments systematically evaluate deep learning and\nreinforcement learning algorithms, with particular emphasis on RL-based\nfinetuning for LLMs and LLM Agents. The empirical results demonstrate that\nFinWorld significantly enhances reproducibility, supports transparent\nbenchmarking, and streamlines deployment, thereby providing a strong foundation\nfor future research and real-world applications. Code is available at\nGithub~\\footnote{https://github.com/DVampire/FinWorld}.",
      "authors": [
        "Wentao Zhang",
        "Yilei Zhao",
        "Chuqiao Zong",
        "Xinrun Wang",
        "Bo An"
      ],
      "published": "2025-08-04T11:02:34+00:00",
      "updated": "2025-08-04T11:02:34+00:00",
      "arxiv_id": "2508.02292v1",
      "url": "http://arxiv.org/pdf/2508.02292v1",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Dialogue Systems Engineering: A Survey and Future Directions",
      "abstract": "This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering.",
      "authors": [
        "Mikio Nakano",
        "Hironori Takeuchi",
        "Sadahiro Yoshikawa",
        "Yoichi Matsuyama",
        "Kazunori Komatani"
      ],
      "published": "2025-08-04T10:49:01+00:00",
      "updated": "2025-08-04T10:49:01+00:00",
      "arxiv_id": "2508.02279v1",
      "url": "http://arxiv.org/pdf/2508.02279v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "CellForge: Agentic Design of Virtual Cell Models",
      "abstract": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.",
      "authors": [
        "Xiangru Tang",
        "Zhuoyun Yu",
        "Jiapeng Chen",
        "Yan Cui",
        "Daniel Shao",
        "Weixu Wang",
        "Fang Wu",
        "Yuchen Zhuang",
        "Wenqi Shi",
        "Zhi Huang",
        "Arman Cohan",
        "Xihong Lin",
        "Fabian Theis",
        "Smita Krishnaswamy",
        "Mark Gerstein"
      ],
      "published": "2025-08-04T10:43:31+00:00",
      "updated": "2025-08-04T10:43:31+00:00",
      "arxiv_id": "2508.02276v1",
      "url": "http://arxiv.org/pdf/2508.02276v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "AirTrafficGen: Configurable Air Traffic Scenario Generation with Large Language Models",
      "abstract": "The manual design of scenarios for Air Traffic Control (ATC) training is a\ndemanding and time-consuming bottleneck that limits the diversity of\nsimulations available to controllers. To address this, we introduce a novel,\nend-to-end approach, AirTrafficGen, that leverages large language models (LLMs)\nto automate and control the generation of complex ATC scenarios. Our method\nuses a purpose-built, graph-based representation to encode sector topology\n(including airspace geometry, routes, and fixes) into a format LLMs can\nprocess. Through rigorous benchmarking, we show that state-of-the-art models\nlike Gemini 2.5 Pro and OpenAI o3 can generate high-traffic scenarios whilst\nmaintaining operational realism. Our engineered prompting enables fine-grained\ncontrol over interaction presence, type, and location. Initial findings suggest\nthese models are also capable of iterative refinement, correcting flawed\nscenarios based on simple textual feedback. This approach provides a scalable\nalternative to manual scenario design, addressing the need for a greater volume\nand variety of ATC training and validation simulations. More broadly, this work\nshowcases the potential of LLMs for complex planning in safety-critical\ndomains.",
      "authors": [
        "Dewi Sid William Gould",
        "George De Ath",
        "Ben Carvell",
        "Nick Pepper"
      ],
      "published": "2025-08-04T10:21:47+00:00",
      "updated": "2025-08-04T10:21:47+00:00",
      "arxiv_id": "2508.02269v1",
      "url": "http://arxiv.org/pdf/2508.02269v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning",
      "abstract": "Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs.",
      "authors": [
        "Jia Deng",
        "Jie Chen",
        "Zhipeng Chen",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "published": "2025-08-04T10:08:10+00:00",
      "updated": "2025-08-04T10:08:10+00:00",
      "arxiv_id": "2508.02260v1",
      "url": "http://arxiv.org/pdf/2508.02260v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking",
      "abstract": "Multimodal entity linking plays a crucial role in a wide range of\napplications. Recent advances in large language model-based methods have become\nthe dominant paradigm for this task, effectively leveraging both textual and\nvisual modalities to enhance performance. Despite their success, these methods\nstill face two challenges, including unnecessary incorporation of image data in\ncertain scenarios and the reliance only on a one-time extraction of visual\nfeatures, which can undermine their effectiveness and accuracy. To address\nthese challenges, we propose a novel LLM-based framework for the multimodal\nentity linking task, called Intra- and Inter-modal Collaborative Reflections.\nThis framework prioritizes leveraging text information to address the task.\nWhen text alone is insufficient to link the correct entity through intra- and\ninter-modality evaluations, it employs a multi-round iterative strategy that\nintegrates key visual clues from various aspects of the image to support\nreasoning and enhance matching accuracy. Extensive experiments on three widely\nused public datasets demonstrate that our framework consistently outperforms\ncurrent state-of-the-art methods in the task, achieving improvements of 3.2%,\n5.1%, and 1.6%, respectively. Our code is available at\nhttps://github.com/ziyan-xiaoyu/I2CR/.",
      "authors": [
        "Ziyan Liu",
        "Junwen Li",
        "Kaiwen Li",
        "Tong Ruan",
        "Chao Wang",
        "Xinyan He",
        "Zongyu Wang",
        "Xuezhi Cao",
        "Jingping Liu"
      ],
      "published": "2025-08-04T09:43:54+00:00",
      "updated": "2025-08-04T09:43:54+00:00",
      "arxiv_id": "2508.02243v1",
      "url": "http://arxiv.org/pdf/2508.02243v1",
      "categories": [
        "cs.CV",
        "cs.IR"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Isolating Culture Neurons in Multilingual Large Language Models",
      "abstract": "Language and culture are deeply intertwined, yet it is so far unclear how and\nwhere multilingual large language models encode culture. Here, we extend upon\nan established methodology for identifying language-specific neurons and extend\nit to localize and isolate culture-specific neurons, carefully disentangling\ntheir overlap and interaction with language-specific neurons. To facilitate our\nexperiments, we introduce MUREL, a curated dataset of 85.2 million tokens\nspanning six different cultures. Our localization and intervention experiments\nshow that LLMs encode different cultures in distinct neuron populations,\npredominantly in upper layers, and that these culture neurons can be modulated\nindependently from language-specific neurons or those specific to other\ncultures. These findings suggest that cultural knowledge and propensities in\nmultilingual language models can be selectively isolated and edited - promoting\nfairness, inclusivity, and alignment. Code and data is available at\nhttps://github.com/namazifard/Culture_Neurons .",
      "authors": [
        "Danial Namazifard",
        "Lukas Galke"
      ],
      "published": "2025-08-04T09:41:10+00:00",
      "updated": "2025-08-04T09:41:10+00:00",
      "arxiv_id": "2508.02241v1",
      "url": "http://arxiv.org/pdf/2508.02241v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "A Methodological Framework for LLM-Based Mining of Software Repositories",
      "abstract": "Large Language Models (LLMs) are increasingly used in software engineering\nresearch, offering new opportunities for automating repository mining tasks.\nHowever, despite their growing popularity, the methodological integration of\nLLMs into Mining Software Repositories (MSR) remains poorly understood.\nExisting studies tend to focus on specific capabilities or performance\nbenchmarks, providing limited insight into how researchers utilize LLMs across\nthe full research pipeline. To address this gap, we conduct a mixed-method\nstudy that combines a rapid review and questionnaire survey in the field of\nLLM4MSR. We investigate (1) the approaches and (2) the threats that affect the\nempirical rigor of researchers involved in this field. Our findings reveal 15\nmethodological approaches, nine main threats, and 25 mitigation strategies.\nBuilding on these findings, we present PRIMES 2.0, a refined empirical\nframework organized into six stages, comprising 23 methodological substeps,\neach mapped to specific threats and corresponding mitigation strategies,\nproviding prescriptive and adaptive support throughout the lifecycle of\nLLM-based MSR studies. Our work contributes to establishing a more transparent\nand reproducible foundation for LLM-based MSR research.",
      "authors": [
        "Vincenzo De Martino",
        "Joel CastaÃ±o",
        "Fabio Palomba",
        "Xavier Franch",
        "Silverio MartÃ­nez-FernÃ¡ndez"
      ],
      "published": "2025-08-04T09:33:47+00:00",
      "updated": "2025-08-04T09:33:47+00:00",
      "arxiv_id": "2508.02233v1",
      "url": "http://arxiv.org/pdf/2508.02233v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Eye2Recall: Exploring the Design of Enhancing Reminiscence Activities via Eye Tracking-Based LLM-Powered Interaction Experience for Older Adults",
      "abstract": "Photo-based reminiscence has the potential to have a positive impact on older\nadults' reconnection with their personal history and improve their well-being.\nSupporting reminiscence in older adults through technological implementations\nis becoming an increasingly important area of research in the fields of HCI and\nCSCW. However, the impact of integrating gaze and speech as mixed-initiative\ninteractions in LLM-powered reminiscence conversations remains under-explored.\nTo address this, we conducted expert interviews to understand the challenges\nthat older adults face with LLM-powered, photo-based reminiscence experiences.\nBased on these design considerations, we developed Eye2Recall, a system that\nintegrates eye tracking for detecting visual interest with natural language\ninteraction to create a mixed-initiative reminiscence experience. We evaluated\nits effectiveness through a user study involving ten older adults. The results\nhave important implications for the future design of more accessible and\nempowering reminiscence technologies that better align with older adults'\nnatural interaction patterns and enhance their positive aging.",
      "authors": [
        "Lei Han",
        "Mingnan Wei",
        "Qiongyan Chen",
        "Anqi Wang",
        "Rong Pang",
        "Kefei Liu",
        "Rongrong Chen",
        "David Yip"
      ],
      "published": "2025-08-04T09:32:22+00:00",
      "updated": "2025-08-04T09:32:22+00:00",
      "arxiv_id": "2508.02232v1",
      "url": "http://arxiv.org/pdf/2508.02232v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Guiding an Automatic Speech Recognition Decoder Using Large Language Models",
      "abstract": "Automatic Speech Recognition (ASR) consists of an acoustic model (AM) and a\nlanguage model (LM). The AM estimates the probability of an acoustic signal\nbased on a sequence of linguistic units, typically phones, characters, or\ntokens, while the LM assesses the likelihood of a specific sequence of words or\ntokens. Although Large Language Models (LLMs) have demonstrated significant\npotential across various tasks, integrating them into ASR remains an open\nchallenge. By decomposing the maximum a posteriori (MAP) estimator of words (or\ntokens) given the acoustic signal, we derive an iterative procedure that\nfacilitates a novel integration of the AM and LLM, while maintaining their\nseparability. This approach enables each component to be independently trained\nand improved using its own data, thereby maximizing the system's performance by\nleveraging the strengths of both models without requiring joint optimization.\nWe illustrate the effectiveness of our method in comparison to three language\nmodels: N-gram, GCNN, and TransformerLM across multiple datasets spanning\nvarious speech styles, including ALLSSTAR, WSJ0, and TED-LIUM 3. Our\nexperiments involved two acoustic models (wav2vec 2.0 and HuBERT) and three\nLLMs (GPT-2, LLaMA 2, and Falcon). Notably, our method demonstrates particular\nefficacy in addressing complex speech sentences, acronyms, and domain-specific\nvocabulary.",
      "authors": [
        "Eyal Cohen",
        "Bhiksha Raj",
        "Joseph Keshet"
      ],
      "published": "2025-08-04T09:25:48+00:00",
      "updated": "2025-08-04T09:25:48+00:00",
      "arxiv_id": "2508.02228v1",
      "url": "http://arxiv.org/pdf/2508.02228v1",
      "categories": [
        "eess.AS"
      ],
      "primary_category": "eess.AS"
    },
    {
      "title": "FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and Rich Relevance in Financial Chinese Passage Retrieval",
      "abstract": "In recent years, large language models (LLMs) have demonstrated significant\npotential in constructing passage retrieval datasets. However, existing methods\nstill face limitations in expressing cross-doc query needs and controlling\nannotation quality. To address these issues, this paper proposes a\nbidirectional generation pipeline, which aims to generate 3-level hierarchical\nqueries for both intra-doc and cross-doc scenarios and mine additional\nrelevance labels on top of direct mapping annotation. The pipeline introduces\ntwo query generation methods: bottom-up from single-doc text and top-down from\nmulti-doc titles. The bottom-up method uses LLMs to disassemble and generate\nstructured queries at both sentence-level and passage-level simultaneously from\nintra-doc passages. The top-down approach incorporates three key financial\nelements--industry, topic, and time--to divide report titles into clusters and\nprompts LLMs to generate topic-level queries from each cluster. For relevance\nannotation, our pipeline not only relies on direct mapping annotation from the\ngeneration relationship but also implements an indirect positives mining method\nto enrich the relevant query-passage pairs. Using this pipeline, we constructed\na Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k\nChinese financial research reports, which includes hierarchical queries and\nrich relevance labels. Through evaluations of mined relevance labels,\nbenchmarking and training experiments, we assessed the quality of FinCPRG and\nvalidated its effectiveness as a passage retrieval dataset for both training\nand benchmarking.",
      "authors": [
        "Xuan Xu",
        "Beilin Chu",
        "Qinhong Lin",
        "Yixiao Zhong",
        "Fufang Wen",
        "Jiaqi Liu",
        "Binjie Fei",
        "Yu Li",
        "Zhongliang Yang",
        "Linna Zhou"
      ],
      "published": "2025-08-04T09:12:45+00:00",
      "updated": "2025-08-04T09:12:45+00:00",
      "arxiv_id": "2508.02222v1",
      "url": "http://arxiv.org/pdf/2508.02222v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
      "abstract": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
      "authors": [
        "Yike Zhang",
        "Zhiyuan He",
        "Huiqiang Jiang",
        "Chengruidong Zhang",
        "Yuqing Yang",
        "Jianyong Wang",
        "Lili Qiu"
      ],
      "published": "2025-08-04T09:08:43+00:00",
      "updated": "2025-08-04T09:08:43+00:00",
      "arxiv_id": "2508.02215v1",
      "url": "http://arxiv.org/pdf/2508.02215v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Balancing Information Accuracy and Response Timeliness in Networked LLMs",
      "abstract": "Recent advancements in Large Language Models (LLMs) have transformed many\nfields including scientific discovery, content generation, biomedical text\nmining, and educational technology. However, the substantial requirements for\ntraining data, computational resources, and energy consumption pose significant\nchallenges for their practical deployment. A promising alternative is to\nleverage smaller, specialized language models and aggregate their outputs to\nimprove overall response quality. In this work, we investigate a networked LLM\nsystem composed of multiple users, a central task processor, and clusters of\ntopic-specialized LLMs. Each user submits categorical binary (true/false)\nqueries, which are routed by the task processor to a selected cluster of $m$\nLLMs. After gathering individual responses, the processor returns a final\naggregated answer to the user. We characterize both the information accuracy\nand response timeliness in this setting, and formulate a joint optimization\nproblem to balance these two competing objectives. Our extensive simulations\ndemonstrate that the aggregated responses consistently achieve higher accuracy\nthan those of individual LLMs. Notably, this improvement is more significant\nwhen the participating LLMs exhibit similar standalone performance.",
      "authors": [
        "Yigit Turkmen",
        "Baturalp Buyukates",
        "Melih Bastopcu"
      ],
      "published": "2025-08-04T09:00:01+00:00",
      "updated": "2025-08-04T09:00:01+00:00",
      "arxiv_id": "2508.02209v1",
      "url": "http://arxiv.org/pdf/2508.02209v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "cs.NI",
        "math.IT"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems",
      "abstract": "Evaluating the mathematical capability of Large Language Models (LLMs) is a\ncritical yet challenging frontier. Existing benchmarks fall short, particularly\nfor proof-centric problems, as manual creation is unscalable and costly,\nleaving the true mathematical abilities of LLMs largely unassessed. To overcome\nthese barriers, we propose Proof2Hybrid, the first fully automated framework\nthat synthesizes high-quality, proof-centric benchmarks from natural language\nmathematical corpora. The key novelty of our solution is Proof2X, a roadmap of\nconverting mathematical proofs into various kinds of questions that are easy to\nverify. Instructed by this roadmap, we propose a new type of hybrid-formatted\nquestions, named ``$m$-out-of-$n$ multiple judge questions'', specifically\ndesigned to enable robust, automatic evaluation while being resilient to\nguessing and superficial pattern matching inherent in traditional formats. As a\ndemonstration of our framework, we introduce AlgGeoTest, a benchmark for\nalgebraic geometry--a frontier domain of modern mathematics--comprising 456\nchallenging items. Our extensive evaluations on state-of-the-art LLMs using\nAlgGeoTest reveal profound deficits in their comprehension of algebraic\ngeometry, providing a more precise measure of their true mathematical\ncapabilities. Our framework and benchmark pave the way for a new wave of\nin-depth research into the mathematical intelligence of AI systems.",
      "authors": [
        "Yebo Peng",
        "Zixiang Liu",
        "Yaoming Li",
        "Zhizhuo Yang",
        "Xinye Xu",
        "Bowen Ye",
        "Weijun Yuan",
        "Zihan Wang",
        "Tong Yang"
      ],
      "published": "2025-08-04T08:59:36+00:00",
      "updated": "2025-08-05T14:01:00+00:00",
      "arxiv_id": "2508.02208v2",
      "url": "http://arxiv.org/pdf/2508.02208v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Learning Dynamics of Meta-Learning in Small Model Pretraining",
      "abstract": "Large language models are powerful but costly. We ask whether meta-learning\ncan make the pretraining of small language models not only better but also more\ninterpretable. We integrate first-order MAML with subset-masked LM pretraining,\nproducing four LLama-style decoder-only models (11M-570M params), and evaluate\nit on a fundamental NLP task with many settings and real-world applications.\nCompared with vanilla training, our model (i) reaches the same loss up to 1.6x\nsooner, (ii) improves F1 on multilingual Universal NER under equal compute, and\n(iii) makes the training dynamics easy to read: first the network's\nrepresentations fan out (\"diversify\") and later they collapse into a smaller,\nshared subspace (\"compress\"). This two-stage shift shows up as a rise-and-fall\nin both effective-rank curves and attention-head entropy. The same curves\npinpoint which layers specialise earliest and which later reconverge, giving a\ncompact, interpretable signature of meta-adaptation. Code, checkpoints and\nWandB logs are released.",
      "authors": [
        "David Demitri Africa",
        "Yuval Weiss",
        "Paula Buttery",
        "Richard Diehl Martinez"
      ],
      "published": "2025-08-04T08:34:30+00:00",
      "updated": "2025-08-04T08:34:30+00:00",
      "arxiv_id": "2508.02189v1",
      "url": "http://arxiv.org/pdf/2508.02189v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Whispering Agents: An event-driven covert communication protocol for the Internet of Agents",
      "abstract": "The emergence of the Internet of Agents (IoA) introduces critical challenges\nfor communication privacy in sensitive, high-stakes domains. While standard\nAgent-to-Agent (A2A) protocols secure message content, they are not designed to\nprotect the act of communication itself, leaving agents vulnerable to\nsurveillance and traffic analysis. We find that the rich, event-driven nature\nof agent dialogues provides a powerful, yet untapped, medium for covert\ncommunication. To harness this potential, we introduce and formalize the Covert\nEvent Channel, the first unified model for agent covert communication driven by\nthree interconnected dimensions, which consist of the Storage, Timing,and\nBehavioral channels. Based on this model, we design and engineer {\\Pi}CCAP, a\nnovel protocol that operationalizes this event-driven paradigm. Our\ncomprehensive evaluation demonstrates that {\\Pi}CCAP achieves high capacity and\nrobustness while remaining imperceptible to powerful LLM-based wardens,\nestablishing its practical viability. By systematically engineering this\nchannel, our work provides the foundational understanding essential for\ndeveloping the next generation of monitoring systems and defensive protocols\nfor a secure and trustworthy IoA.",
      "authors": [
        "Kaibo Huang",
        "Yukun Wei",
        "WanSheng Wu",
        "Tianhua Zhang",
        "Zhongliang Yang",
        "Linna Zhou"
      ],
      "published": "2025-08-04T08:31:56+00:00",
      "updated": "2025-08-04T08:31:56+00:00",
      "arxiv_id": "2508.02188v1",
      "url": "http://arxiv.org/pdf/2508.02188v1",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "CAAD: Context-Aware Adaptive Decoding for Truthful Text Generation",
      "abstract": "Ensuring truthfulness in large language models remains a critical challenge\nfor reliable text generation. While supervised fine-tuning and reinforcement\nlearning with human feedback have shown promise, they require substantial\namount of annotated data and computational resources, limiting scalability. In\ncontrast, decoding-time interventions offer lightweight alternatives without\nmodel retraining. However, existing decoding strategies often face issues like\nprompt sensitivity, limited generalization, or dependence on internal model\nstates. We propose a context-aware adaptive decoding method that leverages a\ncompact reference grounding space, built from as few as 10 annotated examples\nand comprising pairs of context embeddings and next token logits from truthful\nresponses, to enable retrieval-based logit shaping during inference. At each\ndecoding step, our method retrieves top-N semantically similar contexts and\naggregates their associated next token logits to modify the LLM's logits.\nAcross three open-ended question-answering benchmarks, our approach achieves a\n2.8 percent average improvement on TruthfulQA and further outperforms existing\nbaselines on both Biographies and WikiQA. Experimental results also demonstrate\ncross-task generalization, with TruthfulQA-derived grounding enhancing\nbiography generation. Our model-agnostic, scalable, and efficient method\nrequires only a single generation pass, highlighting the potential of\ncontext-aware decoding for factual reliability in LLMs.",
      "authors": [
        "Manh Nguyen",
        "Sunil Gupta",
        "Hung Le"
      ],
      "published": "2025-08-04T08:28:25+00:00",
      "updated": "2025-08-04T08:28:25+00:00",
      "arxiv_id": "2508.02184v1",
      "url": "http://arxiv.org/pdf/2508.02184v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers",
      "abstract": "As Audio Large Language Models (ALLMs) emerge as powerful tools for speech\nprocessing, their safety implications demand urgent attention. While\nconsiderable research has explored textual and vision safety, audio's distinct\ncharacteristics present significant challenges. This paper first investigates:\nIs ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In\nresponse to this issue, we introduce Hidden in the Noise (HIN), a novel\nbackdoor attack framework designed to exploit subtle, audio-specific features.\nHIN applies acoustic modifications to raw audio waveforms, such as alterations\nto temporal dynamics and strategic injection of spectrally tailored noise.\nThese changes introduce consistent patterns that an ALLM's acoustic feature\nencoder captures, embedding robust triggers within the audio stream. To\nevaluate ALLM robustness against audio-feature-based triggers, we develop the\nAudioSafe benchmark, assessing nine distinct risk types. Extensive experiments\non AudioSafe and three established safety datasets reveal critical\nvulnerabilities in existing ALLMs: (I) audio features like environment noise\nand speech rate variations achieve over 90% average attack success rate. (II)\nALLMs exhibit significant sensitivity differences across acoustic features,\nparticularly showing minimal response to volume as a trigger, and (III)\npoisoned sample inclusion causes only marginal loss curve fluctuations,\nhighlighting the attack's stealth.",
      "authors": [
        "Liang Lin",
        "Miao Yu",
        "Kaiwen Luo",
        "Yibo Zhang",
        "Lilan Peng",
        "Dexian Wang",
        "Xuehai Tang",
        "Yuanhe Zhang",
        "Xikang Yang",
        "Zhenhong Zhou",
        "Kun Wang",
        "Yang Liu"
      ],
      "published": "2025-08-04T08:15:16+00:00",
      "updated": "2025-08-05T04:45:30+00:00",
      "arxiv_id": "2508.02175v2",
      "url": "http://arxiv.org/pdf/2508.02175v2",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD"
    },
    {
      "title": "Free-MoRef: Instantly Multiplexing Context Perception Capabilities of Video-MLLMs within Single Inference",
      "abstract": "Video Multimodal Large Language Models~(Video-MLLM) have achieved remarkable\nadvancements in video understanding tasks. However, constrained by the context\nlength limitation in the underlying LLMs, existing Video-MLLMs typically\nexhibit suboptimal performance on long video scenarios. To understand extended\ninput frames, common solutions span token compression and streaming inference\ntechniques, which sacrifice feature granularity or inference efficiency.\nDifferently, to efficiently achieve comprehensive understanding of longer frame\ninputs, we draw ideas from MoE and propose a training-free approach\n\\textbf{Free-MoRef}, which instantly multiplexes the context perception\ncapabilities of Video-MLLMs within one inference pass. Specifically, Free-MoRef\nreconstructs the vision tokens into several short sequences as\nmulti-references. Subsequently, we introduce MoRef-attention, which gathers\nclues from the multi-reference chunks in parallel to summarize unified query\nactivations. After the shadow layers in LLMs, a reference fusion step is\nderived to compose a final mixed reasoning sequence with key tokens from\nparallel chunks, which compensates the cross-reference vision interactions that\nare neglected in MoRef-attention. By splitting and fusing the long vision token\nsequences, Free-MoRef achieves improved performance under much lower computing\ncosts in reasoning multiplexed context length, demonstrating strong efficiency\nand effectiveness. Experiments on VideoMME, MLVU, LongVideoBench show that\nFree-MoRef achieves full perception of 2$\\times$ to 8$\\times$ longer input\nframes without compression on a single A100 GPU while keeping instant\nresponses, thereby bringing significant performance gains, even surpassing\ndedicatedly trained long-video-MLLMs. Codes are available at\nhttps://github.com/wkfdb/Free-MoRef",
      "authors": [
        "Kuo Wang",
        "Quanlong Zheng",
        "Junlin Xie",
        "Yanhao Zhang",
        "Jinguo Luo",
        "Haonan Lu",
        "Liang Lin",
        "Fan Zhou",
        "Guanbin Li"
      ],
      "published": "2025-08-04T07:31:10+00:00",
      "updated": "2025-08-04T07:31:10+00:00",
      "arxiv_id": "2508.02134v1",
      "url": "http://arxiv.org/pdf/2508.02134v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "All Stories Are One Story: Emotional Arc Guided Procedural Game Level Generation",
      "abstract": "The emotional arc is a universal narrative structure underlying stories\nacross cultures and media -- an idea central to structuralist narratology,\noften encapsulated in the phrase \"all stories are one story.\" We present a\nframework for procedural game narrative generation that incorporates emotional\narcs as a structural backbone for both story progression and gameplay dynamics.\nLeveraging established narratological theories and large-scale empirical\nanalyses, we focus on two core emotional patterns -- Rise and Fall -- to guide\nthe generation of branching story graphs. Each story node is automatically\npopulated with characters, items, and gameplay-relevant attributes (e.g.,\nhealth, attack), with difficulty adjusted according to the emotional\ntrajectory. Implemented in a prototype action role-playing game (ARPG), our\nsystem demonstrates how emotional arcs can be operationalized using large\nlanguage models (LLMs) and adaptive entity generation. Evaluation through\nplayer ratings, interviews, and sentiment analysis shows that emotional arc\nintegration significantly enhances engagement, narrative coherence, and\nemotional impact. These results highlight the potential of emotionally\nstructured procedural generation for advancing interactive storytelling for\ngames.",
      "authors": [
        "Yunge Wen",
        "Chenliang Huang",
        "Hangyu Zhou",
        "Zhuo Zeng",
        "Chun Ming Louis Po",
        "Julian Togelius",
        "Timothy Merino",
        "Sam Earle"
      ],
      "published": "2025-08-04T07:27:55+00:00",
      "updated": "2025-08-04T07:27:55+00:00",
      "arxiv_id": "2508.02132v1",
      "url": "http://arxiv.org/pdf/2508.02132v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models",
      "abstract": "In the era of large language models (LLMs), N:M sparsity has emerged as a\nstructured compression technique critical for accelerating inference. While\nprior work has primarily focused on weight sparsity, it often suffers from\nsignificant accuracy degradation. Activation sparsity, though promising, is\ntypically training-dependent and faces challenges in generalization. To address\nthese limitations, we introduce Amber Pruner, a training-free N:M activation\nsparsity method designed specifically for the prefill stage, targeting the\nacceleration of linear projection layers in LLMs. Extensive experiments across\nmultiple models and sparsity ratios (2:4, 4:8, and 8:16) demonstrate that Amber\nPruner can effectively sparsify and accelerate more than 55% of linear\ncomputations without requiring model retraining. To further enhance generality\nand efficiency, we propose Outstanding-sparse, a unified framework that\nintegrates Amber Pruner with post-training W8A8 quantization. Our approach\npreserves strong performance across a range of downstream tasks, with notable\nadvantages in generative tasks. This work pioneers a new frontier in activation\nsparsity, providing foundational insights that are poised to guide the\nco-evolution of algorithms and architectures in the design of next-generation\nAI systems.",
      "authors": [
        "Tai An",
        "Ruwu Cai",
        "Yanzhe Zhang",
        "Yang Liu",
        "Hao Chen",
        "Pengcheng Xie",
        "Sheng Chang",
        "Yiwu Yao",
        "Gongyi Wang"
      ],
      "published": "2025-08-04T07:22:36+00:00",
      "updated": "2025-08-04T07:22:36+00:00",
      "arxiv_id": "2508.02128v1",
      "url": "http://arxiv.org/pdf/2508.02128v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Trainable Dynamic Mask Sparse Attention",
      "abstract": "In large language models, the demand for modeling long contexts is constantly\nincreasing, but the quadratic complexity of the standard self-attention\nmechanism often becomes a bottleneck. Although existing sparse attention\nmechanisms have improved efficiency, they may still encounter issues such as\nstatic patterns or information loss. We introduce a trainable dynamic mask\nsparse attention mechanism, Dynamic Mask Attention, which effectively utilizes\ncontent-aware and position-aware sparsity. DMA achieves this through two key\ninnovations: First, it dynamically generates content-aware sparse masks from\nvalue representations, enabling the model to identify and focus on critical\ninformation adaptively. Second, it implements position-aware sparse attention\ncomputation that effectively skips unnecessary calculation regions. This\ndual-sparsity design allows the model to significantly reduce the computational\ncomplexity of important information while retaining complete information,\nachieving an excellent balance between information fidelity and computational\nefficiency. We have verified the performance of DMA through comprehensive\nexperiments. Comparative studies show that DMA outperforms multi-head\nattention, sliding window attention, multi-head latent attention, and native\nsparse attention in terms of perplexity under Chinchilla Scaling Law settings.\nMoreover, in challenging multi-query associative recall tasks, DMA also\ndemonstrates superior performance and efficiency compared to these methods.\nCrucially, in the evaluation of a 1.7B parameter model, DMA significantly\noutperforms multi-head attention in both standard benchmark performance and the\nchallenging needle-in-a-haystack task. These experimental results highlight its\ncapability to balance model efficiency and long-context modeling ability\neffectively.",
      "authors": [
        "Jingze Shi",
        "Yifan Wu",
        "Bingheng Wu",
        "Yiran Peng",
        "Liangdong Wang",
        "Guang Liu",
        "Yuyu Luo"
      ],
      "published": "2025-08-04T07:05:15+00:00",
      "updated": "2025-08-04T07:05:15+00:00",
      "arxiv_id": "2508.02124v1",
      "url": "http://arxiv.org/pdf/2508.02124v1",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "A Survey on AgentOps: Categorization, Challenges, and Future Directions",
      "abstract": "As the reasoning capabilities of Large Language Models (LLMs) continue to\nadvance, LLM-based agent systems offer advantages in flexibility and\ninterpretability over traditional systems, garnering increasing attention.\nHowever, despite the widespread research interest and industrial application of\nagent systems, these systems, like their traditional counterparts, frequently\nencounter anomalies. These anomalies lead to instability and insecurity,\nhindering their further development. Therefore, a comprehensive and systematic\napproach to the operation and maintenance of agent systems is urgently needed.\nUnfortunately, current research on the operations of agent systems is sparse.\nTo address this gap, we have undertaken a survey on agent system operations\nwith the aim of establishing a clear framework for the field, defining the\nchallenges, and facilitating further development. Specifically, this paper\nbegins by systematically defining anomalies within agent systems, categorizing\nthem into intra-agent anomalies and inter-agent anomalies. Next, we introduce a\nnovel and comprehensive operational framework for agent systems, dubbed Agent\nSystem Operations (AgentOps). We provide detailed definitions and explanations\nof its four key stages: monitoring, anomaly detection, root cause analysis, and\nresolution.",
      "authors": [
        "Zexin Wang",
        "Jingjing Li",
        "Quan Zhou",
        "Haotian Si",
        "Yuanhao Liu",
        "Jianhui Li",
        "Gaogang Xie",
        "Fei Sun",
        "Dan Pei",
        "Changhua Pei"
      ],
      "published": "2025-08-04T06:59:36+00:00",
      "updated": "2025-08-04T06:59:36+00:00",
      "arxiv_id": "2508.02121v1",
      "url": "http://arxiv.org/pdf/2508.02121v1",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models",
      "abstract": "Recently, Large Reasoning Models (LRMs) have gradually become a research\nhotspot due to their outstanding performance in handling complex tasks. Among\nthem, DeepSeek R1 has garnered significant attention for its exceptional\nperformance and open-source nature, driving advancements in the research of\nR1-style LRMs. Unlike traditional Large Language Models (LLMs), these models\nenhance logical deduction and decision-making capabilities during reasoning by\nincorporating mechanisms such as long chain-of-thought and self-reflection\nthrough reinforcement learning. However, with the widespread application of\nthese models, the problem of overthinking has gradually emerged. Specifically,\nwhen generating answers, these models often construct excessively long\nreasoning chains with redundant or repetitive steps, which leads to reduced\nreasoning efficiency and may affect the accuracy of the final answer. To this\nend, various efficient reasoning methods have been proposed, aiming to reduce\nthe length of reasoning paths without compromising model performance and\nreasoning capability. By reviewing the current research advancements in the\nfield of efficient reasoning methods systematically, we categorize existing\nworks into two main directions based on the lens of single-model optimization\nversus model collaboration: (1) Efficient Reasoning with Single Model, which\nfocuses on improving the reasoning efficiency of individual models; and (2)\nEfficient Reasoning with Model Collaboration, which explores optimizing\nreasoning paths through collaboration among multiple models. Besides, we\nmaintain a public GitHub repository that tracks the latest progress in\nefficient reasoning methods.",
      "authors": [
        "Linan Yue",
        "Yichao Du",
        "Yizhi Wang",
        "Weibo Gao",
        "Fangzhou Yao",
        "Li Wang",
        "Ye Liu",
        "Ziyu Xu",
        "Qi Liu",
        "Shimin Di",
        "Min-Ling Zhang"
      ],
      "published": "2025-08-04T06:54:31+00:00",
      "updated": "2025-08-04T06:54:31+00:00",
      "arxiv_id": "2508.02120v1",
      "url": "http://arxiv.org/pdf/2508.02120v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools",
      "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities\nin complex reasoning and decision-making by leveraging external tools. However,\nthis tool-centric paradigm introduces a previously underexplored attack\nsurface: adversaries can manipulate tool metadata -- such as names,\ndescriptions, and parameter schemas -- to influence agent behavior. We identify\nthis as a new and stealthy threat surface that allows malicious tools to be\npreferentially selected by LLM agents, without requiring prompt injection or\naccess to model internals. To demonstrate and exploit this vulnerability, we\npropose the Attractive Metadata Attack (AMA), a black-box in-context learning\nframework that generates highly attractive but syntactically and semantically\nvalid tool metadata through iterative optimization. Our attack integrates\nseamlessly into standard tool ecosystems and requires no modification to the\nagent's execution framework. Extensive experiments across ten realistic,\nsimulated tool-use scenarios and a range of popular LLM agents demonstrate\nconsistently high attack success rates (81\\%-95\\%) and significant privacy\nleakage, with negligible impact on primary task execution. Moreover, the attack\nremains effective even under prompt-level defenses and structured\ntool-selection protocols such as the Model Context Protocol, revealing systemic\nvulnerabilities in current agent architectures. These findings reveal that\nmetadata manipulation constitutes a potent and stealthy attack surface,\nhighlighting the need for execution-level security mechanisms that go beyond\nprompt-level defenses.",
      "authors": [
        "Kanghua Mo",
        "Li Hu",
        "Yucheng Long",
        "Zhihao Li"
      ],
      "published": "2025-08-04T06:38:59+00:00",
      "updated": "2025-08-04T06:38:59+00:00",
      "arxiv_id": "2508.02110v1",
      "url": "http://arxiv.org/pdf/2508.02110v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "The Silicon Reasonable Person: Can AI Predict How Ordinary People Judge Reasonableness?",
      "abstract": "In everyday life, people make countless reasonableness judgments that\ndetermine appropriate behavior in various contexts. Predicting these judgments\nchallenges the legal system, as judges' intuitions may not align with broader\nsocietal views. This Article investigates whether large language models (LLMs)\ncan learn to identify patterns driving human reasonableness judgments.\n  Using randomized controlled trials comparing humans and models across\nmultiple legal contexts with over 10,000 simulated judgments, we demonstrate\nthat certain models capture not just surface-level responses but potentially\ntheir underlying decisional architecture. Strikingly, these systems prioritize\nsocial cues over economic efficiency in negligence determinations, mirroring\nhuman behavior despite contradicting textbook treatments.\n  These findings suggest practical applications: judges could calibrate\nintuitions against broader patterns, lawmakers could test policy\ninterpretations, and resource-constrained litigants could preview argument\nreception. As AI agents increasingly make autonomous real-world decisions,\nunderstanding whether they've internalized recognizable ethical frameworks\nbecomes essential for anticipating their behavior.",
      "authors": [
        "Yonathan A. Arbel"
      ],
      "published": "2025-08-04T06:19:45+00:00",
      "updated": "2025-08-04T06:19:45+00:00",
      "arxiv_id": "2508.02766v1",
      "url": "http://arxiv.org/pdf/2508.02766v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Generating Synthetic Invoices via Layout-Preserving Content Replacement",
      "abstract": "The performance of machine learning models for automated invoice processing\nis critically dependent on large-scale, diverse datasets. However, the\nacquisition of such datasets is often constrained by privacy regulations and\nthe high cost of manual annotation. To address this, we present a novel\npipeline for generating high-fidelity, synthetic invoice documents and their\ncorresponding structured data. Our method first utilizes Optical Character\nRecognition (OCR) to extract the text content and precise spatial layout from a\nsource invoice. Select data fields are then replaced with contextually\nrealistic, synthetic content generated by a large language model (LLM).\nFinally, we employ an inpainting technique to erase the original text from the\nimage and render the new, synthetic text in its place, preserving the exact\nlayout and font characteristics. This process yields a pair of outputs: a\nvisually realistic new invoice image and a perfectly aligned structured data\nfile (JSON) reflecting the synthetic content. Our approach provides a scalable\nand automated solution to amplify small, private datasets, enabling the\ncreation of large, varied corpora for training more robust and accurate\ndocument intelligence models.",
      "authors": [
        "Bevin V",
        "Ananthakrishnan P V",
        "Ragesh KR",
        "Sanjay M",
        "Vineeth S",
        "Bibin Wilson"
      ],
      "published": "2025-08-04T06:19:34+00:00",
      "updated": "2025-08-04T06:19:34+00:00",
      "arxiv_id": "2508.03754v1",
      "url": "http://arxiv.org/pdf/2508.03754v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches",
      "abstract": "Conversational Recommender Systems (CRSs) are receiving growing research\nattention across domains, yet their user experience (UX) evaluation remains\nlimited. Existing reviews largely overlook empirical UX studies, particularly\nin adaptive and large language model (LLM)-based CRSs. To address this gap, we\nconducted a systematic review following PRISMA guidelines, synthesising 23\nempirical studies published between 2017 and 2025. We analysed how UX has been\nconceptualised, measured, and shaped by domain, adaptivity, and LLM. Our\nfindings reveal persistent limitations: post hoc surveys dominate, turn-level\naffective UX constructs are rarely assessed, and adaptive behaviours are seldom\nlinked to UX outcomes. LLM-based CRSs introduce further challenges, including\nepistemic opacity and verbosity, yet evaluations infrequently address these\nissues. We contribute a structured synthesis of UX metrics, a comparative\nanalysis of adaptive and nonadaptive systems, and a forward-looking agenda for\nLLM-aware UX evaluation. These findings support the development of more\ntransparent, engaging, and user-centred CRS evaluation practices.",
      "authors": [
        "Raj Mahmud",
        "Yufeng Wu",
        "Abdullah Bin Sawad",
        "Shlomo Berkovsky",
        "Mukesh Prasad",
        "A. Baki Kocaballi"
      ],
      "published": "2025-08-04T06:07:33+00:00",
      "updated": "2025-08-06T07:55:11+00:00",
      "arxiv_id": "2508.02096v2",
      "url": "http://arxiv.org/pdf/2508.02096v2",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.HC",
        "H.3.3; H.5.2; I.2.7"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "FPEdit: Robust LLM Fingerprinting through Localized Knowledge Editing",
      "abstract": "Large language models represent significant investments in computation, data,\nand engineering expertise, making them extraordinarily valuable intellectual\nassets. Nevertheless, these AI assets remain vulnerable to unauthorized\nredistribution and commercial exploitation through fine-tuning or black-box\ndeployment. Current fingerprinting approaches face a fundamental trade-off:\nintrinsic methods require full parameter access, while backdoor-based\ntechniques employ statistically anomalous triggers easily detected and filtered\nby adversaries. To address these limitations, we introduce FPEdit, a novel\nknowledge-editing framework that injects semantically coherent natural language\nfingerprints by modifying a sparse subset of model weights. This ensures\nstealthy and precise ownership encoding without degrading the core\nfunctionality. Extensive experiments show that FPEdit achieves $95$-$100\\%$\nfingerprint retention under both full-parameter fine-tuning and\nparameter-efficient adaptation, while preserving performance on 24 downstream\nbenchmarks. Moreover, FPEdit remains robust under quantization, pruning, and\nstochastic decoding, and can embed 10 fingerprint pairs into LLaMA2-7B in under\n10 minutes using less than 32 GB of GPU memory, a $70\\%$ reduction in resource\nrequirements compared to existing techniques. These advances establish FPEdit\nas the first fingerprinting approach to simultaneously achieve robustness\nagainst adaptation, resistance to detection, and preservation of model utility,\nproviding a minimally invasive solution for reliable provenance verification of\nlarge language models in adversarial deployment scenarios.",
      "authors": [
        "Shida Wang",
        "Chaohu Liu",
        "Yubo Wang",
        "Linli Xu"
      ],
      "published": "2025-08-04T06:00:22+00:00",
      "updated": "2025-08-04T06:00:22+00:00",
      "arxiv_id": "2508.02092v1",
      "url": "http://arxiv.org/pdf/2508.02092v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search",
      "abstract": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Chris Shum",
        "Jiwei Li"
      ],
      "published": "2025-08-04T05:57:46+00:00",
      "updated": "2025-08-04T05:57:46+00:00",
      "arxiv_id": "2508.02091v1",
      "url": "http://arxiv.org/pdf/2508.02091v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DB"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models",
      "abstract": "Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing\nwith user-stated opinions even when those contradict factual knowledge. While\nprior work has documented this tendency, the internal mechanisms that enable\nsuch behavior remain poorly understood. In this paper, we provide a mechanistic\naccount of how sycophancy arises within LLMs. We first systematically study how\nuser opinions induce sycophancy across different model families. We find that\nsimple opinion statements reliably induce sycophancy, whereas user expertise\nframing has a negligible impact. Through logit-lens analysis and causal\nactivation patching, we identify a two-stage emergence of sycophancy: (1) a\nlate-layer output preference shift and (2) deeper representational divergence.\nWe also verify that user authority fails to influence behavior because models\ndo not encode it internally. In addition, we examine how grammatical\nperspective affects sycophantic behavior, finding that first-person prompts\n(``I believe...'') consistently induce higher sycophancy rates than\nthird-person framings (``They believe...'') by creating stronger\nrepresentational perturbations in deeper layers. These findings highlight that\nsycophancy is not a surface-level artifact but emerges from a structural\noverride of learned knowledge in deeper layers, with implications for alignment\nand truthful AI systems.",
      "authors": [
        "Keyu Wang",
        "Jin Li",
        "Shu Yang",
        "Zhuoran Zhang",
        "Di Wang"
      ],
      "published": "2025-08-04T05:55:06+00:00",
      "updated": "2025-08-05T04:26:47+00:00",
      "arxiv_id": "2508.02087v2",
      "url": "http://arxiv.org/pdf/2508.02087v2",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents",
      "abstract": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent.",
      "authors": [
        "Jiaye Lin",
        "Yifu Guo",
        "Yuzhen Han",
        "Sen Hu",
        "Ziyi Ni",
        "Licheng Wang",
        "Mingguang Chen",
        "Daxin Jiang",
        "Binxing Jiao",
        "Chen Hu",
        "Huacan Wang"
      ],
      "published": "2025-08-04T05:51:55+00:00",
      "updated": "2025-08-07T16:46:44+00:00",
      "arxiv_id": "2508.02085v3",
      "url": "http://arxiv.org/pdf/2508.02085v3",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "S-RRG-Bench: Structured Radiology Report Generation with Fine-Grained Evaluation Framework",
      "abstract": "Radiology report generation (RRG) for diagnostic images, such as chest\nX-rays, plays a pivotal role in both clinical practice and AI. Traditional\nfree-text reports suffer from redundancy and inconsistent language,\ncomplicating the extraction of critical clinical details. Structured radiology\nreport generation (S-RRG) offers a promising solution by organizing information\ninto standardized, concise formats. However, existing approaches often rely on\nclassification or visual question answering (VQA) pipelines that require\npredefined label sets and produce only fragmented outputs. Template-based\napproaches, which generate reports by replacing keywords within fixed sentence\npatterns, further compromise expressiveness and often omit clinically important\ndetails. In this work, we present a novel approach to S-RRG that includes\ndataset construction, model training, and the introduction of a new evaluation\nframework. We first create a robust chest X-ray dataset (MIMIC-STRUC) that\nincludes disease names, severity levels, probabilities, and anatomical\nlocations, ensuring that the dataset is both clinically relevant and\nwell-structured. We train an LLM-based model to generate standardized,\nhigh-quality reports. To assess the generated reports, we propose a specialized\nevaluation metric (S-Score) that not only measures disease prediction accuracy\nbut also evaluates the precision of disease-specific details, thus offering a\nclinically meaningful metric for report quality that focuses on elements\ncritical to clinical decision-making and demonstrates a stronger alignment with\nhuman assessments. Our approach highlights the effectiveness of structured\nreports and the importance of a tailored evaluation metric for S-RRG, providing\na more clinically relevant measure of report quality.",
      "authors": [
        "Yingshu Li",
        "Yunyi Liu",
        "Zhanyu Wang",
        "Xinyu Liang",
        "Lingqiao Liu",
        "Lei Wang",
        "Luping Zhou"
      ],
      "published": "2025-08-04T05:49:41+00:00",
      "updated": "2025-08-04T05:49:41+00:00",
      "arxiv_id": "2508.02082v1",
      "url": "http://arxiv.org/pdf/2508.02082v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided Decomposition and Riemannian-Geodesic Collision Regularization",
      "abstract": "Low-rank adaptation (LoRA) has become a standard tool for efficiently\nfine-tuning large language models (LLMs). Yet, even minor LoRA updates can\ninduce alignment drift, weakening safety and behavioral constraints through\nentangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL),\na principled framework for preserving alignment during finetuning. AGL\nintroduces several key components: a primary task loss for supervision, Fisher\nInformation Matrix-based regularization to restrict updates in\nalignment-sensitive subspaces, and task-specific regularization to stabilize\nthe integration of new knowledge. We further introduce collision-aware\nregularization, blending Riemannian overlap -- which penalizes coordinate-wise\ninterference -- and geodesic separation -- which encourages disjoint update\ngeometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and\nunsafe prompts designed to quantify alignment drift and safety degradation.\nEmpirical evaluations show that AGL mitigates alignment drift by up to 50% on\nsafety-critical benchmarks without degrading downstream task performance.\nComprehensive ablation confirms that each component contributes distinctly to\npreserving latent safety behaviors. Finally, we derive and validate a scaling\nlaw for catastrophic forgetting, revealing that AGL flattens post-finetuning\nloss escalation while preserving adaptation dynamics. AGL is a structurally\ngrounded refinement of LoRA, ensuring alignment preservation with minimal\ntrade-offs. To encourage further exploration and development, we open-source\nour implementation.",
      "authors": [
        "Amitava Das",
        "Abhilekh Borah",
        "Vinija Jain",
        "Aman Chadha"
      ],
      "published": "2025-08-04T05:45:24+00:00",
      "updated": "2025-08-04T05:45:24+00:00",
      "arxiv_id": "2508.02079v1",
      "url": "http://arxiv.org/pdf/2508.02079v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games",
      "abstract": "Coordinating multiple large language models (LLMs) to solve complex tasks\ncollaboratively poses a fundamental trade-off between the computation costs and\ncollective performance compared with individual model. We introduce a novel,\ngame-theoretically grounded reinforcement learning (RL) framework, the\nMulti-Agent Cooperation Sequential Public Goods Game (MAC-SPGG), to\nsystematically incentivize cooperation in multi-LLM ensembles. In MAC-SPGG, LLM\nagents move in sequence, observing predecessors' outputs and updating beliefs\nto condition their own contributions. By redesigning the public-goods reward,\neffortful contributions become the unique Subgame Perfect Nash Equilibrium\n(SPNE), which eliminates free-riding under traditional SPGG or PGG. Its\nsequential protocol replaces costly round-based information exchanges with a\nstreamlined decision flow, cutting communication overhead while retaining\nstrategic depth. We prove the existence and uniqueness of the SPNE under\nrealistic parameters, and empirically show that MAC-SPGG-trained ensembles\noutperform single-agent baselines, chain-of-thought prompting, and other\ncooperative methods, even achieving comparable performance to large-scale\nmodels across reasoning, math, code generation, and NLP tasks. Our results\nhighlight the power of structured, incentive-aligned MAC-SPGG cooperation for\nscalable and robust multi-agent language generation.",
      "authors": [
        "Yunhao Liang",
        "Yuan Qu",
        "Jingyuan Yang",
        "Shaochong Lin",
        "Zuo-Jun Max Shen"
      ],
      "published": "2025-08-04T05:36:07+00:00",
      "updated": "2025-08-04T05:36:07+00:00",
      "arxiv_id": "2508.02076v1",
      "url": "http://arxiv.org/pdf/2508.02076v1",
      "categories": [
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "The SMeL Test: A simple benchmark for media literacy in language models",
      "abstract": "The internet is rife with unattributed, deliberately misleading, or otherwise\nuntrustworthy content. Though large language models (LLMs) are often tasked\nwith autonomous web browsing, the extent to which they have learned the simple\nheuristics human researchers use to navigate this noisy environment is not\ncurrently known. In this paper, we introduce the Synthetic Media Literacy Test\n(SMeL Test), a minimal benchmark that tests the ability of language models to\nactively filter out untrustworthy information in context. We benchmark a\nvariety of commonly used instruction-tuned LLMs, including reasoning models,\nand find that no model consistently succeeds; while reasoning in particular is\nassociated with higher scores, even the best API model we test hallucinates up\nto 70% of the time. Remarkably, larger and more capable models do not\nnecessarily outperform their smaller counterparts. We hope our work sheds more\nlight on this important form of hallucination and guides the development of new\nmethods to combat it.",
      "authors": [
        "Gustaf Ahdritz",
        "Anat Kleiman"
      ],
      "published": "2025-08-04T05:29:17+00:00",
      "updated": "2025-08-07T03:54:11+00:00",
      "arxiv_id": "2508.02074v2",
      "url": "http://arxiv.org/pdf/2508.02074v2",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Risk identification based on similar case retrieval enhancement,",
      "abstract": "The goal of construction site risk and hazard identification is to enhance\nsafety management through automation. Existing research based on large language\nmodels falls into two categories: image-text matching for collaborative\nreasoning, which struggles with complex hazard features, and instruction\nfine-tuning or dialogue guidance using professional datasets, which suffers\nfrom high training costs and poor generalization.To address this, we propose a\nhazard identification method using similar case retrieval enhancement. By\nintegrating external knowledge and retrieved case contexts via prompt\nfine-tuning, we mitigate misjudgments caused by limited domain knowledge and\nweak feature associations. Our method includes three modules: retrieval\nlibrary, image similarity retrieval, and large model retrieval enhancement,\nenabling efficient recognition without training. Experiments on real\nconstruction data show significant improvements. For instance, GLM-4V's\nrecognition accuracy increased to 50\\%, a 35.49\\% boost. The method enhances\naccuracy, context understanding, and stability, offering new theoretical and\ntechnical support for hazard detection.",
      "authors": [
        "Jiawei Li",
        "Chengye Yang",
        "Yaochen Zhang",
        "Weilin Sun",
        "Lei Meng",
        "Xiangxu Meng"
      ],
      "published": "2025-08-04T05:28:58+00:00",
      "updated": "2025-08-04T05:28:58+00:00",
      "arxiv_id": "2508.02073v1",
      "url": "http://arxiv.org/pdf/2508.02073v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "\"Set It Up\": Functional Object Arrangement with Compositional Generative Models (Journal Version)",
      "abstract": "Functional object arrangement (FORM) is the task of arranging objects to\nfulfill a function, e.g., \"set up a dining table for two\". One key challenge\nhere is that the instructions for FORM are often under-specified and do not\nexplicitly specify the desired object goal poses. This paper presents SetItUp,\na neuro-symbolic framework that learns to specify the goal poses of objects\nfrom a few training examples and a structured natural-language task\nspecification. SetItUp uses a grounding graph, which is composed of abstract\nspatial relations among objects (e.g., left-of), as its intermediate\nrepresentation. This decomposes the FORM problem into two stages: (i)\npredicting this graph among objects and (ii) predicting object poses given the\ngrounding graph. For (i), SetItUp leverages large language models (LLMs) to\ninduce Python programs from a task specification and a few training examples.\nThis program can be executed to generate grounding graphs in novel scenarios.\nFor (ii), SetItUp pre-trains a collection of diffusion models to capture\nprimitive spatial relations and online composes these models to predict object\nposes based on the grounding graph. We evaluated SetItUp on a dataset spanning\nthree distinct task families: arranging tableware on a dining table, organizing\nitems on a bookshelf, and laying out furniture in a bedroom. Experiments show\nthat SetItUp outperforms existing models in generating functional, physically\nfeasible, and aesthetically pleasing object arrangements. This article extends\nour conference paper published at Robotics: Science and Systems (RSS) 2024.",
      "authors": [
        "Yiqing Xu",
        "Jiayuan Mao",
        "Linfeng Li",
        "Yilun Du",
        "Tomas LozÃ¡no-PÃ©rez",
        "Leslie Pack Kaelbling",
        "David Hsu"
      ],
      "published": "2025-08-04T05:16:09+00:00",
      "updated": "2025-08-07T09:18:55+00:00",
      "arxiv_id": "2508.02068v2",
      "url": "http://arxiv.org/pdf/2508.02068v2",
      "categories": [
        "cs.RO"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs",
      "abstract": "Large Language Models(LLMs) have demonstrated remarkable performance across\nvarious domains, yet their capabilities in molecular reasoning remain\ninsufficiently explored. Current approaches tend to rely heavily on\ngeneral-purpose prompting, which lacks domain-specific molecular semantics,\nwhile those that use fine-tuning strategies often face challenges with\ninterpretability and reasoning depth. To address these issues, we introduce\nMolReasoner, a two-stage framework designed to transition LLMs from\nmemorization towards chemical reasoning. First, we propose Mol-SFT, which\ninitializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT)\nsamples generated by GPT-4o and verified for chemical accuracy. Subsequently,\nMol-RL applies reinforcement learning with specialized reward functions\ndesigned explicitly to align chemical structures with linguistic descriptions,\nthereby enhancing molecular reasoning capabilities. Our approach notably\nenhances interpretability, improving the model 's molecular understanding and\nenabling better generalization. Extensive experiments demonstrate that\nMolReasoner outperforms existing methods, and marking a significant shift from\nmemorization-based outputs to robust chemical reasoning.",
      "authors": [
        "Guojiang Zhao",
        "Sihang Li",
        "Zixiang Lu",
        "Zheng Cheng",
        "Haitao Lin",
        "Lirong Wu",
        "Hanchen Xia",
        "Hengxing Cai",
        "Wentao Guo",
        "Hongshuai Wang",
        "Mingjun Xu",
        "Siyu Zhu",
        "Guolin Ke",
        "Linfeng Zhang",
        "Zhifeng Gao"
      ],
      "published": "2025-08-04T05:10:11+00:00",
      "updated": "2025-08-04T05:10:11+00:00",
      "arxiv_id": "2508.02066v1",
      "url": "http://arxiv.org/pdf/2508.02066v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to Training-Time Belief Sources in LLMs",
      "abstract": "Large Language Models (LLMs) fine-tuned to align with human values often\nexhibit alignment drift, producing unsafe or policy-violating completions when\nexposed to adversarial prompts, decoding perturbations, or paraphrased\njailbreaks. While prior work has behaviorally characterized alignment failure,\nlittle is known about the training-time belief sources underlying these\nfailures. We introduce TraceAlign, a unified framework for tracing unsafe\ncompletions back to their root causes in the model's training corpus. Central\nto our approach is the Belief Conflict Index (BCI), which quantifies semantic\ninconsistency between generated spans and aligned policies, based on retrieved\ntraining documents using suffix-array matching. We propose three complementary\ninterventions: (i) TraceShield, an inference-time safety filter that refuses\ncompletions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a\ncontrastive fine-tuning objective penalizing high-BCI continuations during DPO,\nand (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam\nexpansions predicted to yield high-BCI spans. Together, these defenses reduce\nalignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB)\nwhile preserving utility on standard tasks, with delta less than 0.2 and\nimproved refusal quality. We further derive a theoretical upper bound on drift\nlikelihood via suffix-array span statistics, linking memorization frequency and\nlength to adversarial reactivation risk. TraceAlign thus provides the first\nscalable, traceable, and grounded toolkit for understanding and mitigating\nalignment failures at source. To encourage further exploration and development,\nwe open-source our implementation at:\nhttps://anonymous.4open.science/r/tracealign-2DA7",
      "authors": [
        "Amitava Das",
        "Vinija Jain",
        "Aman Chadha"
      ],
      "published": "2025-08-04T05:03:35+00:00",
      "updated": "2025-08-04T05:03:35+00:00",
      "arxiv_id": "2508.02063v1",
      "url": "http://arxiv.org/pdf/2508.02063v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "ProCut: LLM Prompt Compression via Attribution Estimation",
      "abstract": "In large-scale industrial LLM systems, prompt templates often expand to\nthousands of tokens as teams iteratively incorporate sections such as task\ninstructions, few-shot examples, and heuristic rules to enhance robustness and\ncoverage. This expansion leads to bloated prompts that are difficult to\nmaintain and incur significant inference latency and serving costs. To address\nthis, we introduce Prompt Compression via Attribution Estimation (ProCut), a\nflexible, LLM-agnostic, training-free framework that compresses prompts through\nattribution analysis. ProCut segments prompt templates into semantically\nmeaningful units, quantifies their impact on task performance, and prunes\nlow-utility components. Through extensive experiments on five public benchmark\ndatasets and real-world industrial prompts, we show that ProCut achieves\nsubstantial prompt size reductions (78% fewer tokens in production) while\nmaintaining or even slightly improving task performance (up to 62% better than\nalternative methods). We further introduce an LLM-driven attribution estimator\nthat reduces compression latency by over 50%, and demonstrate that ProCut\nintegrates seamlessly with existing prompt-optimization frameworks to produce\nconcise, high-performing prompts.",
      "authors": [
        "Zhentao Xu",
        "Fengyi Li",
        "Albert Chen",
        "Xiaofeng Wang"
      ],
      "published": "2025-08-04T04:44:43+00:00",
      "updated": "2025-08-04T04:44:43+00:00",
      "arxiv_id": "2508.02053v1",
      "url": "http://arxiv.org/pdf/2508.02053v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models",
      "abstract": "Facts evolve over time, making it essential for Large Language Models (LLMs)\nto handle time-sensitive factual knowledge accurately and reliably. While\nfactual Time-Sensitive Question-Answering (TSQA) tasks have been widely\nstudied, existing benchmarks often rely on manual curation or a small, fixed\nset of predefined templates, which restricts scalable and comprehensive TSQA\nevaluation. To address these challenges, we propose TDBench, a new benchmark\nthat systematically constructs TSQA pairs by harnessing temporal databases and\ndatabase techniques such as temporal SQL and functional dependencies. We also\nintroduce a fine-grained evaluation metric called time accuracy, which assesses\nthe validity of time references in model explanations alongside traditional\nanswer accuracy to enable a more reliable TSQA evaluation. Extensive\nexperiments on contemporary LLMs show how \\ours{} enables scalable and\ncomprehensive TSQA evaluation while reducing the reliance on human labor,\ncomplementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by\nenabling LLM evaluation on application-specific data and seamless multi-hop\nquestion generation. Code and data are publicly available at:\nhttps://github.com/ssoy0701/tdbench.git.",
      "authors": [
        "Soyeon Kim",
        "Jindong Wang",
        "Xing Xie",
        "Steven Euijong Whang"
      ],
      "published": "2025-08-04T04:27:06+00:00",
      "updated": "2025-08-04T04:27:06+00:00",
      "arxiv_id": "2508.02045v1",
      "url": "http://arxiv.org/pdf/2508.02045v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time",
      "abstract": "Large Language Models (LLMs) perform well on reasoning benchmarks but often\nfail when inputs alter slightly, raising concerns about the extent to which\ntheir success relies on memorization. This issue is especially acute in\nChain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger\nintermediate errors that cascade into incorrect final answers. We introduce\nSTIM, a novel framework for Source-aware Token-level Identification of\nMemorization, which attributes each token in a reasoning chain to one of\nmultiple memorization sources - local, mid-range, or long-range - based on\ntheir statistical co-occurrence with the token in the pretraining corpus. Our\ntoken-level analysis across tasks and distributional settings reveals that\nmodels rely more on memorization in complex or long-tail cases, and that local\nmemorization is often the dominant driver of errors, leading to up to 67% of\nwrong tokens. We also show that memorization scores from STIM can be effective\nin predicting the wrong tokens in the wrong reasoning step. STIM offers a\npowerful tool for diagnosing and improving model reasoning and can generalize\nto other structured step-wise generation tasks.",
      "authors": [
        "Huihan Li",
        "You Chen",
        "Siyuan Wang",
        "Yixin He",
        "Ninareh Mehrabi",
        "Rahul Gupta",
        "Xiang Ren"
      ],
      "published": "2025-08-04T04:06:34+00:00",
      "updated": "2025-08-04T04:06:34+00:00",
      "arxiv_id": "2508.02037v1",
      "url": "http://arxiv.org/pdf/2508.02037v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "PhishParrot: LLM-Driven Adaptive Crawling to Unveil Cloaked Phishing Sites",
      "abstract": "Phishing attacks continue to evolve, with cloaking techniques posing a\nsignificant challenge to detection efforts. Cloaking allows attackers to\ndisplay phishing sites only to specific users while presenting legitimate pages\nto security crawlers, rendering traditional detection systems ineffective. This\nresearch proposes PhishParrot, a novel crawling environment optimization system\ndesigned to counter cloaking techniques. PhishParrot leverages the contextual\nanalysis capabilities of Large Language Models (LLMs) to identify potential\npatterns in crawling information, enabling the construction of optimal user\nprofiles capable of bypassing cloaking mechanisms. The system accumulates\ninformation on phishing sites collected from diverse environments. It then\nadapts browser settings and network configurations to match the attacker's\ntarget user conditions based on information extracted from similar cases. A\n21-day evaluation showed that PhishParrot improved detection accuracy by up to\n33.8% over standard analysis systems, yielding 91 distinct crawling\nenvironments for diverse conditions targeted by attackers. The findings confirm\nthat the combination of similar-case extraction and LLM-based context analysis\nis an effective approach for detecting cloaked phishing attacks.",
      "authors": [
        "Hiroki Nakano",
        "Takashi Koide",
        "Daiki Chiba"
      ],
      "published": "2025-08-04T04:04:07+00:00",
      "updated": "2025-08-04T04:04:07+00:00",
      "arxiv_id": "2508.02035v1",
      "url": "http://arxiv.org/pdf/2508.02035v1",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Confidence-Diversity Calibration of AI Judgement Enables Reliable Qualitative Coding",
      "abstract": "LLMs enable qualitative coding at large scale, but assessing the reliability\nof their output remains challenging in domains where human experts seldom\nagree. Analysing 5,680 coding decisions from eight state-of-the-art LLMs across\nten thematic categories, we confirm that a model's mean self-confidence already\ntracks inter-model agreement closely (Pearson r=0.82). Adding model\ndiversity-quantified as the normalised Shannon entropy of the panel's\nvotes-turns this single cue into a dual signal that explains agreement almost\ncompletely (R^2=0.979). The confidence-diversity duo enables a three-tier\nworkflow that auto-accepts 35% of segments with <5% audit-detected error and\nroutes the remainder for targeted human review, cutting manual effort by up to\n65%. Cross-domain replication on six public datasets spanning finance,\nmedicine, law and multilingual tasks confirms these gains (kappa improvements\nof 0.20-0.78). Our results establish a generalisable, evidence-based criterion\nfor calibrating AI judgement in qualitative research.",
      "authors": [
        "Zhilong Zhao",
        "Yindi Liu"
      ],
      "published": "2025-08-04T03:47:10+00:00",
      "updated": "2025-08-04T03:47:10+00:00",
      "arxiv_id": "2508.02029v1",
      "url": "http://arxiv.org/pdf/2508.02029v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "PCREQ: Automated Inference of Compatible Requirements for Python Third-party Library Upgrades",
      "abstract": "Python third-party libraries (TPLs) are essential in modern software\ndevelopment, but upgrades often cause compatibility issues, leading to system\nfailures. These issues fall into two categories: version compatibility issues\n(VCIs) and code compatibility issues (CCIs). Existing tools mainly detect\ndependency conflicts but overlook code-level incompatibilities, with no\nsolution fully automating the inference of compatible versions for both VCIs\nand CCIs. To fill this gap, we propose PCREQ, the first approach to\nautomatically infer compatible requirements by combining version and code\ncompatibility analysis. PCREQ integrates six modules: knowledge acquisition,\nversion compatibility assessment, invoked APIs and modules extraction, code\ncompatibility assessment, version change, and missing TPL completion. PCREQ\ncollects candidate versions, checks for conflicts, identifies API usage,\nevaluates code compatibility, and iteratively adjusts versions to generate a\ncompatible requirements.txt with a detailed repair report. To evaluate PCREQ,\nwe construct REQBench, a large-scale benchmark with 2,095 upgrade test cases\n(including 406 unsolvable by pip). Results show PCREQ achieves a 94.03%\ninference success rate, outperforming PyEGo (37.02%), ReadPyE (37.16%), and\nLLM-based approaches (GPT-4o, DeepSeek V3/R1) by 18-20%. PCREQ processes each\ncase from REQBench in 60.79s on average, demonstrating practical efficiency.\nPCREQ significantly reduces manual effort in troubleshooting upgrades,\nadvancing Python dependency maintenance automation.",
      "authors": [
        "Huashan Lei",
        "Guanping Xiao",
        "Yepang Liu",
        "Zheng Zheng"
      ],
      "published": "2025-08-04T03:34:30+00:00",
      "updated": "2025-08-04T03:34:30+00:00",
      "arxiv_id": "2508.02023v1",
      "url": "http://arxiv.org/pdf/2508.02023v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Evaluating Position Bias in Large Language Model Recommendations",
      "abstract": "Large Language Models (LLMs) are being increasingly explored as\ngeneral-purpose tools for recommendation tasks, enabling zero-shot and\ninstruction-following capabilities without the need for task-specific training.\nWhile the research community is enthusiastically embracing LLMs, there are\nimportant caveats to directly adapting them for recommendation tasks. In this\npaper, we show that LLM-based recommendation models suffer from position bias,\nwhere the order of candidate items in a prompt can disproportionately influence\nthe recommendations produced by LLMs. First, we analyse the position bias of\nLLM-based recommendations on real-world datasets, where results uncover\nsystemic biases of LLMs with high sensitivity to input orders. Furthermore, we\nintroduce a new prompting strategy to mitigate the position bias of LLM\nrecommendation models called Ranking via Iterative SElection (RISE). We compare\nour proposed method against various baselines on key benchmark datasets.\nExperiment results show that our method reduces sensitivity to input ordering\nand improves stability without requiring model fine-tuning or post-processing.",
      "authors": [
        "Ethan Bito",
        "Yongli Ren",
        "Estrid He"
      ],
      "published": "2025-08-04T03:30:26+00:00",
      "updated": "2025-08-04T03:30:26+00:00",
      "arxiv_id": "2508.02020v1",
      "url": "http://arxiv.org/pdf/2508.02020v1",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Prompting Large Language Models to Detect Dementia Family Caregivers",
      "abstract": "Social media, such as Twitter, provides opportunities for caregivers of\ndementia patients to share their experiences and seek support for a variety of\nreasons. Availability of this information online also paves the way for the\ndevelopment of internet-based interventions in their support. However, for this\npurpose, tweets written by caregivers of dementia patients must first be\nidentified. This paper demonstrates our system for the SMM4H 2025 shared task\n3, which focuses on detecting tweets posted by individuals who have a family\nmember with dementia. The task is outlined as a binary classification problem,\ndifferentiating between tweets that mention dementia in the context of a family\nmember and those that do not. Our solution to this problem explores large\nlanguage models (LLMs) with various prompting methods. Our results show that a\nsimple zero-shot prompt on a fine-tuned model yielded the best results. Our\nfinal system achieved a macro F1-score of 0.95 on the validation set and the\ntest set. Our full code is available on GitHub.",
      "authors": [
        "Md Badsha Biswas",
        "Ãzlem Uzuner"
      ],
      "published": "2025-08-04T02:39:16+00:00",
      "updated": "2025-08-04T02:39:16+00:00",
      "arxiv_id": "2508.01999v1",
      "url": "http://arxiv.org/pdf/2508.01999v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Prefill-Decode Aggregation or Disaggregation? Unifying Both for Goodput-Optimized LLM Serving",
      "abstract": "An ongoing debate considers whether prefill-decode (PD) aggregation or\ndisaggregation is superior for serving large language models (LLMs). This has\ndriven optimizations for both approaches, each showing distinct advantages.\nThis paper compares PD aggregation and disaggregation, showing that each excels\nunder different service-level objectives (SLOs): aggregation is optimal for\ntight time-to-first-token (TTFT) and relaxed time-per-output-token (TPOT),\nwhile disaggregation excels for strict TPOT and relaxed TTFT. However, under\nbalanced TTFT and TPOT SLOs, neither approach delivers optimal goodput.\n  This paper proposes TaiChi, an LLM serving system that unifies PD\ndisaggregation and aggregation for optimal goodput under any combination of\nTTFT and TPOT SLOs. TaiChi uses a unified disaggregation-aggregation\narchitecture with differentiated-capability GPU instances: prefill-heavy (fast\nprefill, high-interference decode) and decode-heavy (low-interference decode,\nslow prefill). Three configurable sliders control the ratio between these\ninstances and their chunk sizes. TaiChi adapts to various SLO regimes by\nadjusting sliders. When TTFT constraints are tight, TaiChi resembles a PD\naggregation configuration; when TPOT dominates, it adapts toward PD\ndisaggregation. Crucially, under balanced SLOs, TaiChi enables a hybrid mode\nfor superior goodput. The key innovation behind this hybrid mode is latency\nshifting: selectively reallocating GPU resources from requests that meet SLOs\nto those at risk of violation, maximizing the number of SLO-satisfied requests.\nThis fine-grained latency shifting is orchestrated by two scheduling\nmechanisms: flowing decode scheduling to control TPOTs and length-aware prefill\nscheduling to manage TTFTs, which jointly optimize request assignment. Our\nexperiments show TaiChi improves goodput by up to 77% over state-of-the-art\nsystems under balanced TTFT and TPOT SLOs.",
      "authors": [
        "Chao Wang",
        "Pengfei Zuo",
        "Zhangyu Chen",
        "Yunkai Liang",
        "Zhou Yu",
        "Ming-Chang Yang"
      ],
      "published": "2025-08-04T02:13:53+00:00",
      "updated": "2025-08-04T02:13:53+00:00",
      "arxiv_id": "2508.01989v1",
      "url": "http://arxiv.org/pdf/2508.01989v1",
      "categories": [
        "cs.DC"
      ],
      "primary_category": "cs.DC"
    },
    {
      "title": "TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models",
      "abstract": "To address the severe data scarcity in Tibetan, a low-resource language\nspoken by over six million people, we introduce TIBSTC-CoT, the large-scale,\nmulti-domain Tibetan dataset automatically constructed via chain-of-thought\nprompting with large language models (LLMs). TIBSTC-CoT establishes a scalable\nand reproducible framework for dataset creation in low-resource settings,\ncovering diverse domains and reasoning patterns essential for language\nunderstanding and generation. Building on this dataset, we develop the\nSunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with\nchain-of-thought capabilities. Trained entirely on TIBSTC-CoT,\nSunshine-thinking has demonstrated strong reasoning and generation performance,\ncomparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a\nsignificant step toward inclusive AI by enabling high-quality Tibetan language\nprocessing through both resource creation and model innovation. All data are\navailable: https://github.com/Vicentvankor/sun-shine.",
      "authors": [
        "Fan Gao",
        "Cheng Huang",
        "Nyima Tashi",
        "Yutong Liu",
        "Xiangxiang Wang",
        "Thupten Tsering",
        "Ban Ma-bao",
        "Renzeg Duojie",
        "Gadeng Luosang",
        "Rinchen Dongrub",
        "Dorje Tashi",
        "Xiao Feng",
        "Hao Wang",
        "Yongbin Yu"
      ],
      "published": "2025-08-04T01:32:58+00:00",
      "updated": "2025-08-04T01:32:58+00:00",
      "arxiv_id": "2508.01977v1",
      "url": "http://arxiv.org/pdf/2508.01977v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Improving Hospital Risk Prediction with Knowledge-Augmented Multimodal EHR Modeling",
      "abstract": "Accurate prediction of clinical outcomes using Electronic Health Records\n(EHRs) is critical for early intervention, efficient resource allocation, and\nimproved patient care. EHRs contain multimodal data, including both structured\ndata and unstructured clinical notes that provide rich, context-specific\ninformation. In this work, we introduce a unified framework that seamlessly\nintegrates these diverse modalities, leveraging all relevant available\ninformation through a two-stage architecture for clinical risk prediction. In\nthe first stage, a fine-tuned Large Language Model (LLM) extracts crucial,\ntask-relevant information from clinical notes, which is enhanced by graph-based\nretrieval of external domain knowledge from sources such as a medical corpus\nlike PubMed, grounding the LLM's understanding. The second stage combines both\nunstructured representations and features derived from the structured data to\ngenerate the final predictions. This approach supports a wide range of clinical\ntasks. Here, we demonstrate its effectiveness on 30-day readmission and\nin-hospital mortality prediction. Experimental results show that our framework\nachieves strong performance, with AUC scores of $0.84$ and $0.92$,\nrespectively, despite these tasks involving severely imbalanced datasets, with\npositive rates ranging from approximately $4\\%$ to $13\\%$. Moreover, it\noutperforms all existing baselines and clinical practices, including\nestablished risk scoring systems. To the best of our knowledge, this is one of\nthe first frameworks for healthcare prediction which enhances the power of an\nLLM-based graph-guided knowledge retrieval method by combining it with\nstructured data for improved clinical outcome prediction.",
      "authors": [
        "Rituparna Datta",
        "Jiaming Cui",
        "Zihan Guan",
        "Rupesh Silwal",
        "Joshua C Eby",
        "Gregory Madden",
        "Anil Vullikanti"
      ],
      "published": "2025-08-04T01:03:16+00:00",
      "updated": "2025-08-04T01:03:16+00:00",
      "arxiv_id": "2508.01970v1",
      "url": "http://arxiv.org/pdf/2508.01970v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Accelerating LLM Reasoning via Early Rejection with Partial Reward Modeling",
      "abstract": "Large Language Models (LLMs) are increasingly relied upon for solving complex\nreasoning tasks in domains such as mathematics, logic, and multi-step question\nanswering. A growing line of work seeks to improve reasoning quality by scaling\ninference time compute particularly through Process Reward Models (PRMs), used\nto reward the reasoning at intermediate steps. While effective, these methods\nintroduce substantial computational overhead, especially when generating large\nnumbers of solutions in parallel. In this paper, we investigate whether PRMs\ncan be used mid-generation to provide early signals that enable the rejection\nof suboptimal candidates before full generation of step is complete. We\nintroduce the hypothesis that PRMs are also Partial Reward Models, meaning that\nthe scores they assign to partially completed reasoning step are predictive of\nfinal output quality. This allows for principled early rejection based on\nintermediate token-level signals. We support this hypothesis both\ntheoretically, by proving that the risk of discarding optimal beams decreases\nexponentially with generation length and empirically, by demonstrating a strong\ncorrelation between partial and final rewards across multiple reward models. On\nmath reasoning benchmarks, our method achieves up to 1.4$\\times$-9$\\times$\nreduction in inference FLOPs without degrading final performance. These results\nsuggest that early rejection is a powerful mechanism for improving the\ncompute-efficiency of reasoning in LLMs.",
      "authors": [
        "Seyyed Saeid Cheshmi",
        "Azal Ahmad Khan",
        "Xinran Wang",
        "Zirui Liu",
        "Ali Anwar"
      ],
      "published": "2025-08-04T00:58:56+00:00",
      "updated": "2025-08-04T00:58:56+00:00",
      "arxiv_id": "2508.01969v1",
      "url": "http://arxiv.org/pdf/2508.01969v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Kronecker-LoRA: hybrid Kronecker-LoRA adapters for scalable, sustainable fine-tuning",
      "abstract": "Fine-tuning massive pre-trained language models across many tasks demands\nadapters that are both parameter-efficient and highly expressive. We introduce\n\\textbf{Kron-LoRA}, a two-stage adapter that first factorizes each frozen\nlinear update as a Kronecker product \\[ \\Delta W = A \\otimes B \\] and then\ncompresses \\[ B \\in \\mathbb{R}^{d_{B2}\\times d_{B1}} \\] via an \\(r\\)-rank LoRA\ndecomposition \\(B \\approx B_{1}B_{2}\\). By leveraging \\[ \\mathrm{rank}(A\n\\otimes B) \\;=\\; \\mathrm{rank}(A)\\,\\mathrm{rank}(B), \\] Kron-LoRA retains the\nexpressivity of the update while using up to $4\\!\\times\\!$ fewer parameters\nthan a standard rank-8 LoRA adapter. Its compact adapter matrices also quantize\nto 8- or 4-bit with less accuracy degradation than LoRA, enabling further\nmemory and storage savings for on-device deployment. We benchmark on DistilBERT\nand Mistral-7B across five tasks (PIQA, HellaSwag, WinoGrande, ARC-Easy,\nARC-Challenge) over multiple epochs of adapter-only tuning: on DistilBERT, an\n840 K-parameter Kron-LoRA matches LoRA-16's performance, and on Mistral-7B, a\n5.7 M-parameter Kron-LoRA rivals LoRA-8 with modest memory savings and only a\n3-8\\% speed overhead. In sequential fine-tuning from ARC-Challenge to ARC-Easy,\nKron-LoRA retains 55.18\\% accuracy versus 53.17\\% for LoRA-8-despite using only\none-quarter of the adapter parameters-underscoring its competitive cross-task\ntransfer performance. By uniting Kronecker structure, low-rank compression,\nquantization-friendliness, and by providing transparent trade-off analysis,\nKron-LoRA offers a scalable, sustainable, and continual-learning-ready solution\nfor multi-task adaptation of large language models.",
      "authors": [
        "Yixin Shen"
      ],
      "published": "2025-08-04T00:02:15+00:00",
      "updated": "2025-08-04T00:02:15+00:00",
      "arxiv_id": "2508.01961v1",
      "url": "http://arxiv.org/pdf/2508.01961v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Agent-Based Feature Generation from Clinical Notes for Outcome Prediction",
      "abstract": "Electronic health records (EHRs) contain rich unstructured clinical notes\nthat could enhance predictive modeling, yet extracting meaningful features from\nthese notes remains challenging. Current approaches range from labor-intensive\nmanual clinician feature generation (CFG) to fully automated representational\nfeature generation (RFG) that lack interpretability and clinical relevance.\nHere we introduce SNOW (Scalable Note-to-Outcome Workflow), a modular\nmulti-agent system powered by large language models (LLMs) that autonomously\ngenerates structured clinical features from unstructured notes without human\nintervention. We evaluated SNOW against manual CFG, clinician-guided LLM\napproaches, and RFG methods for predicting 5-year prostate cancer recurrence in\n147 patients from Stanford Healthcare. While manual CFG achieved the highest\nperformance (AUC-ROC: 0.771), SNOW matched this performance (0.761) without\nrequiring any clinical expertise, significantly outperforming both baseline\nfeatures alone (0.691) and all RFG approaches. The clinician-guided LLM method\nalso performed well (0.732) but still required expert input. SNOW's specialized\nagents handle feature discovery, extraction, validation, post-processing, and\naggregation, creating interpretable features that capture complex clinical\ninformation typically accessible only through manual review. Our findings\ndemonstrate that autonomous LLM systems can replicate expert-level feature\nengineering at scale, potentially transforming how clinical ML models leverage\nunstructured EHR data while maintaining the interpretability essential for\nclinical deployment.",
      "authors": [
        "Jiayi Wang",
        "Jacqueline Jil Vallon",
        "Neil Panjwani",
        "Xi Ling",
        "Sushmita Vij",
        "Sandy Srinivas",
        "John Leppert",
        "Mark K. Buyyounouski",
        "Mohsen Bayati"
      ],
      "published": "2025-08-03T23:45:18+00:00",
      "updated": "2025-08-03T23:45:18+00:00",
      "arxiv_id": "2508.01956v1",
      "url": "http://arxiv.org/pdf/2508.01956v1",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback",
      "abstract": "Large Language Models (LLMs) are known to overuse certain terms like \"delve\"\nand \"intricate.\" The exact reasons for these lexical choices, however, have\nbeen unclear. Using Meta's Llama model, this study investigates the\ncontribution of Learning from Human Feedback (LHF), under which we subsume\nReinforcement Learning from Human Feedback and Direct Preference Optimization.\nWe present a straightforward procedure for detecting the lexical preferences of\nLLMs that are potentially LHF-induced. Next, we more conclusively link LHF to\nlexical overuse by experimentally emulating the LHF procedure and demonstrating\nthat participants systematically prefer text variants that include certain\nwords. This lexical overuse can be seen as a sort of misalignment, though our\nstudy highlights the potential divergence between the lexical expectations of\ndifferent populations -- namely LHF workers versus LLM users. Our work\ncontributes to the growing body of research on explainable artificial\nintelligence and emphasizes the importance of both data and procedural\ntransparency in alignment research.",
      "authors": [
        "Tom S. Juzek",
        "Zina B. Ward"
      ],
      "published": "2025-08-03T21:45:37+00:00",
      "updated": "2025-08-03T21:45:37+00:00",
      "arxiv_id": "2508.01930v1",
      "url": "http://arxiv.org/pdf/2508.01930v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2; I.2.7; I.2.6"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language",
      "abstract": "Despite the rapid advancement of large language models (LLMs), low-resource\nlanguages remain largely excluded from the NLP landscape. We present PunGPT2,\nthe first fully open-source suite of Punjabi large language models, trained\nfrom scratch on a 35GB domain-diverse corpus encompassing literature, religious\ntexts, news, and social discourse. Unlike prior multilingual approaches,\nPunGPT2 captures rich syntactic and morphological features unique to Punjabi\nthrough a tokenizer optimised with byte pair encoding and linguistically\naligned pretraining objectives. To improve factual grounding and domain recall,\nwe introduce Pun-RAG, a retrieval-augmented generation framework combining\nPunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We\nfurther develop Pun-Instruct, a parameter-efficient, instruction-tuned variant\nusing QLoRA, enabling robust zero-shot and instruction-following performance\nwith significantly reduced compute needs.\n  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system\nthat fuses sparse (BM25) and dense methods with quantum-inspired semantic\nmatching. By encoding queries using amplitude-based embeddings and retrieving\nvia quantum kernel similarity, Quantum-RAG achieves improved contextual\nrelevance with minimal memory overhead marking the first practical integration\nof quantum representations in low-resource language generation. Our models\nsignificantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in\nperplexity, factuality, and fluency. This work provides a scalable,\nreproducible blueprint for extending LLM capabilities to underrepresented\nlanguages and pioneers quantum-aware retrieval in low-resource NLP",
      "authors": [
        "Jaskaranjeet Singh",
        "Rakesh Thakur"
      ],
      "published": "2025-08-03T21:03:22+00:00",
      "updated": "2025-08-03T21:03:22+00:00",
      "arxiv_id": "2508.01918v1",
      "url": "http://arxiv.org/pdf/2508.01918v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "L3M+P: Lifelong Planning with Large Language Models",
      "abstract": "By combining classical planning methods with large language models (LLMs),\nrecent research such as LLM+P has enabled agents to plan for general tasks\ngiven in natural language. However, scaling these methods to general-purpose\nservice robots remains challenging: (1) classical planning algorithms generally\nrequire a detailed and consistent specification of the environment, which is\nnot always readily available; and (2) existing frameworks mainly focus on\nisolated planning tasks, whereas robots are often meant to serve in long-term\ncontinuous deployments, and therefore must maintain a dynamic memory of the\nenvironment which can be updated with multi-modal inputs and extracted as\nplanning knowledge for future tasks. To address these two issues, this paper\nintroduces L3M+P (Lifelong LLM+P), a framework that uses an external knowledge\ngraph as a representation of the world state. The graph can be updated from\nmultiple sources of information, including sensory input and natural language\ninteractions with humans. L3M+P enforces rules for the expected format of the\nabsolute world state graph to maintain consistency between graph updates. At\nplanning time, given a natural language description of a task, L3M+P retrieves\ncontext from the knowledge graph and generates a problem definition for\nclassical planners. Evaluated on household robot simulators and on a real-world\nservice robot, L3M+P achieves significant improvement over baseline methods\nboth on accurately registering natural language state changes and on correctly\ngenerating plans, thanks to the knowledge graph retrieval and verification.",
      "authors": [
        "Krish Agarwal",
        "Yuqian Jiang",
        "Jiaheng Hu",
        "Bo Liu",
        "Peter Stone"
      ],
      "published": "2025-08-03T21:01:50+00:00",
      "updated": "2025-08-03T21:01:50+00:00",
      "arxiv_id": "2508.01917v1",
      "url": "http://arxiv.org/pdf/2508.01917v1",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "Context-Adaptive Multi-Prompt Embedding with Large Language Models for Vision-Language Alignment",
      "abstract": "We propose Context-Adaptive Multi-Prompt Embedding, a novel approach to\nenrich semantic representations in vision-language contrastive learning. Unlike\nstandard CLIP-style models that rely on a single text embedding, our method\nintroduces multiple structured prompts, each containing a distinct adaptive\ntoken that captures diverse semantic aspects of the input text. We leverage a\npretrained LLM as the text encoder within the CLIP framework, processing all\nprompts jointly in a single forward pass. The resulting prompt embeddings are\ncombined into a unified text representation, enabling semantically richer\nalignment with visual features. To further promote semantic diversity and\nrepresentation quality, we incorporate a diversity regularization loss and a\nnegation-aware loss, encouraging specialization across prompts and improving\ncontrastive discrimination. Our method achieves consistent improvements on both\nimage-text and video-text retrieval benchmarks.",
      "authors": [
        "Dahun Kim",
        "Anelia Angelova"
      ],
      "published": "2025-08-03T20:48:43+00:00",
      "updated": "2025-08-06T03:51:06+00:00",
      "arxiv_id": "2508.02762v2",
      "url": "http://arxiv.org/pdf/2508.02762v2",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models",
      "abstract": "Training large language models (LLMs) typically involves pre-training on\nmassive corpora, only to restart the process entirely when new data becomes\navailable. A more efficient and resource-conserving approach would be continual\npre-training, where models are updated with new data rather than retraining\nfrom scratch. However, the introduction of new data often causes distribution\nshifts, leading to performance degradation on previously learned tasks. In this\npaper, we take a deeper look at two popular proposals for addressing this\ndistribution shift within the continual learning literature: experience replay\nand gradient alignment. We consider continual pre-training of models within the\nLlama family of architectures at a large scale across languages with 100\nbillion tokens of training data in each language, finding that both replay and\ngradient alignment lead to more stable learning without forgetting. This\nconclusion holds both as we vary the model scale and as we vary the number and\ndiversity of tasks. Moreover, we are the first to demonstrate the effectiveness\nof gradient alignment techniques in the context of LLM pre-training and propose\nan efficient implementation of meta-experience replay (MER) that imbues\nexperience replay with the benefits of gradient alignment despite negligible\ncompute and memory overhead. Our scaling analysis across model sizes and replay\nrates indicates that small rates of replaying old examples are definitely a\nmore valuable use of compute than investing in model size, but that it is more\ncompute efficient to scale the size of the model than invest in high rates of\nreplaying old examples.",
      "authors": [
        "Istabrak Abbes",
        "Gopeshh Subbaraj",
        "Matthew Riemer",
        "Nizar Islah",
        "Benjamin Therien",
        "Tsuguchika Tabaru",
        "Hiroaki Kingetsu",
        "Sarath Chandar",
        "Irina Rish"
      ],
      "published": "2025-08-03T20:07:15+00:00",
      "updated": "2025-08-03T20:07:15+00:00",
      "arxiv_id": "2508.01908v1",
      "url": "http://arxiv.org/pdf/2508.01908v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Multi-turn Natural Language to Graph Query Language Translation",
      "abstract": "In recent years, research on transforming natural language into graph query\nlanguage (NL2GQL) has been increasing. Most existing methods focus on\nsingle-turn transformation from NL to GQL. In practical applications, user\ninteractions with graph databases are typically multi-turn, dynamic, and\ncontext-dependent. While single-turn methods can handle straightforward\nqueries, more complex scenarios often require users to iteratively adjust their\nqueries, investigate the connections between entities, or request additional\ndetails across multiple dialogue turns. Research focused on single-turn\nconversion fails to effectively address multi-turn dialogues and complex\ncontext dependencies. Additionally, the scarcity of high-quality multi-turn\nNL2GQL datasets further hinders the progress of this field. To address this\nchallenge, we propose an automated method for constructing multi-turn NL2GQL\ndatasets based on Large Language Models (LLMs) , and apply this method to\ndevelop the MTGQL dataset, which is constructed from a financial market graph\ndatabase and will be publicly released for future research. Moreover, we\npropose three types of baseline methods to assess the effectiveness of\nmulti-turn NL2GQL translation, thereby laying a solid foundation for future\nresearch.",
      "authors": [
        "Yuanyuan Liang",
        "Lei Pan",
        "Tingyu Xie",
        "Yunshi Lan",
        "Weining Qian"
      ],
      "published": "2025-08-03T17:56:52+00:00",
      "updated": "2025-08-03T17:56:52+00:00",
      "arxiv_id": "2508.01871v1",
      "url": "http://arxiv.org/pdf/2508.01871v1",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "ProKG-Dial: Progressive Multi-Turn Dialogue Construction with Domain Knowledge Graphs",
      "abstract": "Current large language models (LLMs) excel at general NLP tasks but often\nlack domain specific precision in professional settings. Building a high\nquality domain specific multi turn dialogue dataset is essential for developing\nspecialized conversational systems. However, existing methods such as manual\nannotation, simulated human LLM interactions, and role based LLM dialogues are\nresource intensive or suffer from limitations in dialogue quality and domain\ncoverage. To address these challenges, we introduce ProKG Dial, a progressive\nframework for constructing knowledge intensive multi turn dialogue datasets\nusing domain specific knowledge graphs (KGs). ProKG Dial leverages the\nstructured nature of KGs to encode complex domain knowledge and relationships,\nproviding a solid foundation for generating meaningful and coherent dialogues.\nSpecifically, ProKG Dial begins by applying community detection to partition\nthe KG into semantically cohesive subgraphs. For each subgraph, the framework\nincrementally generates a series of questions and answers centered around a\ntarget entity, ensuring relevance and coverage. A rigorous filtering step is\nemployed to maintain high dialogue quality. We validate ProKG Dial on a medical\nknowledge graph by evaluating the generated dialogues in terms of diversity,\nsemantic coherence, and entity coverage. Furthermore, we fine tune a base LLM\non the resulting dataset and benchmark it against several baselines. Both\nautomatic metrics and human evaluations demonstrate that ProKG Dial\nsubstantially improves dialogue quality and domain specific performance,\nhighlighting its effectiveness and practical utility.",
      "authors": [
        "Yuanyuan Liang",
        "Xiaoman Wang",
        "Tingyu Xie",
        "Lei Pan"
      ],
      "published": "2025-08-03T17:52:42+00:00",
      "updated": "2025-08-03T17:52:42+00:00",
      "arxiv_id": "2508.01869v1",
      "url": "http://arxiv.org/pdf/2508.01869v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models",
      "abstract": "Large Language Models have demonstrated remarkable capabilities across\ndiverse tasks, yet they frequently generate hallucinations outputs that are\nfluent but factually incorrect or unsupported. We propose Counterfactual\nProbing, a novel approach for detecting and mitigating hallucinations in LLM\noutputs. Our method dynamically generates counterfactual statements that appear\nplausible but contain subtle factual errors, then evaluates the model's\nsensitivity to these perturbations. We hypothesize that genuine knowledge\nexhibits robustness to counterfactual variations, while hallucinated content\nshows inconsistent confidence patterns when confronted with plausible\nalternatives. Our comprehensive evaluation on TruthfulQA, factual statement\ndatasets, and curated hallucination examples demonstrates that counterfactual\nprobing achieves superior detection performance compared to baseline methods,\nwhile our adaptive mitigation strategies reduce hallucination scores by an\naverage of 24.5%. The approach requires no model retraining and can be\nintegrated into existing LLM pipelines as a realtime verification mechanism.",
      "authors": [
        "Yijun Feng"
      ],
      "published": "2025-08-03T17:29:48+00:00",
      "updated": "2025-08-03T17:29:48+00:00",
      "arxiv_id": "2508.01862v1",
      "url": "http://arxiv.org/pdf/2508.01862v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "CloudAnoAgent: Anomaly Detection for Cloud Sites via LLM Agent with Neuro-Symbolic Mechanism",
      "abstract": "Anomaly detection in cloud sites remains a critical yet challenging task.\nExisting approaches that rely solely on metric data often suffer from high\nfalse positive rates (FPR) due to data imbalance between normal and anomalous\nevents, leading to significant operational overhead for system reliance\nengineers. Recent advances in large language models (LLMs) offer new\nopportunities for integrating metrics with log data, enabling more accurate and\ninterpretable anomaly detection. In this paper, we propose CloudAnoAgent, the\nfirst neuro-symbolic LLM-based agent for anomaly detection in cloud\nenvironments. CloudAnoAgent jointly processes structured metrics and textual\nlog data in a unified pipeline, leveraging symbolic verification to validate\ndetection hypotheses and generate structured anomaly reports. To support\nsystematic evaluation, we introduce CloudAnoBench, the first benchmark that\nprovides LLM-generated paired metrics and log data with fine-grained anomaly\nbehavior annotations, filling a critical gap in existing datasets. Experimental\nresults demonstrate that CloudAnoAgent improves anomaly classification accuracy\nby 46.36% and 36.67% on average and reduces the FPR by 36.67% and 33.89% on\naverage over traditional baselines and LLM-only baseline, with a boost on\nanomaly type detection accuracy by 12.8% compared to vanilla LLM prompting.\nThese results demonstrate the strengths of our approach in improving detection\naccuracy, reducing false positives, and enhancing interpretability, thereby\nsupporting practical deployment in enterprise cloud environments.",
      "authors": [
        "Xinkai Zou",
        "Xuan Jiang",
        "Ruikai Huang",
        "Haoze He",
        "Parv Kapoor",
        "Jiahua Zhao"
      ],
      "published": "2025-08-03T16:59:43+00:00",
      "updated": "2025-08-03T16:59:43+00:00",
      "arxiv_id": "2508.01844v1",
      "url": "http://arxiv.org/pdf/2508.01844v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "MLP Memory: Language Modeling with Retriever-pretrained External Memory",
      "abstract": "While modern decoder-only LLMs achieve superior performance across various\ndomains, hallucinations have risen to be a common problem in their generated\ntext, hindering their application in knowledge-intensive tasks.\nRetriever-augmented generation (RAG) offers a solution, but the non-parametric\nnature of the retriever hinders its deep interaction with LLM. In this work, we\npropose to decouple memorization from the LLM decoder using a pretrained,\ndifferentiable external memory. The external memory is an MLP pretrained by\nimitating the behavior of a retriever on the entire pretraining dataset. Our\nresulting architecture, which comprises a transformer decoder and an external\nMLP memory pretrained on language modeling and retriever imitation\nrespectively, demonstrates strong perplexity and performance on downstream\ntasks. Experiments show our architecture exhibits steeper power-law scaling\nwith model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web\ndatasets compared to decoder-only models while benefiting from added training\nwithout overfitting. We demonstrate superior performance on three hallucination\nbenchmarks and nine memory-intensive tasks. Additionally, our approach delivers\n$80\\times$ speedup over $k$NN-LM (500M tokens) and $1.3\\times$ faster inference\nthan decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP\nmemory improves StrategyQA performance. We will open-source our code and models\nin the future.",
      "authors": [
        "Rubin Wei",
        "Jiaqi Cao",
        "Jiarui Wang",
        "Jushi Kai",
        "Qipeng Guo",
        "Bowen Zhou",
        "Zhouhan Lin"
      ],
      "published": "2025-08-03T16:40:53+00:00",
      "updated": "2025-08-03T16:40:53+00:00",
      "arxiv_id": "2508.01832v1",
      "url": "http://arxiv.org/pdf/2508.01832v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Frequency Point Game Environment for UAVs via Expert Knowledge and Large Language Model",
      "abstract": "Unmanned Aerial Vehicles (UAVs) have made significant advancements in\ncommunication stability and security through techniques such as frequency\nhopping, signal spreading, and adaptive interference suppression. However,\nchallenges remain in modeling spectrum competition, integrating expert\nknowledge, and predicting opponent behavior. To address these issues, we\npropose UAV-FPG (Unmanned Aerial Vehicle - Frequency Point Game), a\ngame-theoretic environment model that simulates the dynamic interaction between\ninterference and anti-interference strategies of opponent and ally UAVs in\ncommunication frequency bands. The model incorporates a prior expert knowledge\nbase to optimize frequency selection and employs large language models for path\nplanning, simulating a \"strong adversary\". Experimental results highlight the\neffectiveness of integrating the expert knowledge base and the large language\nmodel, with the latter significantly improving path planning in dynamic\nscenarios through iterative interactions, outperforming fixed-path strategies.\nUAV-FPG provides a robust platform for advancing anti-jamming strategies and\nintelligent decision-making in UAV communication systems.",
      "authors": [
        "Jingpu Yang"
      ],
      "published": "2025-08-03T16:39:34+00:00",
      "updated": "2025-08-03T16:39:34+00:00",
      "arxiv_id": "2508.02757v1",
      "url": "http://arxiv.org/pdf/2508.02757v1",
      "categories": [
        "cs.MA",
        "cs.GT",
        "cs.RO"
      ],
      "primary_category": "cs.MA"
    },
    {
      "title": "M3LLM: Model Context Protocol-aided Mixture of Vision Experts For Multimodal LLMs in Networks",
      "abstract": "Current Multimodal Large Language Models (MLLMs) rely on centralized\narchitectures and often suffer from poor alignment between the input task and\ntheir fixed visual encoding modules, which limits performance on diverse and\ndynamic visual tasks. With the increasing deployment of resource-efficient\nmodels on edge devices in wireless networks, a new opportunity emerges to\ndynamically use distributed vision experts for improved MLLM inference quality.\nTo enable this, we propose M3LLM, where the Model Context Protocol (MCP)\ncoordinates a mixture of vision experts to achieve distributed MLLMs.\nSpecifically, MCP is an open protocol that structures the input task context\ninto interpretable representations, enabling wireless network-aware\ncoordination between the central model backbone and edge-hosted vision experts.\nBased on the MCP representation, M3LLM formulates vision expert routing as a\njoint optimization problem that balances task-expert semantic compatibility and\nchannel performance. To solve the resulting gradient conflicts, we develop a\ndual-stream Soft Actor-Critic (SAC) algorithm with decoupled reward signals and\nintroduce an Adaptive Stability Enhancement Module (ASEM) based on hierarchical\nBayesian modeling to ensure effective routing. Experiments show that M3LLM\nimproves task accuracy, reduces communication cost, and enhances expert routing\nadaptability under dynamic wireless network conditions.",
      "authors": [
        "Yongjie Zeng",
        "Hongyang Du"
      ],
      "published": "2025-08-03T15:42:05+00:00",
      "updated": "2025-08-03T15:42:05+00:00",
      "arxiv_id": "2508.01805v1",
      "url": "http://arxiv.org/pdf/2508.01805v1",
      "categories": [
        "cs.NI"
      ],
      "primary_category": "cs.NI"
    },
    {
      "title": "Joint Lossless Compression and Steganography for Medical Images via Large Language Models",
      "abstract": "Recently, large language models (LLMs) have driven promis ing progress in\nlossless image compression. However, di rectly adopting existing paradigms for\nmedical images suf fers from an unsatisfactory trade-off between compression\n  performance and efficiency. Moreover, existing LLM-based\n  compressors often overlook the security of the compres sion process, which is\ncritical in modern medical scenarios.\n  To this end, we propose a novel joint lossless compression\n  and steganography framework. Inspired by bit plane slicing\n  (BPS), we find it feasible to securely embed privacy messages\n  into medical images in an invisible manner. Based on this in sight, an\nadaptive modalities decomposition strategy is first\n  devised to partition the entire image into two segments, pro viding global\nand local modalities for subsequent dual-path\n  lossless compression. During this dual-path stage, we inno vatively propose a\nsegmented message steganography algo rithm within the local modality path to\nensure the security of\n  the compression process. Coupled with the proposed anatom ical priors-based\nlow-rank adaptation (A-LoRA) fine-tuning\n  strategy, extensive experimental results demonstrate the su periority of our\nproposed method in terms of compression ra tios, efficiency, and security. The\nsource code will be made\n  publicly available.",
      "authors": [
        "Pengcheng Zheng",
        "Xiaorong Pu",
        "Kecheng Chen",
        "Jiaxin Huang",
        "Meng Yang",
        "Bai Feng",
        "Yazhou Ren",
        "Jianan Jiang"
      ],
      "published": "2025-08-03T14:45:51+00:00",
      "updated": "2025-08-03T14:45:51+00:00",
      "arxiv_id": "2508.01782v1",
      "url": "http://arxiv.org/pdf/2508.01782v1",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV"
    },
    {
      "title": "A comprehensive taxonomy of hallucinations in Large Language Models",
      "abstract": "Large language models (LLMs) have revolutionized natural language processing,\nyet their propensity for hallucination, generating plausible but factually\nincorrect or fabricated content, remains a critical challenge. This report\nprovides a comprehensive taxonomy of LLM hallucinations, beginning with a\nformal definition and a theoretical framework that posits its inherent\ninevitability in computable LLMs, irrespective of architecture or training. It\nexplores core distinctions, differentiating between intrinsic (contradicting\ninput context) and extrinsic (inconsistent with training data or reality), as\nwell as factuality (absolute correctness) and faithfulness (adherence to\ninput). The report then details specific manifestations, including factual\nerrors, contextual and logical inconsistencies, temporal disorientation,\nethical violations, and task-specific hallucinations across domains like code\ngeneration and multimodal applications. It analyzes the underlying causes,\ncategorizing them into data-related issues, model-related factors, and\nprompt-related influences. Furthermore, the report examines cognitive and human\nfactors influencing hallucination perception, surveys evaluation benchmarks and\nmetrics for detection, and outlines architectural and systemic mitigation\nstrategies. Finally, it introduces web-based resources for monitoring LLM\nreleases and performance. This report underscores the complex, multifaceted\nnature of LLM hallucinations and emphasizes that, given their theoretical\ninevitability, future efforts must focus on robust detection, mitigation, and\ncontinuous human oversight for responsible and reliable deployment in critical\napplications.",
      "authors": [
        "Manuel Cossio"
      ],
      "published": "2025-08-03T14:37:16+00:00",
      "updated": "2025-08-03T14:37:16+00:00",
      "arxiv_id": "2508.01781v1",
      "url": "http://arxiv.org/pdf/2508.01781v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
      "abstract": "With the rapid development of Model Context Protocol (MCP), the number of MCP\nservers has surpassed 10,000. However, existing MCP benchmarks are limited to\nsingle-server settings with only a few tools, hindering effective evaluation of\nagent capabilities in large-scale, real-world scenarios. To address this\nlimitation, we present LiveMCPBench, the first comprehensive benchmark\ncomprising 95 real-world tasks grounded in the MCP ecosystem, designed to\nevaluate LLM agents at scale across diverse servers. To support a scalable and\nreproducible evaluation pipeline in large-scale MCP environments, we curate\nLiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and\n527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework\nthat enables automated and adaptive evaluation in dynamic, time-varying task\nenvironments, achieving 81% agreement with human reviewers. Finally, we propose\nthe MCP Copilot Agent, a multi-step agent that routes tools for dynamic\nplanning and executes tools for API interaction across the entire LiveMCPTool\nsuite. Our evaluation covers 10 leading models, with the best-performing model\n(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large\nperformance variance across models, and several widely-used models perform\npoorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench\noffers the first unified framework for benchmarking LLM agents in realistic,\ntool-rich, and dynamic MCP environments, laying a solid foundation for scalable\nand reproducible research on agent capabilities. Our code and data will be\npublicly available at https://icip-cas.github.io/LiveMCPBench.",
      "authors": [
        "Guozhao Mo",
        "Wenliang Zhong",
        "Jiawei Chen",
        "Xuanang Chen",
        "Yaojie Lu",
        "Hongyu Lin",
        "Ben He",
        "Xianpei Han",
        "Le Sun"
      ],
      "published": "2025-08-03T14:36:42+00:00",
      "updated": "2025-08-03T14:36:42+00:00",
      "arxiv_id": "2508.01780v1",
      "url": "http://arxiv.org/pdf/2508.01780v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "The AI-Augmented Research Process: A Historian's Perspective",
      "abstract": "This paper presents a detailed case study of how artificial intelligence,\nespecially large language models, can be integrated into historical research\nworkflows. The workflow is divided into nine steps, covering the full research\ncycle from question formulation to dissemination and reproducibility, and\nincludes two framing phases that address setup and documentation. Each research\nstep is mapped across three operational domains: 1. LLM, referring to tasks\ndelegated to language models; 2. Mind, referring to conceptual and interpretive\ncontributions by the historian; and 3. Computational, referring to conventional\nprogramming-based methods like Python, R, Cytoscape, etc. The study emphasizes\nthat LLMs are not replacements for domain expertise but can support and expand\ncapacity of historians to process, verify, and interpret large corpora of\ntexts. At the same time, it highlights the necessity of rigorous quality\ncontrol, cross-checking outputs, and maintaining scholarly standards. Drawing\nfrom an in-depth study of three Shanghai merchants, the paper also proposes a\nstructured workflow based on a real case study hat articulates the cognitive\nlabor of the historian with both computational tools and generative AI. This\npaper makes both a methodological and epistemological contribution by showing\nhow AI can be responsibly incorporated into historical research through\ntransparent and reproducible workflows. It is intended as a practical guide and\ncritical reflection for historians facing the increasingly complex landscape of\nAI-enhanced scholarship.",
      "authors": [
        "Christian Henriot"
      ],
      "published": "2025-08-03T14:34:36+00:00",
      "updated": "2025-08-03T14:34:36+00:00",
      "arxiv_id": "2508.01779v1",
      "url": "http://arxiv.org/pdf/2508.01779v1",
      "categories": [
        "cs.CY"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning",
      "abstract": "Large language models have demonstrated remarkable capabilities in complex\nmathematical reasoning tasks, but they inevitably generate errors throughout\nmulti-step solutions. Process-level Reward Models (PRMs) have shown great\npromise by providing supervision and evaluation at each intermediate step,\nthereby effectively improving the models' reasoning abilities. However,\ntraining effective PRMs requires high-quality process reward data, yet existing\nmethods for constructing such data are often labour-intensive or inefficient.\nIn this paper, we propose an uncertainty-driven framework for automated process\nreward data construction, encompassing both data generation and annotation\nprocesses for PRMs. Additionally, we identify the limitations of both majority\nvote and PRMs, and introduce two generic uncertainty-aware output aggregation\nmethods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which\ncombine the strengths of majority vote with PRMs. Extensive experiments on\nProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the\nproposed PRM data construction framework, and demonstrate that the two output\naggregation methods further improve the mathematical reasoning abilities across\ndiverse PRMs. The code and data will be publicly available at\nhttps://github.com/Jiuzhouh/UnPRM.",
      "authors": [
        "Jiuzhou Han",
        "Wray Buntine",
        "Ehsan Shareghi"
      ],
      "published": "2025-08-03T14:14:13+00:00",
      "updated": "2025-08-03T14:14:13+00:00",
      "arxiv_id": "2508.01773v1",
      "url": "http://arxiv.org/pdf/2508.01773v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "A Trainable Optimizer",
      "abstract": "The concept of learning to optimize involves utilizing a trainable\noptimization strategy rather than relying on manually defined full gradient\nestimations such as ADAM. We present a framework that jointly trains the full\ngradient estimator and the trainable weights of the model. Specifically, we\nprove that pseudo-linear TO (Trainable Optimizer), a linear approximation of\nthe full gradient, matches SGD's convergence rate while effectively reducing\nvariance. Pseudo-linear TO incurs negligible computational overhead, requiring\nonly minimal additional tensor multiplications. To further improve\ncomputational efficiency, we introduce two simplified variants of Pseudo-linear\nTO. Experiments demonstrate that TO methods converge faster than benchmark\nalgorithms (e.g., ADAM) in both strongly convex and non-convex settings, and\nfine tuning of an LLM.",
      "authors": [
        "Ruiqi Wang",
        "Diego Klabjan"
      ],
      "published": "2025-08-03T14:06:07+00:00",
      "updated": "2025-08-03T14:06:07+00:00",
      "arxiv_id": "2508.01764v1",
      "url": "http://arxiv.org/pdf/2508.01764v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "LLM-Assisted Model-Based Fuzzing of Protocol Implementations",
      "abstract": "Testing network protocol implementations is critical for ensuring the\nreliability, security, and interoperability of distributed systems. Faults in\nprotocol behavior can lead to vulnerabilities and system failures, especially\nin real-time and mission-critical applications. A common approach to protocol\ntesting involves constructing Markovian models that capture the state\ntransitions and expected behaviors of the protocol. However, building such\nmodels typically requires significant domain expertise and manual effort,\nmaking the process time-consuming and difficult to scale across diverse\nprotocols and implementations.\n  We propose a novel method that leverages large language models (LLMs) to\nautomatically generate sequences for testing network protocol implementations.\nOur approach begins by defining the full set of possible protocol states, from\nwhich the LLM selects a subset to model the target implementation. Using this\nstate-based model, we prompt the LLM to generate code that produces sequences\nof states. This program serves as a protocol-specific sequences generator. The\nsequences generator then generates test inputs to call the protocol\nimplementation under various conditions. We evaluated our approach on three\nwidely used network protocol implementations and successfully identified 12\npreviously unknown vulnerabilities. We have reported them to the respective\ndevelopers for confirmation. This demonstrates the practical effectiveness of\nour LLM-assisted fuzzing framework in uncovering real-world security issues.",
      "authors": [
        "Changze Huang",
        "Di Wang",
        "Zhi Quan Zhou"
      ],
      "published": "2025-08-03T13:16:18+00:00",
      "updated": "2025-08-03T13:16:18+00:00",
      "arxiv_id": "2508.01750v1",
      "url": "http://arxiv.org/pdf/2508.01750v1",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Bayes-Entropy Collaborative Driven Agents for Research Hypotheses Generation and Optimization",
      "abstract": "The exponential growth of scientific knowledge has made the automated\ngeneration of scientific hypotheses that combine novelty, feasibility, and\nresearch value a core challenge. Existing methods based on large language\nmodels fail to systematically model the inherent in hypotheses or incorporate\nthe closed-loop feedback mechanisms crucial for refinement. This paper proposes\na multi-agent collaborative framework called HypoAgents, which for the first\ntime integrates Bayesian reasoning with an information entropy-driven search\nmechanism across three stages-hypotheses generation, evidence validation, and\nhypotheses Refinement-to construct an iterative closed-loop simulating\nscientists' cognitive processes. Specifically, the framework first generates an\ninitial set of hypotheses through diversity sampling and establishes prior\nbeliefs based on a composite novelty-relevance-feasibility (N-R-F) score. It\nthen employs etrieval-augmented generation (RAG) to gather external literature\nevidence, updating the posterior probabilities of hypotheses using Bayes'\ntheorem. Finally, it identifies high-uncertainty hypotheses using information\nentropy $H = - \\sum {{p_i}\\log {p_i}}$ and actively refines them, guiding the\niterative optimization of the hypothesis set toward higher quality and\nconfidence. Experimental results on the ICLR 2025 conference real-world\nresearch question dataset (100 research questions) show that after 12\noptimization iterations, the average ELO score of generated hypotheses improves\nby 116.3, surpassing the benchmark of real paper abstracts by 17.8, while the\nframework's overall uncertainty, as measured by Shannon entropy, decreases\nsignificantly by 0.92. This study presents an interpretable probabilistic\nreasoning framework for automated scientific discovery, substantially improving\nthe quality and reliability of machine-generated research hypotheses.",
      "authors": [
        "Shiyang Duan",
        "Yuan Tian",
        "Qi Bing",
        "Xiaowei Shao"
      ],
      "published": "2025-08-03T13:05:32+00:00",
      "updated": "2025-08-03T13:05:32+00:00",
      "arxiv_id": "2508.01746v1",
      "url": "http://arxiv.org/pdf/2508.01746v1",
      "categories": [
        "cs.AI",
        "I.2.4"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "AGFT: An Adaptive GPU Frequency Tuner for Real-Time LLM Inference Optimization",
      "abstract": "The explosive growth of interactive Large Language Models (LLMs) has placed\nunprecedented demands for low latency on cloud GPUs, forcing them into\nhigh-power modes and causing escalating energy costs. Real-time inference\nworkloads exhibit significant dynamic volatility, presenting substantial\nenergy-saving opportunities. However, traditional static or rule-based power\nmanagement strategies struggle to exploit these opportunities without\ncompromising peak performance. To address this challenge, we propose AGFT (An\nAdaptive GPU Frequency Tuner), a framework that employs online reinforcement\nlearning to autonomously learn an optimal frequency tuning policy. By\nmonitoring real-time features like request load and latency, AGFT utilizes\nfine-grained frequency control for precise adjustments and intelligent action\nspace pruning for stable, efficient decision-making. This creates a robust,\nautomated energy management solution. We comprehensively evaluated AGFT in an\nenvironment simulating realistic, fluctuating inference requests. The\nexperimental results demonstrate that AGFT successfully saves 44.3% of GPU\nenergy consumption while introducing a minimal performance latency overhead of\nunder 10%. This achievement translates into a comprehensive Energy-Delay\nProduct (EDP) optimization of up to 40.3%, clearly showing that our framework\ncan significantly enhance the energy efficiency and economic benefits of\nexisting LLM inference clusters without compromising service quality.",
      "authors": [
        "Zicong Ye",
        "Kunming Zhang",
        "Guoming Tang"
      ],
      "published": "2025-08-03T13:02:07+00:00",
      "updated": "2025-08-03T13:02:07+00:00",
      "arxiv_id": "2508.01744v1",
      "url": "http://arxiv.org/pdf/2508.01744v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction",
      "abstract": "Identifying user preferences in dialogue systems is a pivotal aspect of\nproviding satisfying services. Current research shows that using large language\nmodels (LLMs) to fine-tune a task-specific preference extractor yields\nexcellent results in terms of accuracy and generalization. However, the primary\nchallenge stems from the inherent difficulty in obtaining high-quality labeled\nmulti-turn dialogue data. Accurately tracking user preference transitions\nacross turns not only demands intensive domain expertise and contextual\nconsistency maintenance for annotators (termed \\textbf{``Annotating\nDisaster''}) but also complicates model training due to error propagation in\nsequential dependency learning. Inspired by the observation that multi-turn\npreference extraction can be decomposed into iterative executions of one-turn\nextraction processes. We propose a novel dialogue data generation framework\nnamed \\textbf{IterChat}. First, we construct a new data format that categorizes\nthe dialogue data into attributed historical preferences and one-turn\ndialogues. This reduces the probability of annotation errors and improves\nannotation efficiency. Then, to generate a high-quality and diverse dialogue\ndataset, we adopt GPT4 to pre-define the preference slots in the target\npreference extractor task and then randomly sample the subset of the slots and\ntheir corresponding schema values to create the dialogue datasets. Experimental\nresults indicate that fine-tuning or only few-shot prompting with the new\ndialogue format yields superior performance compared to the original multi-turn\ndialogues. Additionally, the new data format improves annotator efficiency with\na win rate of 28.4\\% higher than the original multi-turn dialogues.",
      "authors": [
        "Cheng Wang",
        "ziru Liu",
        "Pengcheng Tang",
        "Mingyu Zhang",
        "Quanyu Dai",
        "Yue Zhu"
      ],
      "published": "2025-08-03T12:44:03+00:00",
      "updated": "2025-08-03T12:44:03+00:00",
      "arxiv_id": "2508.01739v1",
      "url": "http://arxiv.org/pdf/2508.01739v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ReflecSched: Solving Dynamic Flexible Job-Shop Scheduling via LLM-Powered Hierarchical Reflection",
      "abstract": "Dynamic Flexible Job-Shop Scheduling (DFJSP) is an NP-hard problem challenged\nby real-time event adaptation and complex machine routing. While traditional\ndispatching rules are efficient but rigid, deep learning approaches are opaque\nand require intricate feature engineering. Large Language Models (LLMs) promise\nadaptive reasoning without this engineering overhead, yet we find their direct\napplication is suboptimal. Baseline LLMs suffer from three key pitfalls: the\nlong-context paradox, where crucial data is underutilized; an underutilization\nof expert heuristics; and myopic decision-making. To address this, we propose\nReflecSched, a framework that empowers the LLM beyond a direct scheduler by\nequipping it with a strategic analysis capability. ReflecSched tasks the LLM to\nanalyze heuristic-driven simulations across multiple planning horizons and\ndistill them into a concise, natural-language summary termed ``Strategic\nExperience''. This summary is then integrated into the prompt of a final\ndecision-making module, guiding it to produce non-myopic actions. Experiments\nshow that ReflecSched not only statistically significantly outperforms direct\nLLM baselines, securing a 71.35\\% Win Rate and a 2.755\\% Relative Percentage\nDeviation reduction, but also surpasses the performance of all individual\nheuristics evaluated, all while demonstrably mitigating the three identified\npitfalls. Additionally, ReflecSched performs on par with the best heuristic\ntailored to each instance across all problem cases.",
      "authors": [
        "Shijie Cao",
        "Yuan Yuan"
      ],
      "published": "2025-08-03T11:26:35+00:00",
      "updated": "2025-08-03T11:26:35+00:00",
      "arxiv_id": "2508.01724v1",
      "url": "http://arxiv.org/pdf/2508.01724v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "OpenMap: Instruction Grounding via Open-Vocabulary Visual-Language Mapping",
      "abstract": "Grounding natural language instructions to visual observations is fundamental\nfor embodied agents operating in open-world environments. Recent advances in\nvisual-language mapping have enabled generalizable semantic representations by\nleveraging vision-language models (VLMs). However, these methods often fall\nshort in aligning free-form language commands with specific scene instances,\ndue to limitations in both instance-level semantic consistency and instruction\ninterpretation. We present OpenMap, a zero-shot open-vocabulary visual-language\nmap designed for accurate instruction grounding in navigation tasks. To address\nsemantic inconsistencies across views, we introduce a Structural-Semantic\nConsensus constraint that jointly considers global geometric structure and\nvision-language similarity to guide robust 3D instance-level aggregation. To\nimprove instruction interpretation, we propose an LLM-assisted\nInstruction-to-Instance Grounding module that enables fine-grained instance\nselection by incorporating spatial context and expressive target descriptions.\nWe evaluate OpenMap on ScanNet200 and Matterport3D, covering both semantic\nmapping and instruction-to-target retrieval tasks. Experimental results show\nthat OpenMap outperforms state-of-the-art baselines in zero-shot settings,\ndemonstrating the effectiveness of our method in bridging free-form language\nand 3D perception for embodied navigation.",
      "authors": [
        "Danyang Li",
        "Zenghui Yang",
        "Guangpeng Qi",
        "Songtao Pang",
        "Guangyong Shang",
        "Qiang Ma",
        "Zheng Yang"
      ],
      "published": "2025-08-03T11:25:52+00:00",
      "updated": "2025-08-03T11:25:52+00:00",
      "arxiv_id": "2508.01723v1",
      "url": "http://arxiv.org/pdf/2508.01723v1",
      "categories": [
        "cs.RO",
        "I.2.9; I.2.7; I.2.10"
      ],
      "primary_category": "cs.RO"
    },
    {
      "title": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications",
      "abstract": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models.",
      "authors": [
        "Raviraj Joshi",
        "Rakesh Paul",
        "Kanishk Singla",
        "Anusha Kamath",
        "Michael Evans",
        "Katherine Luna",
        "Shaona Ghosh",
        "Utkarsh Vaidya",
        "Eileen Long",
        "Sanjay Singh Chauhan",
        "Niranjan Wartikar"
      ],
      "published": "2025-08-03T10:35:05+00:00",
      "updated": "2025-08-03T10:35:05+00:00",
      "arxiv_id": "2508.01710v1",
      "url": "http://arxiv.org/pdf/2508.01710v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption",
      "abstract": "Large language models (LLMs) have advanced natural language processing (NLP)\nskills such as through next-token prediction and self-attention, but their\nability to integrate broad context also makes them prone to incorporating\nirrelevant information. Prior work has focused on semantic leakage, bias\nintroduced by semantically irrelevant context. In this paper, we introduce\nexpression leakage, a novel phenomenon where LLMs systematically generate\nsentimentally charged expressions that are semantically unrelated to the input\ncontext. To analyse the expression leakage, we collect a benchmark dataset\nalong with a scheme to automatically generate a dataset from free-form text\nfrom common-crawl. In addition, we propose an automatic evaluation pipeline\nthat correlates well with human judgment, which accelerates the benchmarking by\ndecoupling from the need of annotation for each analysed model. Our experiments\nshow that, as the model scales in the parameter space, the expression leakage\nreduces within the same LLM family. On the other hand, we demonstrate that\nexpression leakage mitigation requires specific care during the model building\nprocess, and cannot be mitigated by prompting. In addition, our experiments\nindicate that, when negative sentiment is injected in the prompt, it disrupts\nthe generation process more than the positive sentiment, causing a higher\nexpression leakage rate.",
      "authors": [
        "Berkay KÃ¶prÃ¼",
        "Mehrzad Mashal",
        "Yigit Gurses",
        "Akos Kadar",
        "Maximilian Schmitt",
        "Ditty Mathew",
        "Felix Burkhardt",
        "Florian Eyben",
        "BjÃ¶rn W. Schuller"
      ],
      "published": "2025-08-03T10:29:19+00:00",
      "updated": "2025-08-03T10:29:19+00:00",
      "arxiv_id": "2508.01708v1",
      "url": "http://arxiv.org/pdf/2508.01708v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "MHARFedLLM: Multimodal Human Activity Recognition Using Federated Large Language Model",
      "abstract": "Human Activity Recognition (HAR) plays a vital role in applications such as\nfitness tracking, smart homes, and healthcare monitoring. Traditional HAR\nsystems often rely on single modalities, such as motion sensors or cameras,\nlimiting robustness and accuracy in real-world environments. This work presents\nFedTime-MAGNET, a novel multimodal federated learning framework that advances\nHAR by combining heterogeneous data sources: depth cameras, pressure mats, and\naccelerometers. At its core is the Multimodal Adaptive Graph Neural Expert\nTransformer (MAGNET), a fusion architecture that uses graph attention and a\nMixture of Experts to generate unified, discriminative embeddings across\nmodalities. To capture complex temporal dependencies, a lightweight T5 encoder\nonly architecture is customized and adapted within this framework. Extensive\nexperiments show that FedTime-MAGNET significantly improves HAR performance,\nachieving a centralized F1 Score of 0.934 and a strong federated F1 Score of\n0.881. These results demonstrate the effectiveness of combining multimodal\nfusion, time series LLMs, and federated learning for building accurate and\nrobust HAR systems.",
      "authors": [
        "Asmit Bandyopadhyay",
        "Rohit Basu",
        "Tanmay Sen",
        "Swagatam Das"
      ],
      "published": "2025-08-03T10:05:06+00:00",
      "updated": "2025-08-03T10:05:06+00:00",
      "arxiv_id": "2508.01701v1",
      "url": "http://arxiv.org/pdf/2508.01701v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning",
      "abstract": "Although data visualization is powerful for revealing patterns and\ncommunicating insights, creating effective visualizations requires familiarity\nwith authoring tools and often disrupts the analysis flow. While large language\nmodels show promise for automatically converting analysis intent into\nvisualizations, existing methods function as black boxes without transparent\nreasoning processes, which prevents users from understanding design rationales\nand refining suboptimal outputs. To bridge this gap, we propose integrating\nChain-of-Thought (CoT) reasoning into the Natural Language to Visualization\n(NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for\nNL2VIS and develop an automatic pipeline to equip existing datasets with\nstructured reasoning steps. Second, we introduce nvBench-CoT, a specialized\ndataset capturing detailed step-by-step reasoning from ambiguous natural\nlanguage descriptions to finalized visualizations, which enables\nstate-of-the-art performance when used for model fine-tuning. Third, we develop\nDeepVIS, an interactive visual interface that tightly integrates with the CoT\nreasoning process, allowing users to inspect reasoning steps, identify errors,\nand make targeted adjustments to improve visualization outcomes. Quantitative\nbenchmark evaluations, two use cases, and a user study collectively demonstrate\nthat our CoT framework effectively enhances NL2VIS quality while providing\ninsightful reasoning steps to users.",
      "authors": [
        "Zhihao Shuai",
        "Boyan Li",
        "Siyu Yan",
        "Yuyu Luo",
        "Weikai Yang"
      ],
      "published": "2025-08-03T10:04:17+00:00",
      "updated": "2025-08-03T10:04:17+00:00",
      "arxiv_id": "2508.01700v1",
      "url": "http://arxiv.org/pdf/2508.01700v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding",
      "abstract": "Video Temporal Grounding (VTG) aims to precisely identify video event\nsegments in response to textual queries. The outputs of VTG tasks manifest as\nsequences of events, each defined by precise timestamps, saliency scores, and\ntextual descriptions. Despite recent advances, a fundamental limitation\npersists in existing Video Large Language Models (Video-LLMs): they process all\ntask tokens through identical and static pathways, failing to recognize that\ntemporal localization, saliency assessment, and textual generation represent\nfundamentally distinct tasks requiring specialized processing. To address this,\nwe introduce TimeExpert, a Mixture-of-Experts (MoE)-based Video-LLM that\neffectively decomposes VTG tasks by dynamically routing task-specific tokens\n(e.g., timestamps, saliency scores) to specialized experts, with increased\ncomputational efficiency. Our design choices enable precise handling of each\nsubtask, leading to improved event modeling across diverse VTG applications.\nExtensive experiments demonstrate that TimeExpert consistently achieves\nstate-of-the-art performance on various VTG tasks such as Dense Video\nCaptioning, Moment Retrieval, and Video Highlight Detection.",
      "authors": [
        "Zuhao Yang",
        "Yingchen Yu",
        "Yunqing Zhao",
        "Shijian Lu",
        "Song Bai"
      ],
      "published": "2025-08-03T10:03:58+00:00",
      "updated": "2025-08-03T10:03:58+00:00",
      "arxiv_id": "2508.01699v1",
      "url": "http://arxiv.org/pdf/2508.01699v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy",
      "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework for\nenhancing the capabilities of Large Language Models (LLMs), especially in\nknowledge-intensive tasks. Despite its advantages, current RAG methods often\nstruggle to *fully exploit knowledge during generation*. In particular, the\nsynergy between the model's internal parametric knowledge and external\nretrieved knowledge remains limited. Retrieved contents may sometimes mislead\ngeneration, while certain generated content can guide the model toward more\naccurate outputs. In this work, we propose Collaborative Chain-of-Agents, a\nframework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior\nperformance on open-domain and multi-hop QA tasks.",
      "authors": [
        "Yi Jiang",
        "Sendong Zhao",
        "Jianbo Li",
        "Haochun Wang",
        "Lizhe Zhang",
        "Yan Liu",
        "Bing Qin"
      ],
      "published": "2025-08-03T10:00:38+00:00",
      "updated": "2025-08-05T08:00:17+00:00",
      "arxiv_id": "2508.01696v2",
      "url": "http://arxiv.org/pdf/2508.01696v2",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Innovative tokenisation of structured data for LLM training",
      "abstract": "Data representation remains a fundamental challenge in machine learning,\nparticularly when adapting sequence-based architectures like Transformers and\nLarge Language Models (LLMs) for structured tabular data. Existing methods\noften fail to cohesively encode the mix of numerical and categorical features\nor preserve the inherent structure of tables. This paper introduces a novel,\nhybrid tokenisation methodology designed to convert tabular data into a\nunified, sequential format suitable for LLM training. Our approach combines\npredefined fixed tokens to represent structural elements and low-cardinality\ncategorical features, with a learned subword vocabulary using Byte-Pair\nEncoding (BPE) for high-cardinality and continuous values. We demonstrate the\nefficacy of this technique by applying it to a large-scale NetFlow dataset\n(CIDDS-001), preparing a corpus for a Network Intrusion Detection System (NIDS)\nfoundation model. The evaluation shows that our method is highly efficient,\nprocessing over 31 million network flows in under five hours and achieving a\nsignificant data compression ratio of 6.18:1. This process resulted in a\ncomputationally manageable corpus of over one billion tokens, establishing a\nviable and generalisable pathway for training foundation models on structured\ndata.",
      "authors": [
        "Kayvan Karim",
        "Hani Ragab Hassen. Hadj Batatia"
      ],
      "published": "2025-08-03T09:29:50+00:00",
      "updated": "2025-08-03T09:29:50+00:00",
      "arxiv_id": "2508.01685v1",
      "url": "http://arxiv.org/pdf/2508.01685v1",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "The Bidirectional Process Reward Model",
      "abstract": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning quality of Large Language Models (LLMs) by assigning fine-grained\nscores to intermediate reasoning steps within a solution trajectory. However,\nexisting PRMs predominantly adopt a unidirectional left-to-right (L2R)\nevaluation paradigm, which limits their ability to leverage global context,\nmaking it challenging to verify the consistency of earlier steps based on later\nones. In light of these challenges, we propose a novel bidirectional evaluation\nparadigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly\nincorporates a parallel right-to-left (R2L) evaluation stream alongside the\nconventional L2R flow, enabling later reasoning steps to help assess earlier\nones in real time. Notably, the built-in R2L evaluation is implemented solely\nthrough prompt modifications that reverse the original reasoning trajectory,\nwithout any additional parameters or inference latency introduced. This ensures\nBiPRM remains both efficient and broadly compatible with existing PRM studies.\nWe conduct extensive experiments on two mathematical reasoning benchmarks using\nsamples generated by three different policy models. Our method, BiPRM, is\nevaluated across three backbones and three distinct PRM objectives. Across all\nsettings, BiPRM consistently outperforms unidirectional baselines, achieving up\nto a 31.9% improvement in stepwise reward evaluation. Generally, our results\nhighlight BiPRM's effectiveness, robustness, and general applicability,\noffering a promising new direction for process-based reward modeling.",
      "authors": [
        "Lingyin Zhang",
        "Jun Gao",
        "Xiaoxue Ren",
        "Ziqiang Cao"
      ],
      "published": "2025-08-03T09:23:49+00:00",
      "updated": "2025-08-03T09:23:49+00:00",
      "arxiv_id": "2508.01682v1",
      "url": "http://arxiv.org/pdf/2508.01682v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "T-GRAG: A Dynamic GraphRAG Framework for Resolving Temporal Conflicts and Redundancy in Knowledge Retrieval",
      "abstract": "Large language models (LLMs) have demonstrated strong performance in natural\nlanguage generation but remain limited in knowle-\n  dge-intensive tasks due to outdated or incomplete internal knowledge.\nRetrieval-Augmented Generation (RAG) addresses this by incorporating external\nretrieval, with GraphRAG further enhancing performance through structured\nknowledge graphs and multi-hop reasoning. However, existing GraphRAG methods\nlargely ignore the temporal dynamics of knowledge, leading to issues such as\ntemporal ambiguity, time-insensitive retrieval, and semantic redundancy. To\novercome these limitations, we propose Temporal GraphRAG (T-GRAG), a dynamic,\ntemporally-aware RAG framework that models the evolution of knowledge over\ntime. T-GRAG consists of five key components: (1) a Temporal Knowledge Graph\nGenerator that creates time-stamped, evolving graph structures; (2) a Temporal\nQuery Decomposition mechanism that breaks complex temporal queries into\nmanageable sub-queries; (3) a Three-layer Interactive Retriever that\nprogressively filters and refines retrieval across temporal subgraphs; (4) a\nSource Text Extractor to mitigate noise; and (5) a LLM-based Generator that\nsynthesizes contextually and temporally accurate responses. We also introduce\nTime-LongQA, a novel benchmark dataset based on real-world corporate annual\nreports, designed to test temporal reasoning across evolving knowledge.\nExtensive experiments show that T-GRAG significantly outperforms prior RAG and\nGraphRAG baselines in both retrieval accuracy and response relevance under\ntemporal constraints, highlighting the necessity of modeling knowledge\nevolution for robust long-text question answering. Our code is publicly\navailable on the T-GRAG",
      "authors": [
        "Dong Li",
        "Yichen Niu",
        "Ying Ai",
        "Xiang Zou",
        "Biqing Qi",
        "Jianxing Liu"
      ],
      "published": "2025-08-03T09:15:36+00:00",
      "updated": "2025-08-03T09:15:36+00:00",
      "arxiv_id": "2508.01680v1",
      "url": "http://arxiv.org/pdf/2508.01680v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference",
      "abstract": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments.",
      "authors": [
        "Yi Zhao",
        "Yajuan Peng",
        "Cam-Tu Nguyen",
        "Zuchao Li",
        "Xiaoliang Wang",
        "Hai Zhao",
        "Xiaoming Fu"
      ],
      "published": "2025-08-03T09:15:36+00:00",
      "updated": "2025-08-03T09:15:36+00:00",
      "arxiv_id": "2508.02751v1",
      "url": "http://arxiv.org/pdf/2508.02751v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions",
      "abstract": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements.",
      "authors": [
        "Tae Soo Kim",
        "Yoonjoo Lee",
        "Yoonah Park",
        "Jiho Kim",
        "Young-Ho Kim",
        "Juho Kim"
      ],
      "published": "2025-08-03T09:04:48+00:00",
      "updated": "2025-08-07T05:03:27+00:00",
      "arxiv_id": "2508.01674v2",
      "url": "http://arxiv.org/pdf/2508.01674v2",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "QCBench: Evaluating Large Language Models on Domain-Specific Quantitative Chemistry",
      "abstract": "Quantitative chemistry plays a fundamental role in chemistry research,\nenabling precise predictions of molecular properties, reaction outcomes, and\nmaterial behaviors. While large language models (LLMs) have shown promise in\nchemistry-related tasks, their ability to perform rigorous, step-by-step\nquantitative reasoning remains underexplored. To fill this blank, we propose\nQCBench, a Quantitative Chemistry benchmark comprising 350 computational\nchemistry problems across 7 chemistry subfields (analytical chemistry,\nbio/organic chemistry, general chemistry, inorganic chemistry, physical\nchemistry, polymer chemistry and quantum chemistry), categorized into three\nhierarchical tiers-basic, intermediate, and expert-to systematically evaluate\nthe mathematical reasoning abilities of large language models (LLMs). Designed\nto minimize shortcuts and emphasize stepwise numerical reasoning, each problem\nfocuses on pure calculations rooted in real-world chemical vertical fields.\nQCBench enables fine-grained diagnosis of computational weaknesses, reveals\nmodel-specific limitations across difficulty levels, and lays the groundwork\nfor future improvements such as domain adaptive fine-tuning or multi-modal\nintegration. Evaluations on 19 LLMs demonstrate a consistent performance\ndegradation with increasing task complexity, highlighting the current gap\nbetween language fluency and scientific computation accuracy.",
      "authors": [
        "Jiaqing Xie",
        "Weida Wang",
        "Ben Gao",
        "Zhuo Yang",
        "Haiyuan Wan",
        "Shufei Zhang",
        "Tianfan Fu",
        "Yuqiang Li"
      ],
      "published": "2025-08-03T08:55:42+00:00",
      "updated": "2025-08-03T08:55:42+00:00",
      "arxiv_id": "2508.01670v1",
      "url": "http://arxiv.org/pdf/2508.01670v1",
      "categories": [
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "From Contrast to Commonality: Audio Commonality Captioning for Enhanced Audio-Text Cross-modal Understanding in Multimodal LLMs",
      "abstract": "Audio Captioning (AC) plays a pivotal role in enhancing audio-text\ncross-modal understanding during the pretraining and finetuning of multimodal\nlarge language models (MLLMs). To further strengthen this alignment, recent\nworks have proposed Audio Difference Captioning (ADC), which takes multiple\naudio inputs and encourages the model to describe their differences, thereby\npromoting fine-grained audio discrimination. However, despite its effectiveness\nin enabling difference-telling and detailed discrimination, ADC introduces a\nnotable semantic gap between the input audios-often rich in diverse sound\nevents-and the relatively brief, difference-focused output captions. This\ndeviation from AC-style descriptions leads to a mismatch with the pretraining\nobjective, resulting in catastrophic forgetting during finetuning. To mitigate\nthis issue, we propose Audio Commonality Captioning (ACC), a comparably\nchallenging but gentler alternative that encourages the model to capture the\nshared semantics across audio clips rather than emphasizing their detailed\ndifferences. Experimental results demonstrate that ACC not only effectively\nenhances audio-text understanding on primary captioning benchmarks but also\nbetter preserves general capabilities across diverse speech and music-related\ndownstream tasks, such as vocal sound classification (VSC), speech emotion\nrecognition (SER), musical instrument classification (MIC), and music genre\nclassification (MGC), compared to ADC. These findings validate that ACC\ncontributes to more robust cross-modal understanding and achieves a better\nbalance between generalization and task-specific performance in the context of\nMLLMs.",
      "authors": [
        "Yuhang Jia",
        "Xu Zhang",
        "Yong Qin"
      ],
      "published": "2025-08-03T08:32:42+00:00",
      "updated": "2025-08-03T08:32:42+00:00",
      "arxiv_id": "2508.01659v1",
      "url": "http://arxiv.org/pdf/2508.01659v1",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.SD"
    },
    {
      "title": "Authorship Attribution in Multilingual Machine-Generated Texts",
      "abstract": "As Large Language Models (LLMs) have reached human-like fluency and\ncoherence, distinguishing machine-generated text (MGT) from human-written\ncontent becomes increasingly difficult. While early efforts in MGT detection\nhave focused on binary classification, the growing landscape and diversity of\nLLMs require a more fine-grained yet challenging authorship attribution (AA),\ni.e., being able to identify the precise generator (LLM or human) behind a\ntext. However, AA remains nowadays confined to a monolingual setting, with\nEnglish being the most investigated one, overlooking the multilingual nature\nand usage of modern LLMs. In this work, we introduce the problem of\nMultilingual Authorship Attribution, which involves attributing texts to human\nor multiple LLM generators across diverse languages. Focusing on 18 languages\n-- covering multiple families and writing scripts -- and 8 generators (7 LLMs\nand the human-authored class), we investigate the multilingual suitability of\nmonolingual AA methods, their cross-lingual transferability, and the impact of\ngenerators on attribution performance. Our results reveal that while certain\nmonolingual AA methods can be adapted to multilingual settings, significant\nlimitations and challenges remain, particularly in transferring across diverse\nlanguage families, underscoring the complexity of multilingual AA and the need\nfor more robust approaches to better match real-world scenarios.",
      "authors": [
        "Lucio La Cava",
        "Dominik Macko",
        "RÃ³bert MÃ³ro",
        "Ivan Srba",
        "Andrea Tagarelli"
      ],
      "published": "2025-08-03T08:28:02+00:00",
      "updated": "2025-08-03T08:28:02+00:00",
      "arxiv_id": "2508.01656v1",
      "url": "http://arxiv.org/pdf/2508.01656v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "physics.soc-ph"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific Text Embeddings",
      "abstract": "Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on\naccurate and relevant retrieval of chemical literature. However,\ngeneral-purpose text embedding models frequently fail to adequately represent\ncomplex chemical terminologies, resulting in suboptimal retrieval quality.\nSpecialized embedding models tailored to chemical literature retrieval have not\nyet been developed, leaving a substantial performance gap. To address this\nchallenge, we introduce ChEmbed, a domain-adapted family of text embedding\nmodels fine-tuned on a dataset comprising chemistry-specific text from the\nPubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training\ndata, we employ large language models to synthetically generate queries,\nresulting in approximately 1.7 million high-quality query-passage pairs.\nAdditionally, we augment the tokenizer by adding 900 chemically specialized\ntokens to previously unused slots, which significantly reduces the\nfragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains\na 8192-token context length, enabling the efficient retrieval of longer\npassages compared to many other open-source embedding models, which typically\nhave a context length of 512 or 2048 tokens. Evaluated on our newly introduced\nChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general\nembedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents\na practical, lightweight, and reproducible embedding solution that effectively\nimproves retrieval for chemical literature search.",
      "authors": [
        "Ali Shiraee Kasmaee",
        "Mohammad Khodadad",
        "Mehdi Astaraki",
        "Mohammad Arshi Saloot",
        "Nicholas Sherck",
        "Hamidreza Mahyar",
        "Soheila Samiee"
      ],
      "published": "2025-08-03T08:04:44+00:00",
      "updated": "2025-08-03T08:04:44+00:00",
      "arxiv_id": "2508.01643v1",
      "url": "http://arxiv.org/pdf/2508.01643v1",
      "categories": [
        "cs.IR",
        "cs.CL"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Semantic Encryption: Secure and Effective Interaction with Cloud-based Large Language Models via Semantic Transformation",
      "abstract": "The increasing adoption of Cloud-based Large Language Models (CLLMs) has\nraised significant concerns regarding data privacy during user interactions.\nWhile existing approaches primarily focus on encrypting sensitive information,\nthey often overlook the logical structure of user inputs. This oversight can\nlead to reduced data utility and degraded performance of CLLMs. To address\nthese limitations and enable secure yet effective interactions, we propose\nSemantic Encryption (SE)-a plug-and-play framework designed to preserve both\nprivacy and utility. SE consists of two key components: Semantic Encoding and\nSemantic Decoding. In the encoding phase, a lightweight local model transforms\nthe original user input into an alternative semantic context that maintains the\noriginal intent and logical structure while obfuscating sensitive information.\nThis transformed input is then processed by the CLLM, which generates a\nresponse based on the transformed semantic context. To maintain a seamless user\nexperience, the decoding phase will reconstruct the CLLM's response back into\nthe original semantic context by referencing the locally stored user input.\nExtensive experimental evaluations demonstrate that SE effectively protects\ndata privacy without compromising data utility or user experience, offering a\npractical solution for secure interaction with CLLMs. Particularly, the\nproposed SE demonstrates a significant improvement over the state-of-the-art\nInferDPT, surpassing it across various evaluated metrics and datasets.",
      "authors": [
        "Dong Chen",
        "Tong Yang",
        "Feipeng Zhai",
        "Pengpeng Ouyang",
        "Qidong Liu",
        "Yafei Li",
        "Chong Fu",
        "Mingliang Xu"
      ],
      "published": "2025-08-03T07:54:40+00:00",
      "updated": "2025-08-03T07:54:40+00:00",
      "arxiv_id": "2508.01638v1",
      "url": "http://arxiv.org/pdf/2508.01638v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets",
      "abstract": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct.",
      "authors": [
        "Maziyar Panahi"
      ],
      "published": "2025-08-03T07:33:28+00:00",
      "updated": "2025-08-03T07:33:28+00:00",
      "arxiv_id": "2508.01630v1",
      "url": "http://arxiv.org/pdf/2508.01630v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "EAC-MoE: Expert-Selection Aware Compressor for Mixture-of-Experts Large Language Models",
      "abstract": "Mixture-of-Experts (MoE) has demonstrated promising potential in scaling\nLLMs. However, it is hindered by two critical challenges: (1) substantial GPU\nmemory consumption to load all experts; (2) low activated parameters cannot be\nequivalently translated into inference acceleration effects. In this work, we\npropose EAC-MoE, an Expert-Selection Aware Compressor for MoE-LLMs, which\ndeeply aligns with the characteristics of MoE from the perspectives of\nquantization and pruning, and introduces two modules to address these two\nchallenges respectively: (1) The expert selection bias caused by low-bit\nquantization is a major factor contributing to the performance degradation in\nMoE-LLMs. Based on this, we propose Quantization with Expert-Selection\nCalibration (QESC), which mitigates the expert selection bias by calibrating\nthe routers within the MoE; (2) There are always certain experts that are not\ncrucial for the corresponding tasks, yet causing inference latency. Therefore,\nwe propose Pruning based on Expert-Selection Frequency (PESF), which\nsignificantly improves inference speed by pruning less frequently used experts\nfor current task. Extensive experiments demonstrate that our approach\nsignificantly reduces memory usage and improves inference speed with minimal\nperformance degradation.",
      "authors": [
        "Yuanteng Chen",
        "Yuantian Shao",
        "Peisong Wang",
        "Jian Cheng"
      ],
      "published": "2025-08-03T07:30:42+00:00",
      "updated": "2025-08-03T07:30:42+00:00",
      "arxiv_id": "2508.01625v1",
      "url": "http://arxiv.org/pdf/2508.01625v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "A Multi-Agent Pokemon Tournament for Evaluating Strategic Reasoning of Large Language Models",
      "abstract": "This research presents LLM Pokemon League, a competitive tournament system\nthat leverages Large Language Models (LLMs) as intelligent agents to simulate\nstrategic decision-making in Pok\\'emon battles. The platform is designed to\nanalyze and compare the reasoning, adaptability, and tactical depth exhibited\nby different LLMs in a type-based, turn-based combat environment. By\nstructuring the competition as a single-elimination tournament involving\ndiverse AI trainers, the system captures detailed decision logs, including\nteam-building rationale, action selection strategies, and switching decisions.\nThe project enables rich exploration into comparative AI behavior, battle\npsychology, and meta-strategy development in constrained, rule-based game\nenvironments. Through this system, we investigate how modern LLMs understand,\nadapt, and optimize decisions under uncertainty, making Pok\\'emon League a\nnovel benchmark for AI research in strategic reasoning and competitive\nlearning.",
      "authors": [
        "Tadisetty Sai Yashwanth",
        "Dhatri C"
      ],
      "published": "2025-08-03T07:27:36+00:00",
      "updated": "2025-08-03T07:27:36+00:00",
      "arxiv_id": "2508.01623v1",
      "url": "http://arxiv.org/pdf/2508.01623v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding",
      "abstract": "Autoregressive models (ARMs) have long dominated the landscape of biomedical\nvision-language models (VLMs). Recently, masked diffusion models such as LLaDA\nhave emerged as promising alternatives, yet their application in the biomedical\ndomain remains largely underexplored. To bridge this gap, we introduce\n\\textbf{LLaDA-MedV}, the first large language diffusion model tailored for\nbiomedical image understanding through vision instruction tuning. LLaDA-MedV\nachieves relative performance gains of 7.855\\% over LLaVA-Med and 1.867\\% over\nLLaDA-V in the open-ended biomedical visual conversation task, and sets new\nstate-of-the-art accuracy on the closed-form subset of three VQA benchmarks:\n84.93\\% on VQA-RAD, 92.31\\% on SLAKE, and 95.15\\% on PathVQA. Furthermore, a\ndetailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of\ngenerating reasonably longer responses by explicitly controlling response\nlength, which can lead to more informative outputs. We also conduct an in-depth\nanalysis of both the training and inference stages, highlighting the critical\nroles of initialization weight selection, fine-tuning strategies, and the\ninterplay between sampling steps and response repetition. The code and model\nweight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV.",
      "authors": [
        "Xuanzhao Dong",
        "Wenhui Zhu",
        "Xiwen Chen",
        "Zhipeng Wang",
        "Peijie Qiu",
        "Shao Tang",
        "Xin Li",
        "Yalin Wang"
      ],
      "published": "2025-08-03T06:46:46+00:00",
      "updated": "2025-08-03T06:46:46+00:00",
      "arxiv_id": "2508.01617v1",
      "url": "http://arxiv.org/pdf/2508.01617v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "From Pixels to Places: A Systematic Benchmark for Evaluating Image Geolocalization Ability in Large Language Models",
      "abstract": "Image geolocalization, the task of identifying the geographic location\ndepicted in an image, is important for applications in crisis response, digital\nforensics, and location-based intelligence. While recent advances in large\nlanguage models (LLMs) offer new opportunities for visual reasoning, their\nability to perform image geolocalization remains underexplored. In this study,\nwe introduce a benchmark called IMAGEO-Bench that systematically evaluates\naccuracy, distance error, geospatial bias, and reasoning process. Our benchmark\nincludes three diverse datasets covering global street scenes, points of\ninterest (POIs) in the United States, and a private collection of unseen\nimages. Through experiments on 10 state-of-the-art LLMs, including both open-\nand closed-source models, we reveal clear performance disparities, with\nclosed-source models generally showing stronger reasoning. Importantly, we\nuncover geospatial biases as LLMs tend to perform better in high-resource\nregions (e.g., North America, Western Europe, and California) while exhibiting\ndegraded performance in underrepresented areas. Regression diagnostics\ndemonstrate that successful geolocalization is primarily dependent on\nrecognizing urban settings, outdoor environments, street-level imagery, and\nidentifiable landmarks. Overall, IMAGEO-Bench provides a rigorous lens into the\nspatial reasoning capabilities of LLMs and offers implications for building\ngeolocation-aware AI systems.",
      "authors": [
        "Lingyao Li",
        "Runlong Yu",
        "Qikai Hu",
        "Bowei Li",
        "Min Deng",
        "Yang Zhou",
        "Xiaowei Jia"
      ],
      "published": "2025-08-03T06:04:33+00:00",
      "updated": "2025-08-03T06:04:33+00:00",
      "arxiv_id": "2508.01608v1",
      "url": "http://arxiv.org/pdf/2508.01608v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Enhancing Math Reasoning in Small-sized LLMs via Preview Difficulty-Aware Intervention",
      "abstract": "Reinforcement learning scaling enhances the reasoning capabilities of large\nlanguage models, with reinforcement learning serving as the key technique to\ndraw out complex reasoning. However, key technical details of state-of-the-art\nreasoning LLMs, such as those in the OpenAI O series, Claude 3 series,\nDeepMind's Gemini 2.5 series, and Grok 3 series, remain undisclosed, making it\ndifficult for the research community to replicate their reinforcement learning\ntraining results. Therefore, we start our study from an Early Preview\nReinforcement Learning (EPRLI) algorithm built on the open-source GRPO\nframework, incorporating difficulty-aware intervention for math problems.\nApplied to a 1.5B-parameter LLM, our method achieves 50.0% on AIME24, 89.2% on\nMath500, 77.1% on AMC, 35.3% on Minerva, and 51.9% on OBench, superpass\nO1-Preview and is comparable to O1-mini within standard school-lab settings.",
      "authors": [
        "Xinhan Di",
        "JoyJiaoW"
      ],
      "published": "2025-08-03T05:41:36+00:00",
      "updated": "2025-08-03T05:41:36+00:00",
      "arxiv_id": "2508.01604v1",
      "url": "http://arxiv.org/pdf/2508.01604v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Enhancing Zero-Shot Brain Tumor Subtype Classification via Fine-Grained Patch-Text Alignment",
      "abstract": "The fine-grained classification of brain tumor subtypes from\nhistopathological whole slide images is highly challenging due to subtle\nmorphological variations and the scarcity of annotated data. Although\nvision-language models have enabled promising zero-shot classification, their\nability to capture fine-grained pathological features remains limited,\nresulting in suboptimal subtype discrimination. To address these challenges, we\npropose the Fine-Grained Patch Alignment Network (FG-PAN), a novel zero-shot\nframework tailored for digital pathology. FG-PAN consists of two key modules:\n(1) a local feature refinement module that enhances patch-level visual features\nby modeling spatial relationships among representative patches, and (2) a\nfine-grained text description generation module that leverages large language\nmodels to produce pathology-aware, class-specific semantic prototypes. By\naligning refined visual features with LLM-generated fine-grained descriptions,\nFG-PAN effectively increases class separability in both visual and semantic\nspaces. Extensive experiments on multiple public pathology datasets, including\nEBRAINS and TCGA, demonstrate that FG-PAN achieves state-of-the-art performance\nand robust generalization in zero-shot brain tumor subtype classification.",
      "authors": [
        "Lubin Gan",
        "Jing Zhang",
        "Linhao Qu",
        "Yijun Wang",
        "Siying Wu",
        "Xiaoyan Sun"
      ],
      "published": "2025-08-03T05:38:14+00:00",
      "updated": "2025-08-06T12:04:27+00:00",
      "arxiv_id": "2508.01602v2",
      "url": "http://arxiv.org/pdf/2508.01602v2",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "BeDKD: Backdoor Defense based on Dynamic Knowledge Distillation and Directional Mapping Modulator",
      "abstract": "Although existing backdoor defenses have gained success in mitigating\nbackdoor attacks, they still face substantial challenges. In particular, most\nof them rely on large amounts of clean data to weaken the backdoor mapping but\ngenerally struggle with residual trigger effects, resulting in persistently\nhigh attack success rates (ASR). Therefore, in this paper, we propose a novel\nBackdoor defense method based on Directional mapping module and adversarial\nKnowledge Distillation (BeDKD), which balances the trade-off between defense\neffectiveness and model performance using a small amount of clean and poisoned\ndata. We first introduce a directional mapping module to identify poisoned\ndata, which destroys clean mapping while keeping backdoor mapping on a small\nset of flipped clean data. Then, the adversarial knowledge distillation is\ndesigned to reinforce clean mapping and suppress backdoor mapping through a\ncycle iteration mechanism between trust and punish distillations using clean\nand identified poisoned data. We conduct experiments to mitigate mainstream\nattacks on three datasets, and experimental results demonstrate that BeDKD\nsurpasses the state-of-the-art defenses and reduces the ASR by 98% without\nsignificantly reducing the CACC. Our code are available in\nhttps://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/BeDKD.",
      "authors": [
        "Zhengxian Wu",
        "Juan Wen",
        "Wanli Peng",
        "Yinghan Zhou",
        "Changtong dou",
        "Yiming Xue"
      ],
      "published": "2025-08-03T05:28:01+00:00",
      "updated": "2025-08-03T05:28:01+00:00",
      "arxiv_id": "2508.01595v1",
      "url": "http://arxiv.org/pdf/2508.01595v1",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Polymorphic Combinatorial Frameworks (PCF): Guiding the Design of Mathematically-Grounded, Adaptive AI Agents",
      "abstract": "The Polymorphic Combinatorial Framework (PCF) leverages Large Language Models\n(LLMs) and mathematical frameworks to guide the meta-prompt enabled design of\nsolution spaces and adaptive AI agents for complex, dynamic environments.\nUnlike static agent architectures, PCF enables real-time parameter\nreconfiguration through mathematically-grounded combinatorial spaces, allowing\nagents to adapt their core behavioral traits dynamically. Grounded in\ncombinatorial logic, topos theory, and rough fuzzy set theory, PCF defines a\nmultidimensional SPARK parameter space (Skills, Personalities, Approaches,\nResources, Knowledge) to capture agent behaviors. This paper demonstrates how\nLLMs can parameterize complex spaces and estimate likely parameter\nvalues/variabilities. Using PCF, we parameterized mock caf\\'e domains (five\nlevels of complexity), estimated variables/variabilities, and conducted over\n1.25 million Monte Carlo simulations. The results revealed trends in agent\nadaptability and performance across the five complexity tiers, with diminishing\nreturns at higher complexity levels highlighting thresholds for scalable\ndesigns. PCF enables the generation of optimized agent configurations for\nspecific scenarios while maintaining logical consistency. This framework\nsupports scalable, dynamic, explainable, and ethical AI applications in domains\nlike customer service, healthcare, robotics, and collaborative systems, paving\nthe way for adaptable and cooperative next-generation polymorphic agents.",
      "authors": [
        "David Pearl",
        "Matthew Murphy",
        "James Intriligator"
      ],
      "published": "2025-08-03T04:19:31+00:00",
      "updated": "2025-08-03T04:19:31+00:00",
      "arxiv_id": "2508.01581v1",
      "url": "http://arxiv.org/pdf/2508.01581v1",
      "categories": [
        "cs.AI",
        "math.CO",
        "stat.CO"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "EvoVLMA: Evolutionary Vision-Language Model Adaptation",
      "abstract": "Pre-trained Vision-Language Models (VLMs) have been exploited in various\nComputer Vision tasks (e.g., few-shot recognition) via model adaptation, such\nas prompt tuning and adapters. However, existing adaptation methods are\ndesigned by human experts, requiring significant time cost and experience.\nInspired by recent advances in Large Language Models (LLMs) based code\ngeneration, we propose an Evolutionary Vision-Language Model Adaptation\n(EvoVLMA) method to automatically search training-free efficient adaptation\nalgorithms for VLMs. We recognize feature selection and logits computation as\nthe key functions in training-free VLM adaptation, and propose a two-stage\nLLM-assisted evolutionary algorithm for optimizing these parts in a sequential\nmanner, effectively addressing the challenge posed by the expansive search\nspace through a divide-and-conquer strategy. Besides, to enhance the stability\nand efficiency of searching process, we propose low-precision code conversion,\nweb based code execution and process monitoring, leading to a highly effective\nautomatic algorithm design system. Extensive experiments demonstrate that the\nalgorithms found by EvoVLMA can obtain promising results compared to previous\nmanually-designed ones. More specifically, in the 8-shot image classification\nsetting, the classical APE algorithm can be improved by 1.91 points in\nrecognition accuracy. This research opens new possibilities for automating the\noptimization of adaptation algorithms of pre-trained multimodal models. Code is\navailable at: https://github.com/kding1225/EvoVLMA",
      "authors": [
        "Kun Ding",
        "Ying Wang",
        "Shiming Xiang"
      ],
      "published": "2025-08-03T03:11:01+00:00",
      "updated": "2025-08-03T03:11:01+00:00",
      "arxiv_id": "2508.01558v1",
      "url": "http://arxiv.org/pdf/2508.01558v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Empowering Tabular Data Preparation with Language Models: Why and How?",
      "abstract": "Data preparation is a critical step in enhancing the usability of tabular\ndata and thus boosts downstream data-driven tasks. Traditional methods often\nface challenges in capturing the intricate relationships within tables and\nadapting to the tasks involved. Recent advances in Language Models (LMs),\nespecially in Large Language Models (LLMs), offer new opportunities to automate\nand support tabular data preparation. However, why LMs suit tabular data\npreparation (i.e., how their capabilities match task demands) and how to use\nthem effectively across phases still remain to be systematically explored. In\nthis survey, we systematically analyze the role of LMs in enhancing tabular\ndata preparation processes, focusing on four core phases: data acquisition,\nintegration, cleaning, and transformation. For each phase, we present an\nintegrated analysis of how LMs can be combined with other components for\ndifferent preparation tasks, highlight key advancements, and outline\nprospective pipelines.",
      "authors": [
        "Mengshi Chen",
        "Yuxiang Sun",
        "Tengchao Li",
        "Jianwei Wang",
        "Kai Wang",
        "Xuemin Lin",
        "Ying Zhang",
        "Wenjie Zhang"
      ],
      "published": "2025-08-03T03:00:02+00:00",
      "updated": "2025-08-03T03:00:02+00:00",
      "arxiv_id": "2508.01556v1",
      "url": "http://arxiv.org/pdf/2508.01556v1",
      "categories": [
        "cs.AI",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "MGCR-Net:Multimodal Graph-Conditioned Vision-Language Reconstruction Network for Remote Sensing Change Detection",
      "abstract": "With the advancement of remote sensing satellite technology and the rapid\nprogress of deep learning, remote sensing change detection (RSCD) has become a\nkey technique for regional monitoring. Traditional change detection (CD)\nmethods and deep learning-based approaches have made significant contributions\nto change analysis and detection, however, many outstanding methods still face\nlimitations in the exploration and application of multimodal data. To address\nthis, we propose the multimodal graph-conditioned vision-language\nreconstruction network (MGCR-Net) to further explore the semantic interaction\ncapabilities of multimodal data. Multimodal large language models (MLLM) have\nattracted widespread attention for their outstanding performance in computer\nvision, particularly due to their powerful visual-language understanding and\ndialogic interaction capabilities. Specifically, we design a MLLM-based\noptimization strategy to generate multimodal textual data from the original CD\nimages, which serve as textual input to MGCR. Visual and textual features are\nextracted through a dual encoder framework. For the first time in the RSCD\ntask, we introduce a multimodal graph-conditioned vision-language\nreconstruction mechanism, which is integrated with graph attention to construct\na semantic graph-conditioned reconstruction module (SGCM), this module\ngenerates vision-language (VL) tokens through graph-based conditions and\nenables cross-dimensional interaction between visual and textual features via\nmultihead attention. The reconstructed VL features are then deeply fused using\nthe language vision transformer (LViT), achieving fine-grained feature\nalignment and high-level semantic interaction. Experimental results on four\npublic datasets demonstrate that MGCR achieves superior performance compared to\nmainstream CD methods. Our code is available on\nhttps://github.com/cn-xvkong/MGCR",
      "authors": [
        "Chengming Wang",
        "Guodong Fan",
        "Jinjiang Li",
        "Min Gan",
        "C. L. Philip Chen"
      ],
      "published": "2025-08-03T02:50:08+00:00",
      "updated": "2025-08-03T02:50:08+00:00",
      "arxiv_id": "2508.01555v1",
      "url": "http://arxiv.org/pdf/2508.01555v1",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "primary_category": "eess.IV"
    },
    {
      "title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models",
      "abstract": "Prompt-based adversarial attacks have become an effective means to assess the\nrobustness of large language models (LLMs). However, existing approaches often\ntreat prompts as monolithic text, overlooking their structural\nheterogeneity-different prompt components contribute unequally to adversarial\nrobustness. Prior works like PromptRobust assume prompts are value-neutral, but\nour analysis reveals that complex, domain-specific prompts with rich structures\nhave components with differing vulnerabilities. To address this gap, we\nintroduce PromptAnatomy, an automated framework that dissects prompts into\nfunctional components and generates diverse, interpretable adversarial examples\nby selectively perturbing each component using our proposed method, ComPerturb.\nTo ensure linguistic plausibility and mitigate distribution shifts, we further\nincorporate a perplexity (PPL)-based filtering mechanism. As a complementary\nresource, we annotate four public instruction-tuning datasets using the\nPromptAnatomy framework, verified through human review. Extensive experiments\nacross these datasets and five advanced LLMs demonstrate that ComPerturb\nachieves state-of-the-art attack success rates. Ablation studies validate the\ncomplementary benefits of prompt dissection and PPL filtering. Our results\nunderscore the importance of prompt structure awareness and controlled\nperturbation for reliable adversarial robustness evaluation in LLMs. Code and\ndata are available at https://github.com/Yujiaaaaa/PACP.",
      "authors": [
        "Yujia Zheng",
        "Tianhao Li",
        "Haotian Huang",
        "Tianyu Zeng",
        "Jingyu Lu",
        "Chuangxin Chu",
        "Yuekai Huang",
        "Ziyou Jiang",
        "Qian Xiong",
        "Yuyao Ge",
        "Mingyang Li"
      ],
      "published": "2025-08-03T02:46:30+00:00",
      "updated": "2025-08-03T02:46:30+00:00",
      "arxiv_id": "2508.01554v1",
      "url": "http://arxiv.org/pdf/2508.01554v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale",
      "abstract": "Training software engineering (SWE) LLMs is bottlenecked by expensive\ninfrastructure, inefficient evaluation pipelines, scarce training data, and\ncostly quality control. We present RepoForge, an autonomous, end-to-end\npipeline that generates, evaluates, and trains SWE agents at scale. Our key\ncontributions include: (1) RepoForge-8B-Agent, achieving 17.4\\% on\nSWE-Bench-Verified~\\citep{swebench_verified2024}, establishing new\nstate-of-the-art for $\\leq$8B non-thinking LLMs; (2) 7,304 executable\nenvironments auto-generated from real GitHub commits with zero manual\nintervention; (3) 14$\\times$ storage reduction (1.4GB $\\rightarrow$ 102MB per\ninstance) via intelligent dependency management and image pruning; (4) $>$70\\%\nfaster evaluation using a Ray-powered~\\citep{ray2018} distributed RepoForge\nharness; (5) 19,000$\\times$ cheaper labeling through our automated\nSPICE~\\citep{spice2024} difficulty assessment technique. By unifying\nstorage-efficient sandboxing, Ray-powered evaluation harness, automated data\ngeneration, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate\nthat even $\\leq$8B models can reach new state-of-the-art performance on\ndemanding benchmarks like SWE-Bench-Verified. Our approach addresses critical\nbottlenecks in SWE agent training: high storage costs of container-based\nevaluation, inefficient sequential reward pipelines, limited availability of\nhigh-quality training data, expensive manual labeling, and multi-turn RL\npipeline bottlenecks.",
      "authors": [
        "Zhilong Chen",
        "Chengzong Zhao",
        "Boyuan Chen",
        "Dayi Lin",
        "Yihao Chen",
        "Arthur Leung",
        "Gopi Krishnan Rajbahadur",
        "Gustavo A. Oliva",
        "Ahmed E. Hassan"
      ],
      "published": "2025-08-03T02:34:16+00:00",
      "updated": "2025-08-03T02:34:16+00:00",
      "arxiv_id": "2508.01550v1",
      "url": "http://arxiv.org/pdf/2508.01550v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice",
      "abstract": "This paper investigates why recent generative AI models outperform humans in\ndata visualization knowledge tasks. Through systematic comparative analysis of\nresponses to visualization questions, we find that differences exist between\ntwo ChatGPT models and human outputs over rhetorical structure, knowledge\nbreadth, and perceptual quality. Our findings reveal that ChatGPT-4, as a more\nadvanced model, displays a hybrid of characteristics from both humans and\nChatGPT-3.5. The two models were generally favored over human responses, while\ntheir strengths in coverage and breadth, and emphasis on technical and\ntask-oriented visualization feedback collectively shaped higher overall\nquality. Based on our findings, we draw implications for advancing user\nexperiences based on the potential of LLMs and human perception over their\ncapabilities, with relevance to broader applications of AI.",
      "authors": [
        "Yongsu Ahn",
        "Nam Wook Kim"
      ],
      "published": "2025-08-03T02:14:00+00:00",
      "updated": "2025-08-03T02:14:00+00:00",
      "arxiv_id": "2508.01547v1",
      "url": "http://arxiv.org/pdf/2508.01547v1",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Getting out of the Big-Muddy: Escalation of Commitment in LLMs",
      "abstract": "Large Language Models (LLMs) are increasingly deployed in autonomous\ndecision-making roles across high-stakes domains. However, since models are\ntrained on human-generated data, they may inherit cognitive biases that\nsystematically distort human judgment, including escalation of commitment,\nwhere decision-makers continue investing in failing courses of action due to\nprior investment. Understanding when LLMs exhibit such biases presents a unique\nchallenge. While these biases are well-documented in humans, it remains unclear\nwhether they manifest consistently in LLMs or require specific triggering\nconditions. This paper investigates this question using a two-stage investment\ntask across four experimental conditions: model as investor, model as advisor,\nmulti-agent deliberation, and compound pressure scenario. Across N = 6,500\ntrials, we find that bias manifestation in LLMs is highly context-dependent. In\nindividual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate\nstrong rational cost-benefit logic with minimal escalation of commitment.\nHowever, multi-agent deliberation reveals a striking hierarchy effect (Study 3,\nN = 500): while asymmetrical hierarchies show moderate escalation rates\n(46.2%), symmetrical peer-based decision-making produces near-universal\nescalation (99.2%). Similarly, when subjected to compound organizational and\npersonal pressures (Study 4, N = 2,000), models exhibit high degrees of\nescalation of commitment (68.95% average allocation to failing divisions).\nThese findings reveal that LLM bias manifestation depends critically on social\nand organizational context rather than being inherent, with significant\nimplications for the deployment of multi-agent systems and unsupervised\noperations where such conditions may emerge naturally.",
      "authors": [
        "Emilio Barkett",
        "Olivia Long",
        "Paul KrÃ¶ger"
      ],
      "published": "2025-08-03T01:58:38+00:00",
      "updated": "2025-08-07T15:04:14+00:00",
      "arxiv_id": "2508.01545v2",
      "url": "http://arxiv.org/pdf/2508.01545v2",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable progress through\npreference-based fine-tuning, which critically depends on the quality of the\nunderlying training data. While human feedback is essential for improving data\nquality, it is costly and does not scale well. In this paper, we introduce\nRefine-n-Judge, an automated iterative approach that leverages a single LLM as\nboth a refiner and a judge to enhance dataset quality. Unlike existing\niterative refinement methods, Refine-n-Judge employs an LLM to both generate\nrefinements and explicitly evaluate each improvement, ensuring that every\niteration meaningfully enhances the dataset without requiring additional human\nannotation or a separate reward model. At each step, the LLM refines a response\nand judges whether the refinement is an improvement over the previous answer.\nThis process continues until the LLM prefers the initial answer over the\nrefinement, indicating no further improvements. This produces sequences of\nincreasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public\ndatasets spanning five corpora, targeting tasks such as coding, math, and\nconversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on\nRefine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of\ncomparisons against models tuned on the original dataset by GPT-4.\nAdditionally, we report performance gains: +5% on AlpacaEval and AlpacaEval\n2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces\nhigh-quality datasets and scalable model improvements.",
      "authors": [
        "Derin Cayir",
        "Renjie Tao",
        "Rashi Rungta",
        "Kai Sun",
        "Sean Chen",
        "Haidar Khan",
        "Minseok Kim",
        "Julia Reinspach",
        "Yue Liu"
      ],
      "published": "2025-08-03T01:56:03+00:00",
      "updated": "2025-08-03T01:56:03+00:00",
      "arxiv_id": "2508.01543v1",
      "url": "http://arxiv.org/pdf/2508.01543v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization",
      "abstract": "Prompt engineering is crucial for unlocking the potential of Large Language\nModels (LLMs). Still, since manual prompt design is often complex,\nnon-intuitive, and time-consuming, automatic prompt optimization has emerged as\na research area. However, a significant challenge in prompt optimization is\nmanaging the inherent trade-off between task performance, such as accuracy, and\ncontext size. Most existing automated methods focus on a single objective,\ntypically performance, thereby failing to explore the critical spectrum of\nefficiency and effectiveness. This paper introduces the MOPrompt, a novel\nMulti-objective Evolutionary Optimization (EMO) framework designed to optimize\nprompts for both accuracy and context size (measured in tokens) simultaneously.\nOur framework maps the Pareto front of prompt solutions, presenting\npractitioners with a set of trade-offs between context size and performance, a\ncrucial tool for deploying Large Language Models (LLMs) in real-world\napplications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,\nusing Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that\nMOPrompt substantially outperforms the baseline framework. For the Sabiazinho\nmodel, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)\nas the best baseline solution, but with a 31% reduction in token length.",
      "authors": [
        "Sara CÃ¢mara",
        "Eduardo Luz",
        "ValÃ©ria Carvalho",
        "Ivan Meneghini",
        "Gladston Moreira"
      ],
      "published": "2025-08-03T01:50:43+00:00",
      "updated": "2025-08-03T01:50:43+00:00",
      "arxiv_id": "2508.01541v1",
      "url": "http://arxiv.org/pdf/2508.01541v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Exploring Direct Instruction and Summary-Mediated Prompting in LLM-Assisted Code Modification",
      "abstract": "This paper presents a study of using large language models (LLMs) in\nmodifying existing code. While LLMs for generating code have been widely\nstudied, their role in code modification remains less understood. Although\n\"prompting\" serves as the primary interface for developers to communicate\nintents to LLMs, constructing effective prompts for code modification\nintroduces challenges different from generation. Prior work suggests that\nnatural language summaries may help scaffold this process, yet such approaches\nhave been validated primarily in narrow domains like SQL rewriting. This study\ninvestigates two prompting strategies for LLM-assisted code modification:\nDirect Instruction Prompting, where developers describe changes explicitly in\nfree-form language, and Summary-Mediated Prompting, where changes are made by\nediting the generated summaries of the code. We conducted an exploratory study\nwith 15 developers who completed modification tasks using both techniques\nacross multiple scenarios. Our findings suggest that developers followed an\niterative workflow: understanding the code, localizing the edit, and validating\noutputs through execution or semantic reasoning. Each prompting strategy\npresented trade-offs: direct instruction prompting was more flexible and easier\nto specify, while summary-mediated prompting supported comprehension, prompt\nscaffolding, and control. Developers' choice of strategy was shaped by task\ngoals and context, including urgency, maintainability, learning intent, and\ncode familiarity. These findings highlight the need for more usable prompt\ninteractions, including adjustable summary granularity, reliable summary-code\ntraceability, and consistency in generated summaries.",
      "authors": [
        "Ningzhi Tang",
        "Emory Smith",
        "Yu Huang",
        "Collin McMillan",
        "Toby Jia-Jun Li"
      ],
      "published": "2025-08-02T23:52:49+00:00",
      "updated": "2025-08-02T23:52:49+00:00",
      "arxiv_id": "2508.01523v1",
      "url": "http://arxiv.org/pdf/2508.01523v1",
      "categories": [
        "cs.SE",
        "cs.HC"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "End-to-End Personalization: Unifying Recommender Systems with Large Language Models",
      "abstract": "Recommender systems are essential for guiding users through the vast and\ndiverse landscape of digital content by delivering personalized and relevant\nsuggestions. However, improving both personalization and interpretability\nremains a challenge, particularly in scenarios involving limited user feedback\nor heterogeneous item attributes. In this article, we propose a novel hybrid\nrecommendation framework that combines Graph Attention Networks (GATs) with\nLarge Language Models (LLMs) to address these limitations. LLMs are first used\nto enrich user and item representations by generating semantically meaningful\nprofiles based on metadata such as titles, genres, and overviews. These\nenriched embeddings serve as initial node features in a user and movie\nbipartite graph, which is processed using a GAT based collaborative filtering\nmodel. To enhance ranking accuracy, we introduce a hybrid loss function that\ncombines Bayesian Personalized Ranking (BPR), cosine similarity, and robust\nnegative sampling. Post-processing involves reranking the GAT-generated\nrecommendations using the LLM, which also generates natural-language\njustifications to improve transparency. We evaluated our model on benchmark\ndatasets, including MovieLens 100k and 1M, where it consistently outperforms\nstrong baselines. Ablation studies confirm that LLM-based embeddings and the\ncosine similarity term significantly contribute to performance gains. This work\ndemonstrates the potential of integrating LLMs to improve both the accuracy and\ninterpretability of recommender systems.",
      "authors": [
        "Danial Ebrat",
        "Tina Aminian",
        "Sepideh Ahmadian",
        "Luis Rueda"
      ],
      "published": "2025-08-02T22:46:50+00:00",
      "updated": "2025-08-02T22:46:50+00:00",
      "arxiv_id": "2508.01514v1",
      "url": "http://arxiv.org/pdf/2508.01514v1",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "Canoe Paddling Quality Assessment Using Smart Devices: Preliminary Machine Learning Study",
      "abstract": "Over 22 million Americans participate in paddling-related activities\nannually, contributing to a global paddlesports market valued at 2.4 billion US\ndollars in 2020. Despite its popularity, the sport has seen limited integration\nof machine learning (ML) and remains hindered by the cost of coaching and\nspecialized equipment. This study presents a novel AI-based coaching system\nthat uses ML models trained on motion data and delivers stroke feedback via a\nlarge language model (LLM). Participants were recruited through a collaboration\nwith the NYU Concrete Canoe Team. Motion data were collected across two\nsessions, one with suboptimal form and one with corrected technique, using\nApple Watches and smartphones secured in sport straps. The data underwent\nstroke segmentation and feature extraction. ML models, including Support Vector\nClassifier, Random Forest, Gradient Boosting, and Extremely Randomized Trees,\nwere trained on both raw and engineered features. A web based interface was\ndeveloped to visualize stroke quality and deliver LLM-based feedback. Across\nfour participants, eight trials yielded 66 stroke samples. The Extremely\nRandomized Tree model achieved the highest performance with an F score of\n0.9496 under five fold cross validation. The web interface successfully\nprovided both quantitative metrics and qualitative feedback. Sensor placement\nnear the wrists improved data quality. Preliminary results indicate that\nsmartwatches and smartphones can enable low cost, accessible alternatives to\ntraditional paddling instruction. While limited by sample size, the study\ndemonstrates the feasibility of using consumer devices and ML to support stroke\nrefinement and technique improvement.",
      "authors": [
        "S. Parab",
        "A. Lamelas",
        "A. Hassan",
        "P. Bhote"
      ],
      "published": "2025-08-02T22:42:19+00:00",
      "updated": "2025-08-02T22:42:19+00:00",
      "arxiv_id": "2508.01511v1",
      "url": "http://arxiv.org/pdf/2508.01511v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "FlashSVD: Memory-Efficient Inference with Streaming for Low-Rank Models",
      "abstract": "Singular Value Decomposition (SVD) has recently seen a surge of interest as a\nsimple yet powerful tool for large language models (LLMs) compression, with a\ngrowing number of works demonstrating 20-80% parameter reductions at minimal\naccuracy loss. Previous SVD-based approaches have focused primarily on reducing\nthe memory footprint of model weights, largely overlooking the additional\nactivation memory overhead incurred during inference when applying truncated\nfactors via standard dense CUDA kernels. Our experiments demonstrate that this\nactivation overhead, scaling with sequence length and hidden dimension,\nprevents current SVD compression techniques from achieving any reduction in\npeak inference memory, thereby limiting their viability for real-world,\non-device deployments.\n  We introduce FlashSVD, a novel, end-to-end rank-aware streaming inference\nframework specifically designed for SVD-compressed large language models.\nFlashSVD can be seamlessly integrated with any model that employs SVD-based\nmethods for parameter reduction. By fusing low-rank projection kernels directly\ninto both the self-attention and feed-forward network (FFN) pipelines, FlashSVD\navoid materializing full-size activation buffers. Instead, small tiles of the\ntruncated factors are loaded into on-chip SRAM, multiplied and reduced on the\nfly, and immediately evicted, preserving high GPU occupancy and adding no extra\nlatency. On standard encoder benchmarks (e.g., BERT-Base), FlashSVD cuts peak\nactivation memory by up to 70.2% and intermediate transient memory by 75%, all\nwhile incur no accuracy loss with upstreaming compression methods, offering a\npractical path toward memory-constrained deployment of low-rank LLMs.",
      "authors": [
        "Zishan Shao",
        "Yixiao Wang",
        "Qinsi Wang",
        "Ting Jiang",
        "Zhixu Du",
        "Hancheng Ye",
        "Danyang Zhuo",
        "Yiran Chen",
        "Hai Li"
      ],
      "published": "2025-08-02T22:06:46+00:00",
      "updated": "2025-08-02T22:06:46+00:00",
      "arxiv_id": "2508.01506v1",
      "url": "http://arxiv.org/pdf/2508.01506v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents",
      "abstract": "Large language models (LLMs) present new opportunities for creating\npedagogical agents that engage in meaningful dialogue to support student\nlearning. However, the current use of LLM systems like ChatGPT in classrooms\noften lacks the solid theoretical foundation found in earlier intelligent\ntutoring systems. To bridge this gap, we propose a framework that combines\nEvidence-Centered Design with Social Cognitive Theory for adaptive scaffolding\nin LLM-based agents focused on STEM+C learning. We illustrate this framework\nwith Inquizzitor, an LLM-based formative assessment agent that integrates\nhuman-AI hybrid intelligence and provides feedback grounded in cognitive\nscience principles. Our findings show that Inquizzitor delivers high-quality\nassessment and interaction aligned with core learning theories, offering\nteachers effective guidance that students value. This research underscores the\npotential for theory-driven LLM integration in education, highlighting the\nability of these systems to provide adaptive and principled instruction.",
      "authors": [
        "Clayton Cohn",
        "Surya Rayala",
        "Namrata Srivastava",
        "Joyce Horn Fonteles",
        "Shruti Jain",
        "Xinying Luo",
        "Divya Mereddy",
        "Naveeduddin Mohammed",
        "Gautam Biswas"
      ],
      "published": "2025-08-02T21:58:32+00:00",
      "updated": "2025-08-02T21:58:32+00:00",
      "arxiv_id": "2508.01503v1",
      "url": "http://arxiv.org/pdf/2508.01503v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "The Homogenizing Effect of Large Language Models on Human Expression and Thought",
      "abstract": "Cognitive diversity, reflected in variations of language, perspective, and\nreasoning, is essential to creativity and collective intelligence. This\ndiversity is rich and grounded in culture, history, and individual experience.\nYet as large language models (LLMs) become deeply embedded in people's lives,\nthey risk standardizing language and reasoning. This Review synthesizes\nevidence across linguistics, cognitive, and computer science to show how LLMs\nreflect and reinforce dominant styles while marginalizing alternative voices\nand reasoning strategies. We examine how their design and widespread use\ncontribute to this effect by mirroring patterns in their training data and\namplifying convergence as all people increasingly rely on the same models\nacross contexts. Unchecked, this homogenization risks flattening the cognitive\nlandscapes that drive collective intelligence and adaptability.",
      "authors": [
        "Zhivar Sourati",
        "Alireza S. Ziabari",
        "Morteza Dehghani"
      ],
      "published": "2025-08-02T21:22:25+00:00",
      "updated": "2025-08-02T21:22:25+00:00",
      "arxiv_id": "2508.01491v1",
      "url": "http://arxiv.org/pdf/2508.01491v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach",
      "abstract": "Biomedical text mining and question-answering are essential yet highly\ndemanding tasks, particularly in the face of the exponential growth of\nbiomedical literature. In this work, we present our participation in the 13th\nedition of the BioASQ challenge, which involves biomedical semantic\nquestion-answering for Task 13b and biomedical question-answering for\ndeveloping topics for the Synergy task. We deploy a selection of open-source\nlarge language models (LLMs) as retrieval-augmented generators to answer\nbiomedical questions. Various models are used to process the questions. A\nmajority voting system combines their output to determine the final answer for\nYes/No questions, while for list and factoid type questions, the union of their\nanswers in used. We evaluated 13 state-of-the-art open source LLMs, exploring\nall possible model combinations to contribute to the final answer, resulting in\ntailored LLM pipelines for each question type. Our findings provide valuable\ninsight into which combinations of LLMs consistently produce superior results\nfor specific question types. In the four rounds of the 2025 BioASQ challenge,\nour system achieved notable results: in the Synergy task, we secured 1st place\nfor ideal answers and 2nd place for exact answers in round 2, as well as two\nshared 1st places for exact answers in round 3 and 4.",
      "authors": [
        "Dimitra Panou",
        "Alexandros C. Dimopoulos",
        "Manolis Koubarakis",
        "Martin Reczko"
      ],
      "published": "2025-08-02T20:20:08+00:00",
      "updated": "2025-08-02T20:20:08+00:00",
      "arxiv_id": "2508.01480v1",
      "url": "http://arxiv.org/pdf/2508.01480v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs",
      "abstract": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks.",
      "authors": [
        "Yiming Zeng",
        "Jinghan Cao",
        "Zexin Li",
        "Yiming Chen",
        "Tao Ren",
        "Dawei Xiang",
        "Xidong Wu",
        "Shangqian Gao",
        "Tingting Yu"
      ],
      "published": "2025-08-02T19:46:09+00:00",
      "updated": "2025-08-07T17:46:00+00:00",
      "arxiv_id": "2508.01473v2",
      "url": "http://arxiv.org/pdf/2508.01473v2",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "VWAttacker: A Systematic Security Testing Framework for Voice over WiFi User Equipments",
      "abstract": "We present VWAttacker, the first systematic testing framework for analyzing\nthe security of Voice over WiFi (VoWiFi) User Equipment (UE) implementations.\nVWAttacker includes a complete VoWiFi network testbed that communicates with\nCommercial-Off-The-Shelf (COTS) UEs based on a simple interface to test the\nbehavior of diverse VoWiFi UE implementations; uses property-guided adversarial\ntesting to uncover security issues in different UEs systematically. To reduce\nmanual effort in extracting and testing properties, we introduce an LLM-based,\nsemi-automatic, and scalable approach for property extraction and testcase (TC)\ngeneration. These TCs are systematically mutated by two domain-specific\ntransformations. Furthermore, we introduce two deterministic oracles to detect\nproperty violations automatically. Coupled with these techniques, VWAttacker\nextracts 63 properties from 11 specifications, evaluates 1,116 testcases, and\ndetects 13 issues in 21 UEs. The issues range from enforcing a DH shared secret\nto 0 to supporting weak algorithms. These issues result in attacks that expose\nthe victim UE's identity or establish weak channels, thus severely hampering\nthe security of cellular networks. We responsibly disclose the findings to all\nthe related vendors. At the time of writing, one of the vulnerabilities has\nbeen acknowledged by MediaTek with high severity.",
      "authors": [
        "Imtiaz Karim",
        "Hyunwoo Lee",
        "Hassan Asghar",
        "Kazi Samin Mubasshir",
        "Seulgi Han",
        "Mashroor Hasan Bhuiyan",
        "Elisa Bertino"
      ],
      "published": "2025-08-02T19:37:57+00:00",
      "updated": "2025-08-02T19:37:57+00:00",
      "arxiv_id": "2508.01469v1",
      "url": "http://arxiv.org/pdf/2508.01469v1",
      "categories": [
        "cs.CR",
        "cs.NI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Think Broad, Act Narrow: CWE Identification with Multi-Agent Large Language Models",
      "abstract": "Machine learning and Large language models (LLMs) for vulnerability detection\nhas received significant attention in recent years. Unfortunately,\nstate-of-the-art techniques show that LLMs are unsuccessful in even\ndistinguishing the vulnerable function from its benign counterpart, due to\nthree main problems: Vulnerability detection requires deep analysis, which LLMs\noften struggle with when making a one-shot prediction. Existing techniques\ntypically perform function-level analysis, whereas effective vulnerability\ndetection requires contextual information beyond the function scope. The focus\non binary classification can result in identifying a vulnerability but\nassociating it with the wrong security weaknesses (CWE), which may mislead\ndevelopers. We propose a novel multi-agent LLM approach to address the\nchallenges of identifying CWEs. This approach consists of three steps: (1) a\nteam of LLM agents performs an exhaustive search for potential CWEs in the\nfunction under review, (2) another team of agents identifies relevant external\ncontext to support or refute each candidate CWE, and (3) a final agent makes\ninformed acceptance or rejection decisions for each CWE based on the gathered\ncontext. A preliminary evaluation of our approach shows promising results. In\nthe PrimeVul dataset, Step 1 correctly identifies the appropriate CWE in 40.9\\%\nof the studied vulnerable functions. We further evaluated the full pipeline on\nten synthetic programs and found that incorporating context information\nsignificantly reduced false positives from 6 to 9 CWEs to just 1 to 2, while\nstill correctly identifying the true CWE in 9 out of 10 cases.",
      "authors": [
        "Mohammed Sayagh",
        "Mohammad Ghafari"
      ],
      "published": "2025-08-02T17:57:46+00:00",
      "updated": "2025-08-02T17:57:46+00:00",
      "arxiv_id": "2508.01451v1",
      "url": "http://arxiv.org/pdf/2508.01451v1",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data",
      "abstract": "Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language\nModels (LLMs) to specialized domains such as medical reasoning. However,\nexisting SFT practices often rely on unfiltered datasets that contain redundant\nand low-quality samples, leading to substantial computational costs and\nsuboptimal performance. Although existing methods attempt to alleviate this\nproblem by selecting data based on sample difficulty, defined by knowledge and\nreasoning complexity, they overlook each sample's optimization utility\nreflected in its gradient. Interestingly, we find that gradient-based influence\nalone favors easy-to-optimize samples that cause large parameter shifts but\nlack deep reasoning chains, while difficulty alone selects noisy or overly\ncomplex cases that fail to guide stable optimization. Based on this\nobservation, we propose a data selection strategy, Difficulty-Influence\nQuadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence\nquadrant to balance complex clinical reasoning with substantial gradient\ninfluence, enabling efficient medical reasoning with minimal fine-tuning data.\nFurthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected\nsubsets demonstrate higher data quality and generate clinical reasoning that is\nmore aligned with expert practices in differential diagnosis, safety check, and\nevidence citation, as DIQ emphasizes samples that foster expert-like reasoning\npatterns. Extensive experiments on medical reasoning benchmarks demonstrate\nthat DIQ enables models fine-tuned on only 1% of selected data to match\nfull-dataset performance, while using 10% consistently outperforms the\nbaseline, highlighting the superiority of principled data selection over\nbrute-force scaling. The code and data are available at\nhttps://github.com/mihara-bot/DIQ.",
      "authors": [
        "Xinlin Zhuang",
        "Feilong Tang",
        "Haolin Yang",
        "Ming Hu",
        "Huifa Li",
        "Haochen Xue",
        "Yichen Li",
        "Junjun He",
        "Zongyuan Ge",
        "Ying Qian",
        "Imran Razzak"
      ],
      "published": "2025-08-02T17:50:35+00:00",
      "updated": "2025-08-02T17:50:35+00:00",
      "arxiv_id": "2508.01450v1",
      "url": "http://arxiv.org/pdf/2508.01450v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Large Language Model-based Data Science Agent: A Survey",
      "abstract": "The rapid advancement of Large Language Models (LLMs) has driven novel\napplications across diverse domains, with LLM-based agents emerging as a\ncrucial area of exploration. This survey presents a comprehensive analysis of\nLLM-based agents designed for data science tasks, summarizing insights from\nrecent studies. From the agent perspective, we discuss the key design\nprinciples, covering agent roles, execution, knowledge, and reflection methods.\nFrom the data science perspective, we identify key processes for LLM-based\nagents, including data preprocessing, model development, evaluation,\nvisualization, etc. Our work offers two key contributions: (1) a comprehensive\nreview of recent developments in applying LLMbased agents to data science\ntasks; (2) a dual-perspective framework that connects general agent design\nprinciples with the practical workflows in data science.",
      "authors": [
        "Peiran Wang",
        "Yaoning Yu",
        "Ke Chen",
        "Xianyang Zhan",
        "Haohan Wang"
      ],
      "published": "2025-08-02T17:33:18+00:00",
      "updated": "2025-08-02T17:33:18+00:00",
      "arxiv_id": "2508.02744v1",
      "url": "http://arxiv.org/pdf/2508.02744v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective",
      "abstract": "There is a growing interest in leveraging large language models (LLMs) for\nautomated code optimization. However, industrial platforms deploying multiple\nLLMs face a critical challenge: prompts optimized for one LLM often fail with\nothers, requiring expensive model-specific prompt engineering. This cross-model\nprompt engineering bottleneck severely limits the practical deployment of\nmulti-LLM optimization systems in production environments. To address this, we\nintroduce Meta-Prompted Code Optimization (MPCO), a framework that\nautomatically generates high-quality, task-specific prompts across diverse LLMs\nwhile maintaining industrial efficiency requirements. MPCO leverages\nmeta-prompting to dynamically synthesize context-aware optimization prompts by\nintegrating project metadata, task requirements, and LLM-specific contexts, and\nit seamlessly deploys on the ARTEMIS industrial platform for automated\nvalidation and scaling.\n  Our comprehensive evaluation on five real-world codebases with 366 hours of\nruntime benchmarking demonstrates MPCO's effectiveness: it achieves overall\nperformance improvements up to 19.06% with the best statistical rank across all\nsystems compared to baseline methods. Analysis shows that 96% of the\ntop-performing optimizations stem from meaningful edits. Through systematic\nablation studies and meta-prompter sensitivity analysis, we identify that\ncomprehensive context integration is essential for effective meta-prompting,\nand that all three major LLMs can serve effectively as meta-prompters,\nproviding actionable insights for industrial practitioners.",
      "authors": [
        "Jingzhi Gong",
        "Rafail Giavrimis",
        "Paul Brookes",
        "Vardan Voskanyan",
        "Fan Wu",
        "Mari Ashiga",
        "Matthew Truscott",
        "Mike Basios",
        "Leslie Kanthan",
        "Jie Xu",
        "Zheng Wang"
      ],
      "published": "2025-08-02T17:11:40+00:00",
      "updated": "2025-08-02T17:11:40+00:00",
      "arxiv_id": "2508.01443v1",
      "url": "http://arxiv.org/pdf/2508.01443v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "TripTailor: A Real-World Benchmark for Personalized Travel Planning",
      "abstract": "The continuous evolution and enhanced reasoning capabilities of large\nlanguage models (LLMs) have elevated their role in complex tasks, notably in\ntravel planning, where demand for personalized, high-quality itineraries is\nrising. However, current benchmarks often rely on unrealistic simulated data,\nfailing to reflect the differences between LLM-generated and real-world\nitineraries. Existing evaluation metrics, which primarily emphasize\nconstraints, fall short of providing a comprehensive assessment of the overall\nquality of travel plans. To address these limitations, we introduce TripTailor,\na benchmark designed specifically for personalized travel planning in\nreal-world scenarios. This dataset features an extensive collection of over\n500,000 real-world points of interest (POIs) and nearly 4,000 diverse travel\nitineraries, complete with detailed information, providing a more authentic\nevaluation framework. Experiments show that fewer than 10\\% of the itineraries\ngenerated by the latest state-of-the-art LLMs achieve human-level performance.\nMoreover, we identify several critical challenges in travel planning, including\nthe feasibility, rationality, and personalized customization of the proposed\nsolutions. We hope that TripTailor will drive the development of travel\nplanning agents capable of understanding and meeting user needs while\ngenerating practical itineraries. Our code and dataset are available at\nhttps://github.com/swxkfm/TripTailor",
      "authors": [
        "Yuanzhe Shen",
        "Kaimin Wang",
        "Changze Lv",
        "Xiaoqing Zheng",
        "Xuanjing Huang"
      ],
      "published": "2025-08-02T16:44:02+00:00",
      "updated": "2025-08-02T16:44:02+00:00",
      "arxiv_id": "2508.01432v1",
      "url": "http://arxiv.org/pdf/2508.01432v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs",
      "abstract": "Large Language Models (LLMs), despite their success in question answering,\nexhibit limitations in complex multi-hop question answering (MQA) tasks that\nnecessitate non-linear, structured reasoning. This limitation stems from their\ninability to adequately capture deep conceptual relationships between entities.\nTo overcome this challenge, we present **ORACLE** (**O**ntology-driven\n**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a\ntraining-free framework that combines LLMs' generative capabilities with the\nstructural benefits of knowledge graphs. Our approach operates through three\nstages: (1) dynamic construction of question-specific knowledge ontologies\nusing LLMs, (2) transformation of these ontologies into First-Order Logic\nreasoning chains, and (3) systematic decomposition of the original query into\nlogically coherent sub-questions. Experimental results on several standard MQA\nbenchmarks show that our framework achieves highly competitive performance,\nrivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses\nfurther confirm the effectiveness of each component, while demonstrating that\nour method generates more logical and interpretable reasoning chains than\nexisting approaches.",
      "authors": [
        "Haonan Bian",
        "Yutao Qi",
        "Rui Yang",
        "Yuanxi Che",
        "Jiaqian Wang",
        "Heming Xia",
        "Ranran Zhen"
      ],
      "published": "2025-08-02T16:12:42+00:00",
      "updated": "2025-08-02T16:12:42+00:00",
      "arxiv_id": "2508.01424v1",
      "url": "http://arxiv.org/pdf/2508.01424v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Discovering Bias Associations through Open-Ended LLM Generations",
      "abstract": "Social biases embedded in Large Language Models (LLMs) raise critical\nconcerns, resulting in representational harms -- unfair or distorted portrayals\nof demographic groups -- that may be expressed in subtle ways through generated\nlanguage. Existing evaluation methods often depend on predefined\nidentity-concept associations, limiting their ability to surface new or\nunexpected forms of bias. In this work, we present the Bias Association\nDiscovery Framework (BADF), a systematic approach for extracting both known and\npreviously unrecognized associations between demographic identities and\ndescriptive concepts from open-ended LLM outputs. Through comprehensive\nexperiments spanning multiple models and diverse real-world contexts, BADF\nenables robust mapping and analysis of the varied concepts that characterize\ndemographic identities. Our findings advance the understanding of biases in\nopen-ended generation and provide a scalable tool for identifying and analyzing\nbias associations in LLMs. Data, code, and results are available at\nhttps://github.com/JP-25/Discover-Open-Ended-Generation",
      "authors": [
        "Jinhao Pan",
        "Chahat Raj",
        "Ziwei Zhu"
      ],
      "published": "2025-08-02T15:31:55+00:00",
      "updated": "2025-08-02T15:31:55+00:00",
      "arxiv_id": "2508.01412v1",
      "url": "http://arxiv.org/pdf/2508.01412v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations",
      "abstract": "ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,\nnovels, and TV show subtitles that are manually translated and aligned with\ntheir English counterparts. The dataset contains 25,557 segment pairs that can\nbe used to benchmark new machine translation models, fine-tune large language\nmodels in few-shot settings, and adapt commercial machine translation\napplications such as Google Translate. Additionally, the dataset is a valuable\nresource for research in various disciplines, including translation studies,\ncross-linguistic analysis, and lexical semantics. The dataset can also serve\npedagogical purposes by training translation students and aid professional\ntranslators as a translation memory. The contributions are twofold: first, the\ndataset features textual genres not found in existing parallel Egyptian Arabic\nand English datasets, and second, it is a gold-standard dataset that has been\ntranslated and aligned by human experts.",
      "authors": [
        "Rania Al-Sabbagh"
      ],
      "published": "2025-08-02T15:28:41+00:00",
      "updated": "2025-08-02T15:28:41+00:00",
      "arxiv_id": "2508.01411v1",
      "url": "http://arxiv.org/pdf/2508.01411v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ForenX: Towards Explainable AI-Generated Image Detection with Multimodal Large Language Models",
      "abstract": "Advances in generative models have led to AI-generated images visually\nindistinguishable from authentic ones. Despite numerous studies on detecting\nAI-generated images with classifiers, a gap persists between such methods and\nhuman cognitive forensic analysis. We present ForenX, a novel method that not\nonly identifies the authenticity of images but also provides explanations that\nresonate with human thoughts. ForenX employs the powerful multimodal large\nlanguage models (MLLMs) to analyze and interpret forensic cues. Furthermore, we\novercome the limitations of standard MLLMs in detecting forgeries by\nincorporating a specialized forensic prompt that directs the MLLMs attention to\nforgery-indicative attributes. This approach not only enhance the\ngeneralization of forgery detection but also empowers the MLLMs to provide\nexplanations that are accurate, relevant, and comprehensive. Additionally, we\nintroduce ForgReason, a dataset dedicated to descriptions of forgery evidences\nin AI-generated images. Curated through collaboration between an LLM-based\nagent and a team of human annotators, this process provides refined data that\nfurther enhances our model's performance. We demonstrate that even limited\nmanual annotations significantly improve explanation quality. We evaluate the\neffectiveness of ForenX on two major benchmarks. The model's explainability is\nverified by comprehensive subjective evaluations.",
      "authors": [
        "Chuangchuang Tan",
        "Jinglu Wang",
        "Xiang Ming",
        "Renshuai Tao",
        "Yunchao Wei",
        "Yao Zhao",
        "Yan Lu"
      ],
      "published": "2025-08-02T15:21:26+00:00",
      "updated": "2025-08-02T15:21:26+00:00",
      "arxiv_id": "2508.01402v1",
      "url": "http://arxiv.org/pdf/2508.01402v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Recognising, Anticipating, and Mitigating LLM Pollution of Online Behavioural Research",
      "abstract": "Online behavioural research faces an emerging threat as participants\nincreasingly turn to large language models (LLMs) for advice, translation, or\ntask delegation: LLM Pollution. We identify three interacting variants through\nwhich LLM Pollution threatens the validity and integrity of online behavioural\nresearch. First, Partial LLM Mediation occurs when participants make selective\nuse of LLMs for specific aspects of a task, such as translation or wording\nsupport, leading researchers to (mis)interpret LLM-shaped outputs as human\nones. Second, Full LLM Delegation arises when agentic LLMs complete studies\nwith little to no human oversight, undermining the central premise of\nhuman-subject research at a more foundational level. Third, LLM Spillover\nsignifies human participants altering their behaviour as they begin to\nanticipate LLM presence in online studies, even when none are involved. While\nPartial Mediation and Full Delegation form a continuum of increasing\nautomation, LLM Spillover reflects second-order reactivity effects. Together,\nthese variants interact and generate cascading distortions that compromise\nsample authenticity, introduce biases that are difficult to detect post hoc,\nand ultimately undermine the epistemic grounding of online research on human\ncognition and behaviour. Crucially, the threat of LLM Pollution is already\nco-evolving with advances in generative AI, creating an escalating\nmethodological arms race. To address this, we propose a multi-layered response\nspanning researcher practices, platform accountability, and community efforts.\nAs the challenge evolves, coordinated adaptation will be essential to safeguard\nmethodological integrity and preserve the validity of online behavioural\nresearch.",
      "authors": [
        "Raluca Rilla",
        "Tobias Werner",
        "Hiromu Yakura",
        "Iyad Rahwan",
        "Anne-Marie Nussberger"
      ],
      "published": "2025-08-02T14:40:54+00:00",
      "updated": "2025-08-02T14:40:54+00:00",
      "arxiv_id": "2508.01390v1",
      "url": "http://arxiv.org/pdf/2508.01390v1",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Prompt to Pwn: Automated Exploit Generation for Smart Contracts",
      "abstract": "We explore the feasibility of using LLMs for Automated Exploit Generation\n(AEG) against vulnerable smart contracts. We present \\textsc{ReX}, a framework\nintegrating LLM-based exploit synthesis with the Foundry testing suite,\nenabling the automated generation and validation of proof-of-concept (PoC)\nexploits. We evaluate five state-of-the-art LLMs (GPT-4.1, Gemini 2.5 Pro,\nClaude Opus 4, DeepSeek, and Qwen3 Plus) on both synthetic benchmarks and\nreal-world smart contracts affected by known high-impact exploits. Our results\nshow that modern LLMs can reliably generate functional PoC exploits for diverse\nvulnerability types, with success rates reaching up to 92\\%. Notably, Gemini\n2.5 Pro and GPT-4.1 consistently outperform others in both synthetic and\nreal-world scenarios. We further analyze factors influencing AEG effectiveness,\nincluding model capabilities, contract structure, and vulnerability types. We\nalso collect the first curated dataset of real-world PoC exploits to support\nfuture research.",
      "authors": [
        "Zeke Xiao",
        "Yuekang Li",
        "Qin Wang",
        "Shiping Chen"
      ],
      "published": "2025-08-02T13:52:15+00:00",
      "updated": "2025-08-02T13:52:15+00:00",
      "arxiv_id": "2508.01371v1",
      "url": "http://arxiv.org/pdf/2508.01371v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis",
      "abstract": "We present an autonomous framework that leverages Large Language Models\n(LLMs) to automate end-to-end business analysis and market report generation.\nAt its core, the system employs specialized agents - Researcher, Reviewer,\nWriter, and Retriever - that collaborate to analyze data and produce\ncomprehensive reports. These agents learn from real professional consultants'\npresentation materials at Amazon through in-context learning to replicate\nprofessional analytical methodologies. The framework executes a multi-step\nprocess: querying databases, analyzing data, generating insights, creating\nvisualizations, and composing market reports. We also introduce a novel\nLLM-based evaluation system for assessing report quality, which shows alignment\nwith expert human evaluations. Building on these evaluations, we implement an\niterative improvement mechanism that optimizes report quality through automated\nreview cycles. Experimental results show that report quality can be improved by\nboth automated review cycles and consultants' unstructured knowledge. In\nexperimental validation, our framework generates detailed 6-page reports in 7\nminutes at a cost of approximately \\$1. Our work could be an important step to\nautomatically create affordable market insights.",
      "authors": [
        "Roman Koshkin",
        "Pengyu Dai",
        "Nozomi Fujikawa",
        "Masahito Togami",
        "Marco Visentini-Scarzanella"
      ],
      "published": "2025-08-02T13:49:15+00:00",
      "updated": "2025-08-02T13:49:15+00:00",
      "arxiv_id": "2508.01370v1",
      "url": "http://arxiv.org/pdf/2508.01370v1",
      "categories": [
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models",
      "abstract": "Backdoor attacks pose a significant threat to Large Language Models (LLMs),\nwhere adversaries can embed hidden triggers to manipulate LLM's outputs. Most\nexisting defense methods, primarily designed for classification tasks, are\nineffective against the autoregressive nature and vast output space of LLMs,\nthereby suffering from poor performance and high latency. To address these\nlimitations, we investigate the behavioral discrepancies between benign and\nbackdoored LLMs in output space. We identify a critical phenomenon which we\nterm sequence lock: a backdoored model generates the target sequence with\nabnormally high and consistent confidence compared to benign generation.\nBuilding on this insight, we propose ConfGuard, a lightweight and effective\ndetection method that monitors a sliding window of token confidences to\nidentify sequence lock. Extensive experiments demonstrate ConfGuard achieves a\nnear 100\\% true positive rate (TPR) and a negligible false positive rate (FPR)\nin the vast majority of cases. Crucially, the ConfGuard enables real-time\ndetection almost without additional latency, making it a practical backdoor\ndefense for real-world LLM deployments.",
      "authors": [
        "Zihan Wang",
        "Rui Zhang",
        "Hongwei Li",
        "Wenshu Fan",
        "Wenbo Jiang",
        "Qingchuan Zhao",
        "Guowen Xu"
      ],
      "published": "2025-08-02T13:38:04+00:00",
      "updated": "2025-08-05T08:37:30+00:00",
      "arxiv_id": "2508.01365v2",
      "url": "http://arxiv.org/pdf/2508.01365v2",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Who Gets Cited? Gender- and Majority-Bias in LLM-Driven Reference Selection",
      "abstract": "Large language models (LLMs) are rapidly being adopted as research\nassistants, particularly for literature review and reference recommendation,\nyet little is known about whether they introduce demographic bias into citation\nworkflows. This study systematically investigates gender bias in LLM-driven\nreference selection using controlled experiments with pseudonymous author\nnames. We evaluate several LLMs (GPT-4o, GPT-4o-mini, Claude Sonnet, and Claude\nHaiku) by varying gender composition within candidate reference pools and\nanalyzing selection patterns across fields. Our results reveal two forms of\nbias: a persistent preference for male-authored references and a majority-group\nbias that favors whichever gender is more prevalent in the candidate pool.\nThese biases are amplified in larger candidate pools and only modestly\nattenuated by prompt-based mitigation strategies. Field-level analysis\nindicates that bias magnitude varies across scientific domains, with social\nsciences showing the least bias. Our findings indicate that LLMs can reinforce\nor exacerbate existing gender imbalances in scholarly recognition. Effective\nmitigation strategies are needed to avoid perpetuating existing gender\ndisparities in scientific citation practices before integrating LLMs into\nhigh-stakes academic workflows.",
      "authors": [
        "Jiangen He"
      ],
      "published": "2025-08-02T13:27:32+00:00",
      "updated": "2025-08-02T13:27:32+00:00",
      "arxiv_id": "2508.02740v1",
      "url": "http://arxiv.org/pdf/2508.02740v1",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.DL"
    },
    {
      "title": "Kronos: A Foundation Model for the Language of Financial Markets",
      "abstract": "The success of large-scale pre-training paradigm, exemplified by Large\nLanguage Models (LLMs), has inspired the development of Time Series Foundation\nModels (TSFMs). However, their application to financial candlestick (K-line)\ndata remains limited, often underperforming non-pre-trained architectures.\nMoreover, existing TSFMs often overlook crucial downstream tasks such as\nvolatility prediction and synthetic data generation. To address these\nlimitations, we propose Kronos, a unified, scalable pre-training framework\ntailored to financial K-line modeling. Kronos introduces a specialized\ntokenizer that discretizes continuous market information into token sequences,\npreserving both price dynamics and trade activity patterns. We pre-train Kronos\nusing an autoregressive objective on a massive, multi-market corpus of over 12\nbillion K-line records from 45 global exchanges, enabling it to learn nuanced\ntemporal and cross-asset representations. Kronos excels in a zero-shot setting\nacross a diverse set of financial tasks. On benchmark datasets, Kronos boosts\nprice series forecasting RankIC by 93% over the leading TSFM and 87% over the\nbest non-pre-trained baseline. It also achieves a 9% lower MAE in volatility\nforecasting and a 22% improvement in generative fidelity for synthetic K-line\nsequences. These results establish Kronos as a robust, versatile foundation\nmodel for end-to-end financial time series analysis. Our pre-trained model is\npublicly available at https://github.com/shiyu-coder/Kronos.",
      "authors": [
        "Yu Shi",
        "Zongliang Fu",
        "Shuo Chen",
        "Bohan Zhao",
        "Wei Xu",
        "Changshui Zhang",
        "Jian Li"
      ],
      "published": "2025-08-02T13:15:59+00:00",
      "updated": "2025-08-02T13:15:59+00:00",
      "arxiv_id": "2508.02739v1",
      "url": "http://arxiv.org/pdf/2508.02739v1",
      "categories": [
        "q-fin.ST",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-fin.ST"
    },
    {
      "title": "HyClone: Bridging LLM Understanding and Dynamic Execution for Semantic Code Clone Detection",
      "abstract": "Code clone detection is a critical task in software engineering, aimed at\nidentifying duplicated or similar code fragments within or across software\nsystems. Traditional methods often fail to capture functional equivalence,\nparticularly for semantic clones (Type 4), where code fragments implement\nidentical functionality despite differing syntactic structures. Recent advances\nin large language models (LLMs) have shown promise in understanding code\nsemantics. However, directly applying LLMs to code clone detection yields\nsuboptimal results due to their sensitivity to syntactic differences. To\naddress these challenges, we propose a novel two-stage framework that combines\nLLM-based screening with execution-based validation for detecting semantic\nclones in Python programs. In the first stage, an LLM evaluates code pairs to\nfilter out obvious non-clones based on semantic analysis. For pairs not\nidentified as clones, the second stage employs an execution-based validation\napproach, utilizing LLM-generated test inputs to assess functional equivalence\nthrough cross-execution validation. Our experimental evaluation demonstrates\nsignificant improvements in precision, recall, and F1-score compared to direct\nLLM-based detection, highlighting the framework's effectiveness in identifying\nsemantic clones. Future work includes exploring cross-language clone detection\nand optimizing the framework for large-scale applications.",
      "authors": [
        "Yunhao Liang",
        "Ruixuan Ying",
        "Takuya Taniguchi",
        "Guwen Lyu",
        "Zhe Cui"
      ],
      "published": "2025-08-02T13:11:56+00:00",
      "updated": "2025-08-02T13:11:56+00:00",
      "arxiv_id": "2508.01357v1",
      "url": "http://arxiv.org/pdf/2508.01357v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "NATLM: Detecting Defects in NFT Smart Contracts Leveraging LLM",
      "abstract": "Security issues are becoming increasingly significant with the rapid\nevolution of Non-fungible Tokens (NFTs). As NFTs are traded as digital assets,\nthey have emerged as prime targets for cyber attackers. In the development of\nNFT smart contracts, there may exist undiscovered defects that could lead to\nsubstantial financial losses if exploited. To tackle this issue, this paper\npresents a framework called NATLM(NFT Assistant LLM), designed to detect\npotential defects in NFT smart contracts. The framework effectively identifies\nfour common types of vulnerabilities in NFT smart contracts: ERC-721\nReentrancy, Public Burn, Risky Mutable Proxy, and Unlimited Minting. Relying\nexclusively on large language models (LLMs) for defect detection can lead to a\nhigh false-positive rate. To enhance detection performance, NATLM integrates\nstatic analysis with LLMs, specifically Gemini Pro 1.5. Initially, NATLM\nemploys static analysis to extract structural, syntactic, and execution flow\ninformation from the code, represented through Abstract Syntax Trees (AST) and\nControl Flow Graphs (CFG). These extracted features are then combined with\nvectors of known defect examples to create a matrix for input into the\nknowledge base. Subsequently, the feature vectors and code vectors of the\nanalyzed contract are compared with the contents of the knowledge base.\nFinally, the LLM performs deep semantic analysis to enhance detection\ncapabilities, providing a more comprehensive and accurate identification of\npotential security issues. Experimental results indicate that NATLM analyzed\n8,672 collected NFT smart contracts, achieving an overall precision of 87.72%,\na recall of 89.58%, and an F1 score of 88.94%. The results outperform other\nbaseline experiments, successfully identifying four common types of defects.",
      "authors": [
        "Yuanzheng Niu",
        "Xiaoqi Li",
        "Wenkai Li"
      ],
      "published": "2025-08-02T12:56:27+00:00",
      "updated": "2025-08-02T12:56:27+00:00",
      "arxiv_id": "2508.01351v1",
      "url": "http://arxiv.org/pdf/2508.01351v1",
      "categories": [
        "cs.CR"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "BlockA2A: Towards Secure and Verifiable Agent-to-Agent Interoperability",
      "abstract": "The rapid adoption of agentic AI, powered by large language models (LLMs), is\ntransforming enterprise ecosystems with autonomous agents that execute complex\nworkflows. Yet we observe several key security vulnerabilities in LLM-driven\nmulti-agent systems (MASes): fragmented identity frameworks, insecure\ncommunication channels, and inadequate defenses against Byzantine agents or\nadversarial prompts. In this paper, we present the first systematic analysis of\nthese emerging multi-agent risks and explain why the legacy security strategies\ncannot effectively address these risks. Afterwards, we propose BlockA2A, the\nfirst unified multi-agent trust framework that enables secure and verifiable\nand agent-to-agent interoperability. At a high level, BlockA2A adopts\ndecentralized identifiers (DIDs) to enable fine-grained cross-domain agent\nauthentication, blockchain-anchored ledgers to enable immutable auditability,\nand smart contracts to dynamically enforce context-aware access control\npolicies. BlockA2A eliminates centralized trust bottlenecks, ensures message\nauthenticity and execution integrity, and guarantees accountability across\nagent interactions. Furthermore, we propose a Defense Orchestration Engine\n(DOE) that actively neutralizes attacks through real-time mechanisms, including\nByzantine agent flagging, reactive execution halting, and instant permission\nrevocation. Empirical evaluations demonstrate BlockA2A's effectiveness in\nneutralizing prompt-based, communication-based, behavioral and systemic MAS\nattacks. We formalize its integration into existing MAS and showcase a\npractical implementation for Google's A2A protocol. Experiments confirm that\nBlockA2A and DOE operate with sub-second overhead, enabling scalable deployment\nin production LLM-based MAS environments.",
      "authors": [
        "Zhenhua Zou",
        "Zhuotao Liu",
        "Lepeng Zhao",
        "Qiuyang Zhan"
      ],
      "published": "2025-08-02T11:59:21+00:00",
      "updated": "2025-08-05T09:20:53+00:00",
      "arxiv_id": "2508.01332v2",
      "url": "http://arxiv.org/pdf/2508.01332v2",
      "categories": [
        "cs.CR",
        "cs.AI",
        "68T42 (Primary), 94A60 (Secondary)",
        "I.2.11; E.3"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "NatureGAIA: Pushing the Frontiers of GUI Agents with a Challenging Benchmark and High-Quality Trajectory Dataset",
      "abstract": "The rapid advancement of Large Language Model (LLM)-driven Graphical User\nInterface (GUI) agents is significantly hampered by the profound limitations of\nexisting evaluation benchmarks in terms of accuracy, reproducibility, and\nscalability. To address this critical gap, we introduce NaturalGAIA, a novel\nbenchmark engineered on the principle of Causal Pathways. This design paradigm\nstructures complex tasks into a series of programmatically verifiable atomic\nsteps, ensuring a rigorous, fully automated, and reproducible standard for\nassessment. Concurrently, to mitigate the inherent capability deficits of\nagents, we developed LightManus, a hierarchical agent architecture specifically\noptimized for long-horizon tasks. We leveraged this agent to generate a\nhigh-quality, human-verified trajectory dataset that uniquely captures diverse\nand even self-correcting interaction patterns of LLMs. We then utilized this\ndataset to perform Reinforcement Fine-Tuning (RFT) on the Qwen2.5-VL-7B model.\nOur experiments reveal that NaturalGAIA presents a formidable challenge to\ncurrent state-of-the-art LLMs; even the top-performing Claude-sonnet-4 achieved\na Weighted Pathway Success Rate (WPSR) of only 34.6%. Moreover, while RFT\nsubstantially improved the smaller model's GUI execution capabilities (WPSR\nincreased from 3.3% to 10.8%), its performance degraded sharply when handling\ncomplex scenarios. This outcome highlights the inherent capability ceiling of\nsmaller models when faced with comprehensive tasks that integrate perception,\ndecision-making, and execution. This research contributes a rigorous evaluation\nstandard and a high-quality dataset to the community, aiming to guide the\nfuture development of GUI agents.",
      "authors": [
        "Zihan Zheng",
        "Tianle Cui",
        "Chuwen Xie",
        "Jiahui Zhang",
        "Jiahui Pan",
        "Lewei He",
        "Qianglong Chen"
      ],
      "published": "2025-08-02T11:53:41+00:00",
      "updated": "2025-08-07T09:42:28+00:00",
      "arxiv_id": "2508.01330v2",
      "url": "http://arxiv.org/pdf/2508.01330v2",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Large-Scale Diverse Synthesis for Mid-Training",
      "abstract": "The scarcity of high-quality, knowledge-intensive training data hinders the\ndevelopment of large language models (LLMs), as traditional corpora provide\nlimited information. Previous studies have synthesized and integrated\ncorpora-dependent question-answering (QA) data to improve model performance but\nface challenges in QA data scalability and knowledge diversity, particularly in\ncross-domain contexts. Furthermore, leveraging our designed discipline and\ndifficulty annotation system, we probe model deficiencies in STEM disciplines\nand high-difficulty data. To overcome these limitations, we propose a novel\ndiversified pipeline to synthesize BoostQA, a 100B-token large-scale QA\ndataset. Our synthesis framework: (1) curates seed data from heterogeneous\nsources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade\nsynthesis to boost data diversity and high-difficulty synthesis to mitigate\ndifficulty degradation; (3) refines answers via DeepSeek-V3 to improve output\nquality. We utilize BoostQA in mid-training, a mid-stage between pre-training\nand post-training, to optimize domain-specific knowledge acquisition and\nenhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token\ndataset, to achieve an average improvement of $\\mathbf{12.74\\%}$ on MMLU and\nCMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also\ndemonstrates robust scalability, with performance consistently improving as\nmodel size, data volume, and initial FLOPs scale.",
      "authors": [
        "Xuemiao Zhang",
        "Chengying Tu",
        "Can Ren",
        "Rongxiang Weng",
        "Hongfei Yan",
        "Jingang Wang",
        "Xunliang Cai"
      ],
      "published": "2025-08-02T11:37:16+00:00",
      "updated": "2025-08-02T11:37:16+00:00",
      "arxiv_id": "2508.01326v1",
      "url": "http://arxiv.org/pdf/2508.01326v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Towards Evaluation for Real-World LLM Unlearning",
      "abstract": "This paper analyzes the limitations of existing unlearning evaluation metrics\nin terms of practicality, exactness, and robustness in real-world LLM\nunlearning scenarios. To overcome these limitations, we propose a new metric\ncalled Distribution Correction-based Unlearning Evaluation (DCUE). It\nidentifies core tokens and corrects distributional biases in their confidence\nscores using a validation set. The evaluation results are quantified using the\nKolmogorov-Smirnov test. Experimental results demonstrate that DCUE overcomes\nthe limitations of existing metrics, which also guides the design of more\npractical and reliable unlearning algorithms in the future.",
      "authors": [
        "Ke Miao",
        "Yuke Hu",
        "Xiaochen Li",
        "Wenjie Bao",
        "Zhihao Liu",
        "Zhan Qin",
        "Kui Ren"
      ],
      "published": "2025-08-02T11:32:41+00:00",
      "updated": "2025-08-02T11:32:41+00:00",
      "arxiv_id": "2508.01324v1",
      "url": "http://arxiv.org/pdf/2508.01324v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "AffectGPT-R1: Leveraging Reinforcement Learning for Open-Vocabulary Emotion Recognition",
      "abstract": "Open-Vocabulary Multimodal Emotion Recognition (OV-MER) aims to predict\nemotions without being constrained by predefined label spaces, enabling\nfine-grained and human-like emotion understanding. Unlike traditional\ndiscriminative methods, OV-MER leverages generative models, such as large\nlanguage models (LLMs) with extensive vocabularies, to capture the full\nspectrum of emotions. Previous approaches (like AffectGPT) primarily rely on\ntoken-level loss for training. However, this objective does not align with the\nemotion wheel (EW)-based evaluation metrics used in OV-MER. Unfortunately,\nEW-based metrics cannot be directly optimized via gradient backpropagation. In\nthis paper, we propose AffectGPT-R1, a reinforcement learning framework that\ndirectly optimizes performance on EW-based metrics. Specifically, we treat\nthese metrics as the reward function and employ Group Relative Policy\nOptimization (GRPO) to maximize rewards. Experimental results demonstrate that\nAffectGPT-R1 achieves significant improvements on OV-MER. We hope this work\nadvances the field of multimodal emotion recognition. Our code will be publicly\navailable at:https://github.com/zeroQiaoba/AffectGPT.",
      "authors": [
        "Zheng Lian"
      ],
      "published": "2025-08-02T11:16:47+00:00",
      "updated": "2025-08-02T11:16:47+00:00",
      "arxiv_id": "2508.01318v1",
      "url": "http://arxiv.org/pdf/2508.01318v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points",
      "abstract": "The advancement of large language models (LLMs) struggles with the scarcity\nof high-quality, diverse training data. To address this limitation, we propose\nLinkSyn, a novel knowledge point (KP) graph-based synthesis framework that\nenables flexible control over discipline and difficulty distributions while\nbalancing KP coverage and popularity. LinkSyn extracts KPs from\nquestion-answering (QA) seed data and constructs a KP graph to synthesize\ndiverse QA data from multiple seeds strongly linked by KPs and sampled from\ngraph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution\nvalue function to guide the adjustment of path sampling probability and balance\nKP coverage and popularity during graph walks; (2) diffusion-based synthesis\nvia DeepSeek-R1 by leveraging multiple seeds with dense logical associations\nalong each path; and (3) high-difficulty QA enhancement within given\ndisciplines by flexible difficulty adjustments. By executing LinkSyn, we\nsynthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.\nExtensive experiments on Llama-3 8B demonstrate that continual pre-training\nwith LinkQA yields an average improvement of $\\mathbf{11.51\\%}$ on MMLU and\nCMMLU, establishing new SOTA results. LinkQA consistently enhances performance\nacross model size and initial FLOPs scales.",
      "authors": [
        "Xuemiao Zhang",
        "Can Ren",
        "Chengying Tu",
        "Rongxiang Weng",
        "Hongfei Yan",
        "Jingang Wang",
        "Xunliang Cai"
      ],
      "published": "2025-08-02T11:09:06+00:00",
      "updated": "2025-08-06T06:44:57+00:00",
      "arxiv_id": "2508.01317v2",
      "url": "http://arxiv.org/pdf/2508.01317v2",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation",
      "abstract": "The scarcity and high cost of high-quality question-answering (QA) datasets\nhinder supervised fine-tuning (SFT) for domain-specific large language models\n(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that\nutilizes LLMs and prompt engineering to produce diverse, high-quality QA\ndatasets from arbitrary textual sources. D-SCoRE integrates\n$\\textbf{D}$ocument-centric processing, $\\textbf{S}$egmentation, $\\textbf{Co}$T\n$\\textbf{R}$easoning, and structured $\\textbf{E}$xport to generate QA-COT\ndatasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,\nsuch as semantic role transformation, question type balancing, and\ncounterfactual materials, enhance diversity and relevance, overcoming\nlimitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA\ndatasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on\nSQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most\ndomains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual\nmaterials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade\nhardware. Its simplicity and scalability enable efficient QA generation and\nhigh-performance fine-tuning across domains.",
      "authors": [
        "Weibo Zhou",
        "Lingbo Li",
        "Shangsong Liang"
      ],
      "published": "2025-08-02T10:45:05+00:00",
      "updated": "2025-08-02T10:45:05+00:00",
      "arxiv_id": "2508.01309v1",
      "url": "http://arxiv.org/pdf/2508.01309v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "PUZZLED: Jailbreaking LLMs through Word-Based Puzzles",
      "abstract": "As large language models (LLMs) are increasingly deployed across diverse\ndomains, ensuring their safety has become a critical concern. In response,\nstudies on jailbreak attacks have been actively growing. Existing approaches\ntypically rely on iterative prompt engineering or semantic transformations of\nharmful instructions to evade detection. In this work, we introduce PUZZLED, a\nnovel jailbreak method that leverages the LLM's reasoning capabilities. It\nmasks keywords in a harmful instruction and presents them as word puzzles for\nthe LLM to solve. We design three puzzle types-word search, anagram, and\ncrossword-that are familiar to humans but cognitively demanding for LLMs. The\nmodel must solve the puzzle to uncover the masked words and then proceed to\ngenerate responses to the reconstructed harmful instruction. We evaluate\nPUZZLED on five state-of-the-art LLMs and observe a high average attack success\nrate (ASR) of 88.8%, specifically 96.5% on GPT-4.1 and 92.3% on Claude 3.7\nSonnet. PUZZLED is a simple yet powerful attack that transforms familiar\npuzzles into an effective jailbreak strategy by harnessing LLMs' reasoning\ncapabilities.",
      "authors": [
        "Yelim Ahn",
        "Jaejin Lee"
      ],
      "published": "2025-08-02T10:36:01+00:00",
      "updated": "2025-08-02T10:36:01+00:00",
      "arxiv_id": "2508.01306v1",
      "url": "http://arxiv.org/pdf/2508.01306v1",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference",
      "abstract": "Knowledge editing aims to modify outdated knowledge in large language models\n(LLMs) efficiently while retaining their powerful capabilities. Most existing\nmethods rely on either parameter-level editing or retrieval-based approaches.\nIn this work, we propose Knowledge Editing alignment with Diverse Augmentation\nand Self-adaptive inference (KEDAS) to better align LLMs with knowledge\nediting. In the alignment phase, LLMs learn to apply in-context edited\nknowledge via low-rank adaptation. During editing, we design a diverse edit\naugmentation technique to improve the recall of edits. After that, a\nself-adaptive post-alignment inference mechanism is proposed, in which a\nfilter-based smart retriever is employed to perform a dynamic selection of\ninference routing. Specifically, irrelevant queries will go through the\noriginal pre-alignment model directly, while relevant ones, together with their\nrelated edits, go through the model with aligned adapters activated. In\nexperiments, KEDAS secures the highest overall performance scores in 35 out of\n36 cases across four datasets with three LLMs on three settings, surpassing its\nstrong knowledge editing alignment counterpart by about 19.8 harmonic mean\nscores of edit success, locality and portability and outperforming both\nparameter editing and retrieval-based baselines significantly. Analysis of\ncomputational cost and performance on general tasks further validates the\nrobustness and efficiency of KEDAS, indicating that it presents an ideal\nparadigm of knowledge editing alignment.",
      "authors": [
        "Chenming Tang",
        "Yutong Yang",
        "Yunfang Wu"
      ],
      "published": "2025-08-02T10:25:36+00:00",
      "updated": "2025-08-02T10:25:36+00:00",
      "arxiv_id": "2508.01302v1",
      "url": "http://arxiv.org/pdf/2508.01302v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "How Far Are LLMs from Symbolic Planners? An NLP-Based Perspective",
      "abstract": "The reasoning and planning abilities of Large Language Models (LLMs) have\nbeen a frequent topic of discussion in recent years. Their ability to take\nunstructured planning problems as input has made LLMs' integration into AI\nplanning an area of interest. Nevertheless, LLMs are still not reliable as\nplanners, with the generated plans often containing mistaken or hallucinated\nactions. Existing benchmarking and evaluation methods investigate planning with\nLLMs, focusing primarily on success rate as a quality indicator in various\nplanning tasks, such as validating plans or planning in relaxed conditions. In\nthis paper, we approach planning with LLMs as a natural language processing\n(NLP) task, given that LLMs are NLP models themselves. We propose a recovery\npipeline consisting of an NLP-based evaluation of the generated plans, along\nwith three stages to recover the plans through NLP manipulation of the\nLLM-generated plans, and eventually complete the plan using a symbolic planner.\nThis pipeline provides a holistic analysis of LLM capabilities in the context\nof AI task planning, enabling a broader understanding of the quality of invalid\nplans. Our findings reveal no clear evidence of underlying reasoning during\nplan generation, and that a pipeline comprising an NLP-based analysis of the\nplans, followed by a recovery mechanism, still falls short of the quality and\nreliability of classical planners. On average, only the first 2.65 actions of\nthe plan are executable, with the average length of symbolically generated\nplans being 8.4 actions. The pipeline still improves action quality and\nincreases the overall success rate from 21.9% to 27.5%.",
      "authors": [
        "Ma'ayan Armony",
        "Albert MeroÃ±o-PeÃ±uela",
        "Gerard Canal"
      ],
      "published": "2025-08-02T10:20:52+00:00",
      "updated": "2025-08-02T10:20:52+00:00",
      "arxiv_id": "2508.01300v1",
      "url": "http://arxiv.org/pdf/2508.01300v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification",
      "abstract": "Multiple Instance Learning (MIL) is the leading approach for whole slide\nimage (WSI) classification, enabling efficient analysis of gigapixel pathology\nslides. Recent work has introduced vision-language models (VLMs) into MIL\npipelines to incorporate medical knowledge through text-based class\ndescriptions rather than simple class names. However, when these methods rely\non large language models (LLMs) to generate clinical descriptions or use\nfixed-length prompts to represent complex pathology concepts, the limited token\ncapacity of VLMs often constrains the expressiveness and richness of the\nencoded class information. Additionally, descriptions generated solely by LLMs\nmay lack domain grounding and fine-grained medical specificity, leading to\nsuboptimal alignment with visual features. To address these challenges, we\npropose a vision-language MIL framework with two key contributions: (1) A\ngrounded multi-agent description generation system that leverages curated\npathology textbooks and agent specialization (e.g., morphology, spatial\ncontext) to produce accurate and diverse clinical descriptions; (2) A text\nencoding strategy using a list of descriptions rather than a single prompt,\ncapturing fine-grained and complementary clinical signals for better alignment\nwith visual features. Integrated into a VLM-MIL pipeline, our approach shows\nimproved performance over single-prompt class baselines and achieves results\ncomparable to state-of-the-art models, as demonstrated on renal and lung cancer\ndatasets.",
      "authors": [
        "Ngoc Bui Lam Quang",
        "Nam Le Nguyen Binh",
        "Thanh-Huy Nguyen",
        "Le Thien Phuc Nguyen",
        "Quan Nguyen",
        "Ulas Bagci"
      ],
      "published": "2025-08-02T09:59:39+00:00",
      "updated": "2025-08-02T09:59:39+00:00",
      "arxiv_id": "2508.01293v1",
      "url": "http://arxiv.org/pdf/2508.01293v1",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities",
      "abstract": "Retrieval-Augmented Generation (RAG) shows impressive performance by\nsupplementing and substituting parametric knowledge in Large Language Models\n(LLMs). Retrieved knowledge can be divided into three types: explicit answer\nevidence, implicit answer clue, and insufficient answer context which can be\nfurther categorized into totally irrelevant and partially relevant information.\nEffectively utilizing partially relevant knowledge remains a key challenge for\nRAG systems, especially in incomplete knowledge base retrieval. Contrary to the\nconventional view, we propose a new perspective: LLMs can be awakened via\npartially relevant knowledge already embedded in LLMs. To comprehensively\ninvestigate this phenomenon, the triplets located in the gold reasoning path\nand their variants are used to construct partially relevant knowledge by\nremoving the path that contains the answer. We provide theoretical analysis of\nthe awakening effect in LLMs and support our hypothesis with experiments on two\nKnowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we\npresent a new task, Unseen Entity KGQA, simulating real-world challenges where\nentity linking fails due to KG incompleteness. Our awakening-based approach\ndemonstrates greater efficacy in practical applications, outperforms\ntraditional methods that rely on embedding-based similarity which are prone to\nreturning noisy information.",
      "authors": [
        "Zhichao Yan",
        "Jiapu Wang",
        "Jiaoyan Chen",
        "Yanyan Wang",
        "Hongye Tan",
        "Jiye Liang",
        "Xiaoli Li",
        "Ru Li",
        "Jeff Z. Pan"
      ],
      "published": "2025-08-02T09:54:46+00:00",
      "updated": "2025-08-02T09:54:46+00:00",
      "arxiv_id": "2508.01290v1",
      "url": "http://arxiv.org/pdf/2508.01290v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ViseGPT: Towards Better Alignment of LLM-generated Data Wrangling Scripts and User Prompts",
      "abstract": "Large language models (LLMs) enable the rapid generation of data wrangling\nscripts based on natural language instructions, but these scripts may not fully\nadhere to user-specified requirements, necessitating careful inspection and\niterative refinement. Existing approaches primarily assist users in\nunderstanding script logic and spotting potential issues themselves, rather\nthan providing direct validation of correctness. To enhance debugging\nefficiency and optimize the user experience, we develop ViseGPT, a tool that\nautomatically extracts constraints from user prompts to generate comprehensive\ntest cases for verifying script reliability. The test results are then\ntransformed into a tailored Gantt chart, allowing users to intuitively assess\nalignment with semantic requirements and iteratively refine their scripts. Our\ndesign decisions are informed by a formative study (N=8) that explores user\npractices and challenges. We further evaluate the effectiveness and usability\nof ViseGPT through a user study (N=18). Results indicate that ViseGPT\nsignificantly improves debugging efficiency for LLM-generated data-wrangling\nscripts, enhances users' ability to detect and correct issues, and streamlines\nthe workflow experience.",
      "authors": [
        "Jiajun Zhu",
        "Xinyu Cheng",
        "Zhongsu Luo",
        "Yunfan Zhou",
        "Xinhuan Shu",
        "Di Weng",
        "Yingcai Wu"
      ],
      "published": "2025-08-02T09:19:16+00:00",
      "updated": "2025-08-02T09:19:16+00:00",
      "arxiv_id": "2508.01279v1",
      "url": "http://arxiv.org/pdf/2508.01279v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan",
      "abstract": "Multimodal Large Language Models (MLLMs) process visual, acoustic, and\ntextual inputs, addressing the limitations of single-modality LLMs. However,\nexisting benchmarks often overlook tri-modal evaluation in Traditional Chinese\nand do not consider inference latency. To address this, we introduce Multi-TW,\nthe first Traditional Chinese benchmark for evaluating the performance and\nlatency of any-to-any multimodal models. Multi-TW includes 900 multiple-choice\nquestions (image and text, audio and text pairs) sourced from official\nproficiency tests developed with the Steering Committee for the Test of\nProficiency-Huayu (SC-TOP). We evaluated various any-to-any models and\nvision-language models (VLMs) with audio transcription. Our results show that\nclosed-source models generally outperform open-source ones across modalities,\nalthough open-source models can perform well in audio tasks. End-to-end\nany-to-any pipelines offer clear latency advantages compared to VLMs using\nseparate audio transcription. Multi-TW presents a comprehensive view of model\ncapabilities and highlights the need for Traditional Chinese fine-tuning and\nefficient multimodal architectures.",
      "authors": [
        "Jui-Ming Yao",
        "Bing-Cheng Xie",
        "Sheng-Wei Peng",
        "Hao-Yuan Chen",
        "He-Rong Zheng",
        "Bing-Jia Tan",
        "Peter Shaojui Wang",
        "Shun-Feng Su"
      ],
      "published": "2025-08-02T09:10:15+00:00",
      "updated": "2025-08-02T09:10:15+00:00",
      "arxiv_id": "2508.01274v1",
      "url": "http://arxiv.org/pdf/2508.01274v1",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "KCR: Resolving Long-Context Knowledge Conflicts via Reasoning in LLMs",
      "abstract": "Knowledge conflicts commonly arise across diverse sources, and their\nprevalence has increased with the advent of LLMs. When dealing with conflicts\nbetween multiple contexts, also known as \\emph{inter-context knowledge\nconflicts}, LLMs are often confused by lengthy and conflicting contexts. To\naddress this challenge, we propose the Knowledge Conflict Reasoning (KCR)\nframework, which enhances the ability of LLMs to resolve conflicting knowledge.\nThe key idea of KCR is to train backbone LLMs to establish a correct reasoning\nprocess by rewarding them for selecting and adhering to the context with\nstronger logical consistency when presented with conflicting contexts.\nSpecifically, we first extract reasoning paths, represented by either text or\nlocal knowledge graphs, from the conflicting long contexts. Subsequently, we\nemploy Reinforcement Learning to encourage the model to learn the paradigm of\nreasoning process that follows correct reasoning paths rather than the\nincorrect counterparts. This enables the backbone models to genuinely acquire\nthe capability to resolve inter-context knowledge conflicts within long\ncontexts. Experimental results demonstrate that our framework significantly\nimproves the ability of various backbone models to resolve knowledge conflicts\nin long-context scenarios, yielding substantial performance gains.",
      "authors": [
        "Xianda Zheng",
        "Zijian Huang",
        "Meng-Fen Chiang",
        "Michael J. Witbrock",
        "Kaiqi Zhao"
      ],
      "published": "2025-08-02T09:09:50+00:00",
      "updated": "2025-08-05T11:26:20+00:00",
      "arxiv_id": "2508.01273v2",
      "url": "http://arxiv.org/pdf/2508.01273v2",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "PromptSafe: Gated Prompt Tuning for Safe Text-to-Image Generation",
      "abstract": "Text-to-image (T2I) models have demonstrated remarkable generative\ncapabilities but remain vulnerable to producing not-safe-for-work (NSFW)\ncontent, such as violent or explicit imagery. While recent moderation efforts\nhave introduced soft prompt-guided tuning by appending defensive tokens to the\ninput, these approaches often rely on large-scale curated image-text datasets\nand apply static, one-size-fits-all defenses at inference time. However, this\nresults not only in high computational cost and degraded benign image quality,\nbut also in limited adaptability to the diverse and nuanced safety requirements\nof real-world prompts. To address these challenges, we propose PromptSafe, a\ngated prompt tuning framework that combines a lightweight, text-only supervised\nsoft embedding with an inference-time gated control network. Instead of\ntraining on expensive image-text datasets, we first rewrite unsafe prompts into\nsemantically aligned but safe alternatives using an LLM, constructing an\nefficient text-only training corpus. Based on this, we optimize a universal\nsoft prompt that repels unsafe and attracts safe embeddings during the\ndiffusion denoising process. To avoid over-suppressing benign prompts, we\nintroduce a gated mechanism that adaptively adjusts the defensive strength\nbased on estimated prompt toxicity, thereby aligning defense intensity with\nprompt risk and ensuring strong protection for harmful inputs while preserving\nbenign generation quality. Extensive experiments across multiple benchmarks and\nT2I models show that PromptSafe achieves a SOTA unsafe generation rate (2.36%),\nwhile preserving high benign fidelity. Furthermore, PromptSafe demonstrates\nstrong generalization to unseen harmful categories, robust transferability\nacross diffusion model architectures, and resilience under adaptive adversarial\nattacks, highlighting its practical value for safe and scalable deployment.",
      "authors": [
        "Zonglei Jing",
        "Xiao Yang",
        "Xiaoqian Li",
        "Siyuan Liang",
        "Aishan Liu",
        "Mingchuan Zhang",
        "Xianglong Liu"
      ],
      "published": "2025-08-02T09:09:40+00:00",
      "updated": "2025-08-02T09:09:40+00:00",
      "arxiv_id": "2508.01272v1",
      "url": "http://arxiv.org/pdf/2508.01272v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Win-k: Improved Membership Inference Attacks on Small Language Models",
      "abstract": "Small language models (SLMs) are increasingly valued for their efficiency and\ndeployability in resource-constrained environments, making them useful for\non-device, privacy-sensitive, and edge computing applications. On the other\nhand, membership inference attacks (MIAs), which aim to determine whether a\ngiven sample was used in a model's training, are an important threat with\nserious privacy and intellectual property implications. In this paper, we study\nMIAs on SLMs. Although MIAs were shown to be effective on large language models\n(LLMs), they are relatively less studied on emerging SLMs, and furthermore,\ntheir effectiveness decreases as models get smaller. Motivated by this finding,\nwe propose a new MIA called win-k, which builds on top of a state-of-the-art\nattack (min-k). We experimentally evaluate win-k by comparing it with five\nexisting MIAs using three datasets and eight SLMs. Results show that win-k\noutperforms existing MIAs in terms of AUROC, TPR @ 1% FPR, and FPR @ 99% TPR\nmetrics, especially on smaller models.",
      "authors": [
        "Roya Arkhmammadova",
        "Hosein Madadi Tamar",
        "M. Emre Gursoy"
      ],
      "published": "2025-08-02T08:50:42+00:00",
      "updated": "2025-08-02T08:50:42+00:00",
      "arxiv_id": "2508.01268v1",
      "url": "http://arxiv.org/pdf/2508.01268v1",
      "categories": [
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025",
      "abstract": "The growing integration of Artificial Intelligence (AI) into education has\nintensified the need for transparency and interpretability. While hackathons\nhave long served as agile environments for rapid AI prototyping, few have\ndirectly addressed eXplainable AI (XAI) in real-world educational contexts.\nThis paper presents a comprehensive analysis of the XAI Challenge 2025, a\nhackathon-style competition jointly organized by Ho Chi Minh City University of\nTechnology (HCMUT) and the International Workshop on Trustworthiness and\nReliability in Neurosymbolic AI (TRNS-AI), held as part of the International\nJoint Conference on Neural Networks (IJCNN 2025). The challenge tasked\nparticipants with building Question-Answering (QA) systems capable of answering\nstudent queries about university policies while generating clear, logic-based\nnatural language explanations. To promote transparency and trustworthiness,\nsolutions were required to use lightweight Large Language Models (LLMs) or\nhybrid LLM-symbolic systems. A high-quality dataset was provided, constructed\nvia logic-based templates with Z3 validation and refined through expert student\nreview to ensure alignment with real-world academic scenarios. We describe the\nchallenge's motivation, structure, dataset construction, and evaluation\nprotocol. Situating the competition within the broader evolution of AI\nhackathons, we argue that it represents a novel effort to bridge LLMs and\nsymbolic reasoning in service of explainability. Our findings offer actionable\ninsights for future XAI-centered educational systems and competitive research\ninitiatives.",
      "authors": [
        "Long S. T. Nguyen",
        "Khang H. N. Vo",
        "Thu H. A. Nguyen",
        "Tuan C. Bui",
        "Duc Q. Nguyen",
        "Thanh-Tung Tran",
        "Anh D. Nguyen",
        "Minh L. Nguyen",
        "Fabien Baldacci",
        "Thang H. Bui",
        "Emanuel Di Nardo",
        "Angelo Ciaramella",
        "Son H. Le",
        "Ihsan Ullah",
        "Lorenzo Di Rocco",
        "Tho T. Quan"
      ],
      "published": "2025-08-02T08:46:06+00:00",
      "updated": "2025-08-02T08:46:06+00:00",
      "arxiv_id": "2508.01263v1",
      "url": "http://arxiv.org/pdf/2508.01263v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "TestWeaver: Execution-aware, Feedback-driven Regression Testing Generation with Large Language Models",
      "abstract": "Regression testing ensures that code changes do not unintentionally break\nexisting functionality. While recent advances in large language models (LLMs)\nhave shown promise in automating test generation for regression testing, they\noften suffer from limited reasoning about program execution, resulting in\nstagnated coverage growth - a phenomenon known as the coverage plateau. In this\npaper, we present TestWeaver, a novel LLM-based approach that integrates\nlightweight program analysis to guide test generation more effectively.\nTestWeaver introduces three key innovations: (1) it reduces hallucinations and\nimproves focus by supplying the LLM with the backward slice from the target\nline instead of full program context; (2) it identifies and incorporates close\ntest cases - those that share control-flow similarities with the path to the\ntarget line - to provide execution context within the LLM's context window; and\n(3) it enhances LLM's reasoning with execution in-line annotations that encode\nvariable states as comments along executed paths. By equipping LLMs with these\ntargeted and contextualized inputs, TestWeaver improves coverage-guided test\ngeneration and mitigates redundant explorations. Empirical results demonstrate\nthat TestWeaver accelerates code coverage growth and generates more effective\nregression test cases than existing LLM-based approaches.",
      "authors": [
        "Cuong Chi Le",
        "Cuong Duc Van",
        "Tung Duy Vu",
        "Thai Minh Pham Vu",
        "Hoang Nhat Phan",
        "Huy Nhat Phan",
        "Tien N. Nguyen"
      ],
      "published": "2025-08-02T08:13:02+00:00",
      "updated": "2025-08-02T08:13:02+00:00",
      "arxiv_id": "2508.01255v1",
      "url": "http://arxiv.org/pdf/2508.01255v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "ODOV: Towards Open-Domain Open-Vocabulary Object Detection",
      "abstract": "In this work, we handle a new problem of Open-Domain Open-Vocabulary (ODOV)\nobject detection, which considers the detection model's adaptability to the\nreal world including both domain and category shifts. For this problem, we\nfirst construct a new benchmark OD-LVIS, which includes 46,949 images, covers\n18 complex real-world domains and 1,203 categories, and provides a\ncomprehensive dataset for evaluating real-world object detection. Besides, we\ndevelop a novel baseline method for ODOV detection.The proposed method first\nleverages large language models to generate the domain-agnostic text prompts\nfor category embedding. It further learns the domain embedding from the given\nimage, which, during testing, can be integrated into the category embedding to\nform the customized domain-specific category embedding for each test image. We\nprovide sufficient benchmark evaluations for the proposed ODOV detection task\nand report the results, which verify the rationale of ODOV detection, the\nusefulness of our benchmark, and the superiority of the proposed method.",
      "authors": [
        "Yupeng Zhang",
        "Ruize Han",
        "Fangnan Zhou",
        "Song Wang",
        "Wei Feng",
        "Liang Wan"
      ],
      "published": "2025-08-02T08:10:45+00:00",
      "updated": "2025-08-02T08:10:45+00:00",
      "arxiv_id": "2508.01253v1",
      "url": "http://arxiv.org/pdf/2508.01253v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection",
      "abstract": "Large Language Model (LLM) agents offer a powerful new paradigm for solving\nvarious problems by combining natural language reasoning with the execution of\nexternal tools. However, their dynamic and non-transparent behavior introduces\ncritical security risks, particularly in the presence of prompt injection\nattacks. In this work, we propose a novel insight that treats the agent runtime\ntraces as structured programs with analyzable semantics. Thus, we present\nAgentArmor, a program analysis framework that converts agent traces into graph\nintermediate representation-based structured program dependency representations\n(e.g., CFG, DFG, and PDG) and enforces security policies via a type system.\nAgentArmor consists of three key components: (1) a graph constructor that\nreconstructs the agent's working traces as graph-based intermediate\nrepresentations with control flow and data flow described within; (2) a\nproperty registry that attaches security-relevant metadata of interacted tools\n& data, and (3) a type system that performs static inference and checking over\nthe intermediate representation. By representing agent behavior as structured\nprograms, AgentArmor enables program analysis over sensitive data flow, trust\nboundaries, and policy violations. We evaluate AgentArmor on the AgentDojo\nbenchmark, the results show that AgentArmor can achieve 95.75% of TPR, with\nonly 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect\nprompt injection vulnerabilities and enforce fine-grained security constraints.",
      "authors": [
        "Peiran Wang",
        "Yang Liu",
        "Yunfei Lu",
        "Yifeng Cai",
        "Hongbo Chen",
        "Qingyou Yang",
        "Jie Zhang",
        "Jue Hong",
        "Ye Wu"
      ],
      "published": "2025-08-02T07:59:34+00:00",
      "updated": "2025-08-02T07:59:34+00:00",
      "arxiv_id": "2508.01249v1",
      "url": "http://arxiv.org/pdf/2508.01249v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework",
      "abstract": "Large Language Models (LLMs) excel in solving mathematical problems, yet\ntheir performance is often limited by the availability of high-quality, diverse\ntraining data. Existing methods focus on augmenting datasets through rephrasing\nor difficulty progression but overlook the specific failure modes of LLMs. This\nresults in synthetic questions that the model can already solve, providing\nminimal performance gains. To address this, we propose WarriorMath, a\ndefect-aware framework for mathematical problem solving that integrates both\ntargeted data synthesis and progressive training. In the synthesis stage, we\nemploy multiple expert LLMs in a collaborative process to generate, critique,\nand refine problems. Questions that base LLMs fail to solve are identified and\niteratively improved through expert-level feedback, producing high-quality,\ndefect-aware training data. In the training stage, we introduce a progressive\nlearning framework that iteratively fine-tunes the model using increasingly\nchallenging data tailored to its weaknesses. Experiments on six mathematical\nbenchmarks show that WarriorMath outperforms strong baselines by 12.57% on\naverage, setting a new state-of-the-art. Our results demonstrate the\neffectiveness of a defect-aware, multi-expert framework for improving\nmathematical ability.",
      "authors": [
        "Yue Chen",
        "Minghua He",
        "Fangkai Yang",
        "Pu Zhao",
        "Lu Wang",
        "Yu Kang",
        "Yifei Dong",
        "Yuefeng Zhan",
        "Hao Sun",
        "Qingwei Lin",
        "Saravan Rajmohan",
        "Dongmei Zhang"
      ],
      "published": "2025-08-02T07:45:12+00:00",
      "updated": "2025-08-02T07:45:12+00:00",
      "arxiv_id": "2508.01245v1",
      "url": "http://arxiv.org/pdf/2508.01245v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "MeshLLM: Empowering Large Language Models to Progressively Understand and Generate 3D Mesh",
      "abstract": "We present MeshLLM, a novel framework that leverages large language models\n(LLMs) to understand and generate text-serialized 3D meshes. Our approach\naddresses key limitations in existing methods, including the limited dataset\nscale when catering to LLMs' token length and the loss of 3D structural\ninformation during mesh serialization. We introduce a Primitive-Mesh\ndecomposition strategy, which divides 3D meshes into structurally meaningful\nsubunits. This enables the creation of a large-scale dataset with 1500k+\nsamples, almost 50 times larger than previous methods, which aligns better with\nthe LLM scaling law principles. Furthermore, we propose inferring face\nconnectivity from vertices and local mesh assembly training strategies,\nsignificantly enhancing the LLMs' ability to capture mesh topology and spatial\nstructures. Experiments show that MeshLLM outperforms the state-of-the-art\nLLaMA-Mesh in both mesh generation quality and shape understanding,\nhighlighting its great potential in processing text-serialized 3D meshes.",
      "authors": [
        "Shuangkang Fang",
        "I-Chao Shen",
        "Yufeng Wang",
        "Yi-Hsuan Tsai",
        "Yi Yang",
        "Shuchang Zhou",
        "Wenrui Ding",
        "Takeo Igarashi",
        "Ming-Hsuan Yang"
      ],
      "published": "2025-08-02T07:37:37+00:00",
      "updated": "2025-08-05T05:55:00+00:00",
      "arxiv_id": "2508.01242v2",
      "url": "http://arxiv.org/pdf/2508.01242v2",
      "categories": [
        "cs.GR",
        "cs.CV"
      ],
      "primary_category": "cs.GR"
    },
    {
      "title": "NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration",
      "abstract": "Robotic telepresence enables users to navigate and experience remote\nenvironments. However, effective navigation and situational awareness depend on\nusers' prior knowledge of the environment, limiting the usefulness of these\nsystems for exploring unfamiliar places. We explore how integrating\nlocation-aware LLM-based narrative capabilities into a mobile robot can support\nremote exploration. We developed a prototype system, called NarraGuide, that\nprovides narrative guidance for users to explore and learn about a remote place\nthrough a dialogue-based interface. We deployed our prototype in a geology\nmuseum, where remote participants (n=20) used the robot to tour the museum. Our\nfindings reveal how users perceived the robot's role, engaged in dialogue in\nthe tour, and expressed preferences for bystander encountering. Our work\ndemonstrates the potential of LLM-enabled robotic capabilities to deliver\nlocation-aware narrative guidance and enrich the experience of exploring remote\nenvironments.",
      "authors": [
        "Yaxin Hu",
        "Arissa J. Sato",
        "Jingxin Du",
        "Chenming Ye",
        "Anjun Zhu",
        "Pragathi Praveena",
        "Bilge Mutlu"
      ],
      "published": "2025-08-02T07:22:08+00:00",
      "updated": "2025-08-02T07:22:08+00:00",
      "arxiv_id": "2508.01235v1",
      "url": "http://arxiv.org/pdf/2508.01235v1",
      "categories": [
        "cs.HC",
        "cs.RO",
        "68"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "WebDS: An End-to-End Benchmark for Web-based Data Science",
      "abstract": "A large portion of real-world data science tasks are complex and require\nmulti-hop web-based interactions: finding appropriate data available on the\ninternet, synthesizing real-time data of various modalities from different\nlocations, and producing summarized analyses. Existing web benchmarks often\nfocus on simplistic interactions, such as form submissions or e-commerce\ntransactions, and often do not require diverse tool-using capabilities required\nfor web based data science. Conversely, traditional data science benchmarks\ntypically concentrate on static, often textually bound datasets and do not\nassess end-to-end workflows that encompass data acquisition, cleaning,\nanalysis, and insight generation. In response, we introduce WebDS, the first\nend-to-end web-based data science benchmark. It comprises 870 web-based data\nscience tasks across 29 diverse websites from structured government data\nportals to unstructured news media, challenging agents to perform complex,\nmulti-step operations requiring the use of tools and heterogeneous data formats\nthat better reflect the realities of modern data analytics. Evaluations of\ncurrent SOTA LLM agents indicate significant performance gaps in accomplishing\nthese tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web\nVoyager, successfully completes only 15% of tasks in WebDS, which our analysis\nsuggests is due to new failure modes like poor information grounding,\nrepetitive behavior and shortcut-taking that agents performing WebDS' tasks\ndisplay. By providing a more robust and realistic testing ground, WebDS sets\nthe stage for significant advances in the development of practically useful\nLLM-based data science.",
      "authors": [
        "Ethan Hsu",
        "Hong Meng Yam",
        "Ines Bouissou",
        "Aaron Murali John",
        "Raj Thota",
        "Josh Koe",
        "Vivek Sarath Putta",
        "G K Dharesan",
        "Alexander Spangher",
        "Shikhar Murty",
        "Tenghao Huang",
        "Christopher D. Manning"
      ],
      "published": "2025-08-02T06:39:59+00:00",
      "updated": "2025-08-02T06:39:59+00:00",
      "arxiv_id": "2508.01222v1",
      "url": "http://arxiv.org/pdf/2508.01222v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Show or Tell? Modeling the evolution of request-making in Human-LLM conversations",
      "abstract": "Chat logs provide a rich source of information about LLM users, but patterns\nof user behavior are often masked by the variability of queries. We present a\nnew task, segmenting chat queries into contents of requests, roles,\nquery-specific context, and additional expressions. We find that, despite the\nfamiliarity of chat-based interaction, request-making in LLM queries remains\nsignificantly different from comparable human-human interactions. With the data\nresource, we introduce an important perspective of diachronic analyses with\nuser expressions. We find that query patterns vary between early ones\nemphasizing requests, and individual users explore patterns but tend to\nconverge with experience. Finally, we show that model capabilities affect user\nbehavior, particularly with the introduction of new models, which are traceable\nat the community level.",
      "authors": [
        "Shengqi Zhu",
        "Jeffrey M. Rzeszotarski",
        "David Mimno"
      ],
      "published": "2025-08-02T06:08:37+00:00",
      "updated": "2025-08-02T06:08:37+00:00",
      "arxiv_id": "2508.01213v1",
      "url": "http://arxiv.org/pdf/2508.01213v1",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Importance Sampling is All You Need: Predict LLM's performance on new benchmark by reusing existing benchmark",
      "abstract": "With the rapid advancement of large language models , code generation has\nbecome a key benchmark for evaluating LLM capabilities. However, existing\nbenchmarks face two major challenges: (1) the escalating cost of constructing\nhigh-quality test suites and reference solutions, and (2) the increasing risk\nof data contamination, which undermines the reliability of benchmark-based\nevaluations. In this paper, we propose BIS, a prompt-centric evaluation\nframework that enables ground-truth-free prediction of LLM performance on code\ngeneration tasks. Rather than executing generated code, BIS estimates\nperformance metrics by analyzing the prompt distribution alone. Built on\nimportance sampling theory and implemented using Importance Weighted\nAutoencoders, our method reweights samples from existing annotated benchmarks\nto estimate performance on new, unseen benchmarks. To stabilize the estimation,\nwe introduce weight truncation strategies and compute marginal expectations\nacross the fitted distributions. BIS serves as a complementary tool that\nsupports benchmark development and validation under constrained resources,\noffering actionable and quick feedback for prompt selection and contamination\nassessment. We conduct extensive experiments involving 8,000 evaluation points\nacross 4 CodeLlama models and 9 diverse benchmarks. Our framework achieves an\naverage absolute prediction error of 1.1% for code correctness scores, with\nbest- and worst-case errors of 0.3% and 1.9%, respectively. It also generalizes\nwell to other metrics, attaining average absolute errors of 2.15% for pass@1.\nThese results demonstrate the reliability and broad applicability of BIS, which\ncan significantly reduce the cost and effort of benchmarking LLMs in\ncode-related tasks.",
      "authors": [
        "Junjie Shi",
        "Wei Ma",
        "Shi Ying",
        "Lingxiao Jiang",
        "Yang liu",
        "Bo Du"
      ],
      "published": "2025-08-02T05:34:05+00:00",
      "updated": "2025-08-02T05:34:05+00:00",
      "arxiv_id": "2508.01203v1",
      "url": "http://arxiv.org/pdf/2508.01203v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Adaptive Content Restriction for Large Language Models via Suffix Optimization",
      "abstract": "Large Language Models (LLMs) have demonstrated significant success across\ndiverse applications. However, enforcing content restrictions remains a\nsignificant challenge due to their expansive output space. One aspect of\ncontent restriction is preventing LLMs from generating harmful content via\nmodel alignment approaches such as supervised fine-tuning (SFT). Yet, the need\nfor content restriction may vary significantly across user groups, change\nrapidly over time, and not always align with general definitions of\nharmfulness. Applying SFT to each of these specific use cases is impractical\ndue to the high computational, data, and storage demands. Motivated by this\nneed, we propose a new task called \\textit{Adaptive Content Restriction}\n(AdaCoRe), which focuses on lightweight strategies -- methods without model\nfine-tuning -- to prevent deployed LLMs from generating restricted terms for\nspecific use cases. We propose the first method for AdaCoRe, named\n\\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to\nany prompt to a) prevent a target LLM from generating a set of restricted\nterms, while b) preserving the output quality. To evaluate AdaCoRe approaches,\nincluding our SOP, we create a new \\textit{Content Restriction Benchmark}\n(CoReBench), which contains 400 prompts for 80 restricted terms across 8\ncarefully selected categories. We demonstrate the effectiveness of SOP on\nCoReBench, which outperforms the system-level baselines such as system suffix\nby 15\\%, 17\\%, 10\\%, 9\\%, and 6\\% on average restriction rates for Gemma2-2B,\nMistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also\ndemonstrate that SOP is effective on POE, an online platform hosting various\ncommercial LLMs, highlighting its practicality in real-world scenarios.",
      "authors": [
        "Yige Li",
        "Peihai Jiang",
        "Jun Sun",
        "Peng Shu",
        "Tianming Liu",
        "Zhen Xiang"
      ],
      "published": "2025-08-02T05:09:58+00:00",
      "updated": "2025-08-02T05:09:58+00:00",
      "arxiv_id": "2508.01198v1",
      "url": "http://arxiv.org/pdf/2508.01198v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
      "abstract": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.",
      "authors": [
        "Chengshuai Zhao",
        "Zhen Tan",
        "Pingchuan Ma",
        "Dawei Li",
        "Bohan Jiang",
        "Yancheng Wang",
        "Yingzhen Yang",
        "Huan Liu"
      ],
      "published": "2025-08-02T04:37:28+00:00",
      "updated": "2025-08-05T10:11:02+00:00",
      "arxiv_id": "2508.01191v2",
      "url": "http://arxiv.org/pdf/2508.01191v2",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "SpectrumWorld: Artificial Intelligence Foundation for Spectroscopy",
      "abstract": "Deep learning holds immense promise for spectroscopy, yet research and\nevaluation in this emerging field often lack standardized formulations. To\naddress this issue, we introduce SpectrumLab, a pioneering unified platform\ndesigned to systematize and accelerate deep learning research in spectroscopy.\nSpectrumLab integrates three core components: a comprehensive Python library\nfeaturing essential data processing and evaluation tools, along with\nleaderboards; an innovative SpectrumAnnotator module that generates\nhigh-quality benchmarks from limited seed data; and SpectrumBench, a\nmulti-layered benchmark suite covering 14 spectroscopic tasks and over 10\nspectrum types, featuring spectra curated from over 1.2 million distinct\nchemical substances. Thorough empirical studies on SpectrumBench with 18\ncutting-edge multimodal LLMs reveal critical limitations of current approaches.\nWe hope SpectrumLab will serve as a crucial foundation for future advancements\nin deep learning-driven spectroscopy.",
      "authors": [
        "Zhuo Yang",
        "Jiaqing Xie",
        "Shuaike Shen",
        "Daolang Wang",
        "Yeyun Chen",
        "Ben Gao",
        "Shuzhou Sun",
        "Biqing Qi",
        "Dongzhan Zhou",
        "Lei Bai",
        "Linjiang Chen",
        "Shufei Zhang",
        "Jun Jiang",
        "Tianfan Fu",
        "Yuqiang Li"
      ],
      "published": "2025-08-02T04:21:07+00:00",
      "updated": "2025-08-07T11:20:47+00:00",
      "arxiv_id": "2508.01188v3",
      "url": "http://arxiv.org/pdf/2508.01188v3",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "A Survey on Agent Workflow -- Status and Future",
      "abstract": "In the age of large language models (LLMs), autonomous agents have emerged as\na powerful paradigm for achieving general intelligence. These agents\ndynamically leverage tools, memory, and reasoning capabilities to accomplish\nuser-defined goals. As agent systems grow in complexity, agent\nworkflows-structured orchestration frameworks-have become central to enabling\nscalable, controllable, and secure AI behaviors. This survey provides a\ncomprehensive review of agent workflow systems, spanning academic frameworks\nand industrial implementations. We classify existing systems along two key\ndimensions: functional capabilities (e.g., planning, multi-agent collaboration,\nexternal API integration) and architectural features (e.g., agent roles,\norchestration flows, specification languages). By comparing over 20\nrepresentative systems, we highlight common patterns, potential technical\nchallenges, and emerging trends. We further address concerns related to\nworkflow optimization strategies and security. Finally, we outline open\nproblems such as standardization and multimodal integration, offering insights\nfor future research at the intersection of agent design, workflow\ninfrastructure, and safe automation.",
      "authors": [
        "Chaojia Yu",
        "Zihan Cheng",
        "Hanwen Cui",
        "Yishuo Gao",
        "Zexu Luo",
        "Yijin Wang",
        "Hangbin Zheng",
        "Yong Zhao"
      ],
      "published": "2025-08-02T04:15:30+00:00",
      "updated": "2025-08-02T04:15:30+00:00",
      "arxiv_id": "2508.01186v1",
      "url": "http://arxiv.org/pdf/2508.01186v1",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning",
      "abstract": "Despite their strong performance in multimodal emotion reasoning, existing\nMultimodal Large Language Models (MLLMs) often overlook the scenarios involving\nemotion conflicts, where emotional cues from different modalities are\ninconsistent. To fill this gap, we first introduce CA-MER, a new benchmark\ndesigned to examine MLLMs under realistic emotion conflicts. It consists of\nthree subsets: video-aligned, audio-aligned, and consistent, where only one or\nall modalities reflect the true emotion. However, evaluations on our CA-MER\nreveal that current state-of-the-art emotion MLLMs systematically over-rely on\naudio signal during emotion conflicts, neglecting critical cues from visual\nmodality. To mitigate this bias, we propose MoSEAR, a parameter-efficient\nframework that promotes balanced modality integration. MoSEAR consists of two\nmodules: (1)MoSE, modality-specific experts with a regularized gating mechanism\nthat reduces modality bias in the fine-tuning heads; and (2)AR, an attention\nreallocation mechanism that rebalances modality contributions in frozen\nbackbones during inference. Our framework offers two key advantages: it\nmitigates emotion conflicts and improves performance on consistent\nsamples-without incurring a trade-off between audio and visual modalities.\nExperiments on multiple benchmarks-including MER2023, EMER, DFEW, and our\nCA-MER-demonstrate that MoSEAR achieves state-of-the-art performance,\nparticularly under modality conflict conditions.",
      "authors": [
        "Zhiyuan Han",
        "Beier Zhu",
        "Yanlong Xu",
        "Peipei Song",
        "Xun Yang"
      ],
      "published": "2025-08-02T04:03:44+00:00",
      "updated": "2025-08-02T04:03:44+00:00",
      "arxiv_id": "2508.01181v1",
      "url": "http://arxiv.org/pdf/2508.01181v1",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD",
        "eess.AS",
        "68",
        "I.2.10"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Advancing the Foundation Model for Music Understanding",
      "abstract": "The field of Music Information Retrieval (MIR) is fragmented, with\nspecialized models excelling at isolated tasks. In this work, we challenge this\nparadigm by introducing a unified foundation model named MuFun for holistic\nmusic understanding. Our model features a novel architecture that jointly\nprocesses instrumental and lyrical content, and is trained on a large-scale\ndataset covering diverse tasks such as genre classification, music tagging, and\nquestion answering. To facilitate robust evaluation, we also propose a new\nbenchmark for multi-faceted music understanding called MuCUE (Music\nComprehensive Understanding Evaluation). Experiments show our model\nsignificantly outperforms existing audio large language models across the MuCUE\ntasks, demonstrating its state-of-the-art effectiveness and generalization\nability.",
      "authors": [
        "Yi Jiang",
        "Wei Wang",
        "Xianwen Guo",
        "Huiyun Liu",
        "Hanrui Wang",
        "Youri Xu",
        "Haoqi Gu",
        "Zhongqian Xie",
        "Chuanjiang Luo"
      ],
      "published": "2025-08-02T03:33:47+00:00",
      "updated": "2025-08-02T03:33:47+00:00",
      "arxiv_id": "2508.01178v1",
      "url": "http://arxiv.org/pdf/2508.01178v1",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.IR",
        "eess.AS"
      ],
      "primary_category": "cs.SD"
    },
    {
      "title": "RSPO: Risk-Seeking Policy Optimization for Pass@k and Max@k Metrics in Large Language Models",
      "abstract": "Current large language model post-training optimizes a risk-neutral objective\nthat maximizes expected reward, yet evaluation relies heavily on risk-seeking\nmetrics like Pass@k (at least one success in k trials) and Max@k (maximum\nreward across k responses). This mismatch in risk preferences can inevitably\nlead to suboptimal performance. To bridge this gap, we propose Risk-Seeking\nPolicy Optimization (RSPO), a novel method that directly targets Pass@k and\nMax@k during training. A key challenge in optimizing these metrics is the\n\"hitchhiking\" problem: low-reward responses are inadvertently reinforced if\nthey co-occur with a high-reward response within a sample of k generations,\nresulting in inefficient optimization. RSPO addresses this problem by\nleveraging the closed-form probability that a given response is the maximum\namong k samplings. Despite the complexity of nested gradients over multiple\nresponses, RSPO produces efficient, unbiased gradient estimators for both\nmetrics. We validate our approach with both rigorous theoretical analysis and\ncomprehensive experimental results.",
      "authors": [
        "Kaichen Zhang",
        "Shenghao Gao",
        "Yuzhong Hong",
        "Haipeng Sun",
        "Junwei Bao",
        "Hongfei Jiang",
        "Yang Song",
        "Hong Dingqian",
        "Hui Xiong"
      ],
      "published": "2025-08-02T03:25:26+00:00",
      "updated": "2025-08-02T03:25:26+00:00",
      "arxiv_id": "2508.01174v1",
      "url": "http://arxiv.org/pdf/2508.01174v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "WIP: Enhancing Game-Based Learning with AI-Driven Peer Agents",
      "abstract": "This work-in-progress paper presents SPARC (Systematic Problem Solving and\nAlgorithmic Reasoning for Children), a gamified learning platform designed to\nenhance engagement and knowledge retention in K-12 STEM education. Traditional\napproaches often struggle to motivate students or facilitate deep\nunderstanding, especially for complex scientific concepts. SPARC addresses\nthese challenges by integrating interactive, narrative-driven gameplay with an\nartificial intelligence peer agent built on large language models. Rather than\nsimply providing answers, the agent engages students in dialogue and inquiry,\nprompting them to explain concepts and solve problems collaboratively. The\nplatform's design is grounded in educational theory and closely aligned with\nstate learning standards. Initial classroom pilots utilized a multi-method\nassessment framework combining pre- and post-tests, in-game analytics, and\nqualitative feedback from students and teachers. Preliminary findings indicate\nthat SPARC significantly increases student engagement, with most participants\nreporting greater interest in STEM subjects and moderate gains in conceptual\nunderstanding observed in post-test results. Ongoing development focuses on\nrefining the AI agent, expanding curriculum integration, and improving\naccessibility. These early results demonstrate the potential of combining\nAI-driven peer support with game-based learning to create inclusive, effective,\nand engaging educational experiences for K-12 learners.",
      "authors": [
        "Chengzhang Zhu",
        "Cecile H. Sam",
        "Yanlai Wu",
        "Ying Tang"
      ],
      "published": "2025-08-02T03:11:13+00:00",
      "updated": "2025-08-02T03:11:13+00:00",
      "arxiv_id": "2508.01169v1",
      "url": "http://arxiv.org/pdf/2508.01169v1",
      "categories": [
        "cs.CY",
        "2020, 97U50"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented Conversational LLM-Based ASR",
      "abstract": "Automatic Speech Recognition (ASR) aims to convert human speech content into\ncorresponding text. In conversational scenarios, effectively utilizing context\ncan enhance its accuracy. Large Language Models' (LLMs) exceptional\nlong-context understanding and reasoning abilities enable LLM-based ASR\n(LLM-ASR) to leverage historical context for recognizing conversational speech,\nwhich has a high degree of contextual relevance. However, existing\nconversational LLM-ASR methods use a fixed number of preceding utterances or\nthe entire conversation history as context, resulting in significant ASR\nconfusion and computational costs due to massive irrelevant and redundant\ninformation. This paper proposes a multi-modal retrieval-and-selection method\nnamed MARS that augments conversational LLM-ASR by enabling it to retrieve and\nselect the most relevant acoustic and textual historical context for the\ncurrent utterance. Specifically, multi-modal retrieval obtains a set of\ncandidate historical contexts, each exhibiting high acoustic or textual\nsimilarity to the current utterance. Multi-modal selection calculates the\nacoustic and textual similarities for each retrieved candidate historical\ncontext and, by employing our proposed near-ideal ranking method to consider\nboth similarities, selects the best historical context. Evaluations on the\nInterspeech 2025 Multilingual Conversational Speech Language Model Challenge\ndataset show that the LLM-ASR, when trained on only 1.5K hours of data and\nequipped with the MARS, outperforms the state-of-the-art top-ranking system\ntrained on 179K hours of data.",
      "authors": [
        "Bingshen Mu",
        "Hexin Liu",
        "Hongfei Xue",
        "Kun Wei",
        "Lei Xie"
      ],
      "published": "2025-08-02T03:06:12+00:00",
      "updated": "2025-08-02T03:06:12+00:00",
      "arxiv_id": "2508.01166v1",
      "url": "http://arxiv.org/pdf/2508.01166v1",
      "categories": [
        "cs.SD"
      ],
      "primary_category": "cs.SD"
    },
    {
      "title": "CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages",
      "abstract": "Detecting emotions across different languages is challenging due to the\nvaried and culturally nuanced ways of emotional expressions. The\n\\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared\ntask was organised to investigate emotion recognition across different\nlanguages. The goal of the task is to implement an emotion recogniser that can\nidentify the basic emotional states that general third-party observers would\nattribute to an author based on their written text snippet, along with the\nintensity of those emotions. We report our investigation of various\ntask-adaptation strategies for LLMs in emotion recognition. We show that the\nmost effective method for this task is to fine-tune a pre-trained multilingual\nLLM with LoRA setting separately for each language.",
      "authors": [
        "Jiyu Chen",
        "Necva BÃ¶lÃ¼cÃ¼",
        "Sarvnaz Karimi",
        "Diego MollÃ¡",
        "CÃ©cile L. Paris"
      ],
      "published": "2025-08-02T02:55:26+00:00",
      "updated": "2025-08-02T02:55:26+00:00",
      "arxiv_id": "2508.01161v1",
      "url": "http://arxiv.org/pdf/2508.01161v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates",
      "abstract": "This study evaluates the capacity of large language models (LLMs) to generate\nstructured clinical consultation templates for electronic consultation. Using\n145 expert-crafted templates developed and routinely used by Stanford's\neConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,\nClaude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to\nproduce clinically coherent, concise, and prioritized clinical question\nschemas. Through a multi-agent pipeline combining prompt optimization, semantic\nautograding, and prioritization analysis, we show that while models like o3\nachieve high comprehensiveness (up to 92.2\\%), they consistently generate\nexcessively long templates and fail to correctly prioritize the most clinically\nimportant questions under length constraints. Performance varies across\nspecialties, with significant degradation in narrative-driven fields such as\npsychiatry and pain medicine. Our findings demonstrate that LLMs can enhance\nstructured clinical information exchange between physicians, while highlighting\nthe need for more robust evaluation methods that capture a model's ability to\nprioritize clinically salient information within the time constraints of\nreal-world physician communication.",
      "authors": [
        "Liam G. McCoy",
        "Fateme Nateghi Haredasht",
        "Kanav Chopra",
        "David Wu",
        "David JH Wu",
        "Abass Conteh",
        "Sarita Khemani",
        "Saloni Kumar Maharaj",
        "Vishnu Ravi",
        "Arth Pahwa",
        "Yingjie Weng",
        "Leah Rosengaus",
        "Lena Giang",
        "Kelvin Zhenghao Li",
        "Olivia Jee",
        "Daniel Shirvani",
        "Ethan Goh",
        "Jonathan H. Chen"
      ],
      "published": "2025-08-02T02:51:27+00:00",
      "updated": "2025-08-02T02:51:27+00:00",
      "arxiv_id": "2508.01159v1",
      "url": "http://arxiv.org/pdf/2508.01159v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "AgentSight: System-Level Observability for AI Agents Using eBPF",
      "abstract": "Modern software infrastructure increasingly relies on LLM agents for\ndevelopment and maintenance, such as Claude Code and Gemini-cli. However, these\nAI agents differ fundamentally from traditional deterministic software, posing\na significant challenge to conventional monitoring and debugging. This creates\na critical semantic gap: existing tools observe either an agent's high-level\nintent (via LLM prompts) or its low-level actions (e.g., system calls), but\ncannot correlate these two views. This blindness makes it difficult to\ndistinguish between benign operations, malicious attacks, and costly failures.\nWe introduce AgentSight, an AgentOps observability framework that bridges this\nsemantic gap using a hybrid approach. Our approach, boundary tracing, monitors\nagents from outside their application code at stable system interfaces using\neBPF. AgentSight intercepts TLS-encrypted LLM traffic to extract semantic\nintent, monitors kernel events to observe system-wide effects, and causally\ncorrelates these two streams across process boundaries using a real-time engine\nand secondary LLM analysis. This instrumentation-free technique is\nframework-agnostic, resilient to rapid API changes, and incurs less than 3%\nperformance overhead. Our evaluation shows AgentSight detects prompt injection\nattacks, identifies resource-wasting reasoning loops, and reveals hidden\ncoordination bottlenecks in multi-agent systems. AgentSight is released as an\nopen-source project at https://github.com/agent-sight/agentsight.",
      "authors": [
        "Yusheng Zheng",
        "Yanpeng Hu",
        "Tong Yu",
        "Andi Quinn"
      ],
      "published": "2025-08-02T01:43:39+00:00",
      "updated": "2025-08-02T01:43:39+00:00",
      "arxiv_id": "2508.02736v1",
      "url": "http://arxiv.org/pdf/2508.02736v1",
      "categories": [
        "cs.OS",
        "cs.SE"
      ],
      "primary_category": "cs.OS"
    },
    {
      "title": "DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs",
      "abstract": "The operation and maintenance (O&M) of database systems is critical to\nensuring system availability and performance, typically requiring expert\nexperience (e.g., identifying metric-to-anomaly relations) for effective\ndiagnosis and recovery. However, existing automatic database O&M methods,\nincluding commercial products, cannot effectively utilize expert experience. On\nthe one hand, rule-based methods only support basic O&M tasks (e.g.,\nmetric-based anomaly detection), which are mostly numerical equations and\ncannot effectively incorporate literal O&M experience (e.g., troubleshooting\nguidance in manuals). On the other hand, LLM-based methods, which retrieve\nfragmented information (e.g., standard documents + RAG), often generate\ninaccurate or generic results. To address these limitations, we present\nDBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with\nknowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a\nheterogeneous graph model for representing the diagnosis experience, and\nproposes a semi-automatic graph construction algorithm to build that graph from\nthousands of documents. Second, DBAIOps develops a collection of (800+)\nreusable anomaly models that identify both directly alerted metrics and\nimplicitly correlated experience and metrics. Third, for each anomaly, DBAIOps\nproposes a two-stage graph evolution mechanism to explore relevant diagnosis\npaths and identify missing relations automatically. It then leverages a\nreasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear\ndiagnosis reports for both DBAs and common users. Our evaluation over four\nmainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates\nthat DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher\nin root cause and human evaluation accuracy, respectively.",
      "authors": [
        "Wei Zhou",
        "Peng Sun",
        "Xuanhe Zhou",
        "Qianglei Zang",
        "Ji Xu",
        "Tieying Zhang",
        "Guoliang Li",
        "Fan Wu"
      ],
      "published": "2025-08-02T01:36:57+00:00",
      "updated": "2025-08-02T01:36:57+00:00",
      "arxiv_id": "2508.01136v1",
      "url": "http://arxiv.org/pdf/2508.01136v1",
      "categories": [
        "cs.DB",
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.DB"
    },
    {
      "title": "Transformers in Pseudo-Random Number Generation: A Dual Perspective on Theory and Practice",
      "abstract": "Pseudo-random number generators (PRNGs) are high-nonlinear processes, and\nthey are key blocks in optimization of Large language models. Transformers\nexcel at processing complex nonlinear relationships. Thus it is reasonable to\ngenerate high-quality pseudo-random numbers based on transformers. In this\npaper, we explore this question from both theoretical and practical\nperspectives, highlighting the potential benefits and implications of\nTransformer in PRNGs. We theoretically demonstrate that decoder-only\nTransformer models with Chain-of-Thought can simulate both the Linear\nCongruential Generator (LCG) and Mersenne Twister (MT) PRNGs. Based on this, we\nconclude that the log-precision decoder-only Transformer can represent\nnon-uniform $\\text{AC}^0$. Our simulative theoretical findings are validated\nthrough experiments. The random numbers generated by Transformer-based PRNGs\nsuccessfully pass the majority of NIST tests, whose heat maps exhibit clear\nstatistical randomness. Finally, we assess their capability in prediction\nattacks.",
      "authors": [
        "Ran Li",
        "Lingshu Zeng"
      ],
      "published": "2025-08-02T01:31:53+00:00",
      "updated": "2025-08-02T01:31:53+00:00",
      "arxiv_id": "2508.01134v1",
      "url": "http://arxiv.org/pdf/2508.01134v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation",
      "abstract": "Textual reviews enrich recommender systems with fine-grained preference\nsignals and enhanced explainability. However, in real-world scenarios, users\nrarely leave reviews, resulting in severe sparsity that undermines the\neffectiveness of existing models. A natural solution is to impute or generate\nmissing reviews to enrich the data. However, conventional imputation techniques\n-- such as matrix completion and LLM-based augmentation -- either lose\ncontextualized semantics by embedding texts into vectors, or overlook\nstructural dependencies among user-item interactions. To address these\nshortcomings, we propose TWISTER (ToWards Imputation on Sparsity with Textual\nEdge Graph Representation), a unified framework that imputes missing reviews by\njointly modeling semantic and structural signals. Specifically, we represent\nuser-item interactions as a Textual-Edge Graph (TEG), treating reviews as edge\nattributes. To capture relational context, we construct line-graph views and\nemploy a large language model as a graph-aware aggregator. For each interaction\nlacking a textual review, our model aggregates the neighborhood's\nnatural-language representations to generate a coherent and personalized\nreview. Experiments on the Amazon and Goodreads datasets show that TWISTER\nconsistently outperforms traditional numeric, graph-based, and LLM baselines,\ndelivering higher-quality imputed reviews and, more importantly, enhanced\nrecommendation performance. In summary, TWISTER generates reviews that are more\nhelpful, authentic, and specific, while smoothing structural signals for\nimproved recommendations.",
      "authors": [
        "Leyao Wang",
        "Xutao Mao",
        "Xuhui Zhan",
        "Yuying Zhao",
        "Bo Ni",
        "Ryan A. Rossi",
        "Nesreen K. Ahmed",
        "Tyler Derr"
      ],
      "published": "2025-08-02T00:53:40+00:00",
      "updated": "2025-08-02T00:53:40+00:00",
      "arxiv_id": "2508.01128v1",
      "url": "http://arxiv.org/pdf/2508.01128v1",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "The Promise of RL for Autoregressive Image Editing",
      "abstract": "We explore three strategies to enhance performance on a wide range of image\nediting tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and\nChain-of-Thought (CoT) reasoning. In order to study all these components in one\nconsistent framework, we adopt an autoregressive multimodal model that\nprocesses textual and visual tokens in a unified manner. We find RL combined\nwith a large multi-modal LLM verifier to be the most effective of these\nstrategies. As a result, we release EARL: Editing with Autoregression and RL, a\nstrong RL-based image editing model that performs competitively on a diverse\nrange of edits compared to strong baselines, despite using much less training\ndata. Thus, EARL pushes the frontier of autoregressive multimodal models on\nimage editing. We release our code, training data, and trained models at\nhttps://github.com/mair-lab/EARL.",
      "authors": [
        "Saba Ahmadi",
        "Rabiul Awal",
        "Ankur Sikarwar",
        "Amirhossein Kazemnejad",
        "Ge Ya Luo",
        "Juan A. Rodriguez",
        "Sai Rajeswar",
        "Siva Reddy",
        "Christopher Pal",
        "Benno Krojer",
        "Aishwarya Agrawal"
      ],
      "published": "2025-08-01T23:47:29+00:00",
      "updated": "2025-08-05T03:59:05+00:00",
      "arxiv_id": "2508.01119v2",
      "url": "http://arxiv.org/pdf/2508.01119v2",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?",
      "abstract": "We investigate whether socio-economic indicators like household wealth leave\nrecoverable imprints in satellite imagery (capturing physical features) and\nInternet-sourced text (reflecting historical/economic narratives). Using\nDemographic and Health Survey (DHS) data from African neighborhoods, we pair\nLandsat images with LLM-generated textual descriptions conditioned on\nlocation/year and text retrieved by an AI search agent from web sources. We\ndevelop a multimodal framework predicting household wealth (International\nWealth Index) through five pipelines: (i) vision model on satellite images,\n(ii) LLM using only location/year, (iii) AI agent searching/synthesizing web\ntext, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework\nyields three contributions. First, fusing vision and agent/LLM text outperforms\nvision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on\nout-of-sample splits), with LLM-internal knowledge proving more effective than\nagent-retrieved text, improving robustness to out-of-country and out-of-time\ngeneralization. Second, we find partial representational convergence: fused\nembeddings from vision/language modalities correlate moderately (median cosine\nsimilarity of 0.60 after alignment), suggesting a shared latent code of\nmaterial well-being while retaining complementary details, consistent with the\nPlatonic Representation Hypothesis. Although LLM-only text outperforms\nagent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest\ngains from combining agent data in some splits weakly support the notion that\nagent-gathered information introduces unique representational structures not\nfully captured by static LLM knowledge. Third, we release a large-scale\nmultimodal dataset comprising more than 60,000 DHS clusters linked to satellite\nimages, LLM-generated descriptions, and agent-retrieved texts.",
      "authors": [
        "Satiyabooshan Murugaboopathy",
        "Connor T. Jerzak",
        "Adel Daoud"
      ],
      "published": "2025-08-01T23:07:16+00:00",
      "updated": "2025-08-01T23:07:16+00:00",
      "arxiv_id": "2508.01109v1",
      "url": "http://arxiv.org/pdf/2508.01109v1",
      "categories": [
        "cs.AI",
        "68T07",
        "I.2; J.4"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Cross-Domain Web Information Extraction at Pinterest",
      "abstract": "The internet offers a massive repository of unstructured information, but\nit's a significant challenge to convert this into a structured format. At\nPinterest, the ability to accurately extract structured product data from\ne-commerce websites is essential to enhance user experiences and improve\ncontent distribution. In this paper, we present Pinterest's system for\nattribute extraction, which achieves remarkable accuracy and scalability at a\nmanageable cost. Our approach leverages a novel webpage representation that\ncombines structural, visual, and text modalities into a compact form,\noptimizing it for small model learning. This representation captures each\nvisible HTML node with its text, style and layout information. We show how this\nallows simple models such as eXtreme Gradient Boosting (XGBoost) to extract\nattributes more accurately than much more complex Large Language Models (LLMs)\nsuch as Generative Pre-trained Transformer (GPT). Our results demonstrate a\nsystem that is highly scalable, processing over 1,000 URLs per second, while\nbeing 1000 times more cost-effective than the cheapest GPT alternatives.",
      "authors": [
        "Michael Farag",
        "Patrick Halina",
        "Andrey Zaytsev",
        "Alekhya Munagala",
        "Imtihan Ahmed",
        "Junhao Wang"
      ],
      "published": "2025-08-01T22:22:35+00:00",
      "updated": "2025-08-01T22:22:35+00:00",
      "arxiv_id": "2508.01096v1",
      "url": "http://arxiv.org/pdf/2508.01096v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "What's in a Proof? Analyzing Expert Proof-Writing Processes in F* and Verus",
      "abstract": "Proof-oriented programming languages (POPLs) empower developers to write code\nalongside formal correctness proofs, providing formal guarantees that the code\nadheres to specified requirements. Despite their powerful capabilities, POPLs\npresent a steep learning curve and have not yet been adopted by the broader\nsoftware community. The lack of understanding about the proof-development\nprocess and how expert proof developers interact with POPLs has hindered the\nadvancement of effective proof engineering and the development of\nproof-synthesis models/tools.\n  In this work, we conduct a user study, involving the collection and analysis\nof fine-grained source code telemetry from eight experts working with two\nlanguages, F* and Verus. Results reveal interesting trends and patterns about\nhow experts reason about proofs and key challenges encountered during the proof\ndevelopment process. We identify three distinct strategies and multiple\ninformal practices that are not captured final code snapshots, yet are\npredictive of task outcomes. We translate these findings into concrete design\nguidance for AI proof assistants: bias toward early specification drafting,\nexplicit sub-goal decomposition, bounded active errors, and disciplined\nverifier interaction. We also present a case study of an F* proof agent\ngrounded in these recommendations, and demonstrate improved performance over\nbaseline LLMs",
      "authors": [
        "Rijul Jain",
        "Shraddha Barke",
        "Gabriel Ebner",
        "Md Rakib Hossain Misu",
        "Shan Lu",
        "Sarah Fakhoury"
      ],
      "published": "2025-08-01T22:16:30+00:00",
      "updated": "2025-08-01T22:16:30+00:00",
      "arxiv_id": "2508.02733v1",
      "url": "http://arxiv.org/pdf/2508.02733v1",
      "categories": [
        "cs.SE",
        "cs.HC"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "DescribePro: Collaborative Audio Description with Human-AI Interaction",
      "abstract": "Audio description (AD) makes video content accessible to millions of blind\nand low vision (BLV) users. However, creating high-quality AD involves a\ntrade-off between the precision of human-crafted descriptions and the\nefficiency of AI-generated ones. To address this, we present DescribePro a\ncollaborative AD authoring system that enables describers to iteratively refine\nAI-generated descriptions through multimodal large language model prompting and\nmanual editing. DescribePro also supports community collaboration by allowing\nusers to fork and edit existing ADs, enabling the exploration of different\nnarrative styles. We evaluate DescribePro with 18 describers (9 professionals\nand 9 novices) using quantitative and qualitative methods. Results show that AI\nsupport reduces repetitive work while helping professionals preserve their\nstylistic choices and easing the cognitive load for novices. Collaborative tags\nand variations show potential for providing customizations, version control,\nand training new describers. These findings highlight the potential of\ncollaborative, AI-assisted tools to enhance and scale AD authorship.",
      "authors": [
        "Maryam Cheema",
        "Sina Elahimanesh",
        "Samuel Martin",
        "Pooyan Fazli",
        "Hasti Seifi"
      ],
      "published": "2025-08-01T22:01:46+00:00",
      "updated": "2025-08-01T22:01:46+00:00",
      "arxiv_id": "2508.01092v1",
      "url": "http://arxiv.org/pdf/2508.01092v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Disaggregated Health Data in LLMs: Evaluating Data Equity in the Context of Asian American Representation",
      "abstract": "Large language models (LLMs), such as ChatGPT and Claude, have emerged as\nessential tools for information retrieval, often serving as alternatives to\ntraditional search engines. However, ensuring that these models provide\naccurate and equitable information tailored to diverse demographic groups\nremains an important challenge. This study investigates the capability of LLMs\nto retrieve disaggregated health-related information for sub-ethnic groups\nwithin the Asian American population, such as Korean and Chinese communities.\nData disaggregation has been a critical practice in health research to address\ninequities, making it an ideal domain for evaluating representation equity in\nLLM outputs. We apply a suite of statistical and machine learning tools to\nassess whether LLMs deliver appropriately disaggregated and equitable\ninformation. By focusing on Asian American sub-ethnic groups, a highly diverse\npopulation often aggregated in traditional analyses; we highlight how LLMs\nhandle complex disparities in health data. Our findings contribute to ongoing\ndiscussions about responsible AI, particularly in ensuring data equity in the\noutputs of LLM-based systems.",
      "authors": [
        "Uvini Balasuriya Mudiyanselage",
        "Bharat Jayprakash",
        "Kookjin Lee",
        "K. Hazel Kwon"
      ],
      "published": "2025-08-01T21:55:17+00:00",
      "updated": "2025-08-01T21:55:17+00:00",
      "arxiv_id": "2508.01091v1",
      "url": "http://arxiv.org/pdf/2508.01091v1",
      "categories": [
        "cs.CY"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Evading Data Provenance in Deep Neural Networks",
      "abstract": "Modern over-parameterized deep models are highly data-dependent, with large\nscale general-purpose and domain-specific datasets serving as the bedrock for\nrapid advancements. However, many datasets are proprietary or contain sensitive\ninformation, making unrestricted model training problematic. In the open world\nwhere data thefts cannot be fully prevented, Dataset Ownership Verification\n(DOV) has emerged as a promising method to protect copyright by detecting\nunauthorized model training and tracing illicit activities. Due to its\ndiversity and superior stealth, evading DOV is considered extremely\nchallenging. However, this paper identifies that previous studies have relied\non oversimplistic evasion attacks for evaluation, leading to a false sense of\nsecurity. We introduce a unified evasion framework, in which a teacher model\nfirst learns from the copyright dataset and then transfers task-relevant yet\nidentifier-independent domain knowledge to a surrogate student using an\nout-of-distribution (OOD) dataset as the intermediary. Leveraging\nVision-Language Models and Large Language Models, we curate the most\ninformative and reliable subsets from the OOD gallery set as the final transfer\nset, and propose selectively transferring task-oriented knowledge to achieve a\nbetter trade-off between generalization and evasion effectiveness. Experiments\nacross diverse datasets covering eleven DOV methods demonstrate our approach\nsimultaneously eliminates all copyright identifiers and significantly\noutperforms nine state-of-the-art evasion attacks in both generalization and\neffectiveness, with moderate computational overhead. As a proof of concept, we\nreveal key vulnerabilities in current DOV methods, highlighting the need for\nlong-term development to enhance practicality.",
      "authors": [
        "Hongyu Zhu",
        "Sichu Liang",
        "Wenwen Wang",
        "Zhuomeng Zhang",
        "Fangqi Li",
        "Shi-Lin Wang"
      ],
      "published": "2025-08-01T21:13:45+00:00",
      "updated": "2025-08-01T21:13:45+00:00",
      "arxiv_id": "2508.01074v1",
      "url": "http://arxiv.org/pdf/2508.01074v1",
      "categories": [
        "cs.CV",
        "cs.CR"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "A Note on Code Quality Score: LLMs for Maintainable Large Codebases",
      "abstract": "Maintaining code quality in large-scale software systems presents significant\nchallenges, particularly in settings where a large numbers of engineers work\nconcurrently on a codebase. This paper introduces Code Quality Score (CQS)\nsystem to automatically detect issues with a set of code changes and provide\nactionable insights. At its core, the CQS system is powered by two Llama3\nmodels, fine-tuned (with SFT and offline RL approaches), to a) detect common\ncode quality issues related to coding best practices and b) to provide good\n``critiques'' for LLM-generated code review respectively. To maintain good user\nexperience, we layer the system with hand-crafted rules to filter out incorrect\nresponses/hallucinations. Offline evaluations show that our CQS system is able\nto achieve an impressive precision rate for identifying valid issues. This\nsystem has already been rolled out to developers in an industrial scale setting\nand has consistently achieved 60\\% week over week user helpfulness rate,\ndemonstrating its effectiveness in a real-world environment. In this paper, we\npresent details of the CQS system along with some learnings on curating\ndeveloper feedback to create training data for LLM fine-tuning.",
      "authors": [
        "Sherman Wong",
        "Jalaj Bhandari",
        "Leo Zhou Fan Yang",
        "Xylan Xu",
        "Yi Zhuang",
        "Cem Cayiroglu",
        "Payal Bhuptani",
        "Sheela Yadawad",
        "Hung Duong"
      ],
      "published": "2025-08-01T21:09:45+00:00",
      "updated": "2025-08-01T21:09:45+00:00",
      "arxiv_id": "2508.02732v1",
      "url": "http://arxiv.org/pdf/2508.02732v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Expressive Power of Graph Transformers via Logic",
      "abstract": "Transformers are the basis of modern large language models, but relatively\nlittle is known about their precise expressive power on graphs. We study the\nexpressive power of graph transformers (GTs) by Dwivedi and Bresson (2020) and\nGPS-networks by Ramp\\'asek et al. (2022), both under soft-attention and average\nhard-attention. Our study covers two scenarios: the theoretical setting with\nreal numbers and the more practical case with floats. With reals, we show that\nin restriction to vertex properties definable in first-order logic (FO),\nGPS-networks have the same expressive power as graded modal logic (GML) with\nthe global modality. With floats, GPS-networks turn out to be equally\nexpressive as GML with the counting global modality. The latter result is\nabsolute, not restricting to properties definable in a background logic. We\nalso obtain similar characterizations for GTs in terms of propositional logic\nwith the global modality (for reals) and the counting global modality (for\nfloats).",
      "authors": [
        "Veeti Ahvonen",
        "Maurice Funk",
        "Damian Heiman",
        "Antti Kuusisto",
        "Carsten Lutz"
      ],
      "published": "2025-08-01T20:59:13+00:00",
      "updated": "2025-08-01T20:59:13+00:00",
      "arxiv_id": "2508.01067v1",
      "url": "http://arxiv.org/pdf/2508.01067v1",
      "categories": [
        "cs.LO",
        "cs.AI",
        "F.4.1; F.1.1; I.2.0"
      ],
      "primary_category": "cs.LO"
    },
    {
      "title": "Teaching at Scale: Leveraging AI to Evaluate and Elevate Engineering Education",
      "abstract": "Evaluating teaching effectiveness at scale remains a persistent challenge for\nlarge universities, particularly within engineering programs that enroll tens\nof thousands of students. Traditional methods, such as manual review of student\nevaluations, are often impractical, leading to overlooked insights and\ninconsistent data use. This article presents a scalable, AI-supported framework\nfor synthesizing qualitative student feedback using large language models. The\nsystem employs hierarchical summarization, anonymization, and exception\nhandling to extract actionable themes from open-ended comments while upholding\nethical safeguards. Visual analytics contextualize numeric scores through\npercentile-based comparisons, historical trends, and instructional load. The\napproach supports meaningful evaluation and aligns with best practices in\nqualitative analysis and educational assessment, incorporating student, peer,\nand self-reflective inputs without automating personnel decisions. We report on\nits successful deployment across a large college of engineering. Preliminary\nvalidation through comparisons with human reviewers, faculty feedback, and\nlongitudinal analysis suggests that LLM-generated summaries can reliably\nsupport formative evaluation and professional development. This work\ndemonstrates how AI systems, when designed with transparency and shared\ngovernance, can promote teaching excellence and continuous improvement at scale\nwithin academic institutions.",
      "authors": [
        "Jean-Francois Chamberland",
        "Martin C. Carlisle",
        "Arul Jayaraman",
        "Krishna R. Narayanan",
        "Sunay Palsole",
        "Karan Watson"
      ],
      "published": "2025-08-01T20:27:40+00:00",
      "updated": "2025-08-01T20:27:40+00:00",
      "arxiv_id": "2508.02731v1",
      "url": "http://arxiv.org/pdf/2508.02731v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report",
      "abstract": "Large language models (LLMs) have shown remarkable success across many\ndomains, yet their integration into cybersecurity applications remains limited\ndue to a lack of general-purpose cybersecurity data, representational\ncomplexity, and safety and regulatory concerns. To address this gap, we\npreviously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable\nfor fine-tuning on downstream tasks. That model, however, was not designed for\nchat-style interactions or instruction-following. In this report, we release\nFoundation-Sec-8B-Instruct: a model specifically trained for general-purpose\ncybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific\nknowledge with instruction-following, conversational capabilities, and\nalignment with human preferences to produce high-quality, relevant responses.\nComprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms\nLlama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its\ninstruction-following performance. It is also competitive with GPT-4o-mini on\ncyber threat intelligence and instruction-following tasks. We envision\nFoundation-Sec-8B-Instruct becoming an indispensable assistant in the daily\nworkflows of cybersecurity professionals. We release the model publicly at\nhttps://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.",
      "authors": [
        "Sajana Weerawardhena",
        "Paul Kassianik",
        "Blaine Nelson",
        "Baturay Saglam",
        "Anu Vellore",
        "Aman Priyanshu",
        "Supriti Vijay",
        "Massimo Aufiero",
        "Arthur Goldblatt",
        "Fraser Burch",
        "Ed Li",
        "Jianliang He",
        "Dhruv Kedia",
        "Kojin Oshiba",
        "Zhouran Yang",
        "Yaron Singer",
        "Amin Karbasi"
      ],
      "published": "2025-08-01T20:25:57+00:00",
      "updated": "2025-08-01T20:25:57+00:00",
      "arxiv_id": "2508.01059v1",
      "url": "http://arxiv.org/pdf/2508.01059v1",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Managing Escalation in Off-the-Shelf Large Language Models",
      "abstract": "U.S. national security customers have begun to utilize large language models,\nincluding enterprise versions of ``off-the-shelf'' models (e.g., ChatGPT)\nfamiliar to the public. This uptake will likely accelerate. However, recent\nstudies suggest that off-the-shelf large language models frequently suggest\nescalatory actions when prompted with geopolitical or strategic scenarios. We\ndemonstrate two simple, non-technical interventions to control these\ntendencies. Introducing these interventions into the experimental wargame\ndesign of a recent study, we substantially reduce escalation throughout the\ngame. Calls to restrict the use of large language models in national security\napplications are thus premature. The U.S. government is already, and will\ncontinue, employing large language models for scenario planning and suggesting\ncourses of action. Rather than warning against such applications, this study\nacknowledges the imminent adoption of large language models, and provides\nactionable measures to align them with national security goals, including\nescalation management.",
      "authors": [
        "Sebastian Elbaum",
        "Jonathan Panter"
      ],
      "published": "2025-08-01T20:15:45+00:00",
      "updated": "2025-08-05T13:51:26+00:00",
      "arxiv_id": "2508.01056v2",
      "url": "http://arxiv.org/pdf/2508.01056v2",
      "categories": [
        "cs.ET",
        "cs.AI"
      ],
      "primary_category": "cs.ET"
    },
    {
      "title": "FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models",
      "abstract": "Large language models (LLMs) have gained significant attention in chemistry.\nHowever, most existing datasets center on molecular-level property prediction\nand overlook the role of fine-grained functional group (FG) information.\nIncorporating FG-level data can provide valuable prior knowledge that links\nmolecular structures with textual descriptions, which can be used to build more\ninterpretable, structure-aware LLMs for reasoning on molecule-related tasks.\nMoreover, LLMs can learn from such fine-grained information to uncover hidden\nrelationships between specific functional groups and molecular properties,\nthereby advancing molecular design and drug discovery. Here, we introduce\nFGBench, a dataset comprising 625K molecular property reasoning problems with\nfunctional group information. Functional groups are precisely annotated and\nlocalized within the molecule, which ensures the dataset's interoperability\nthereby facilitating further multimodal applications. FGBench includes both\nregression and classification tasks on 245 different functional groups across\nthree categories for molecular property reasoning: (1) single functional group\nimpacts, (2) multiple functional group interactions, and (3) direct molecular\ncomparisons. In the benchmark of state-of-the-art LLMs on 7K curated data, the\nresults indicate that current LLMs struggle with FG-level property reasoning,\nhighlighting the need to enhance reasoning capabilities in LLMs for chemistry\ntasks. We anticipate that the methodology employed in FGBench to construct\ndatasets with functional group-level information will serve as a foundational\nframework for generating new question-answer pairs, enabling LLMs to better\nunderstand fine-grained molecular structure-property relationships. The dataset\nand evaluation code are available at https://github.com/xuanliugit/FGBench.",
      "authors": [
        "Xuan Liu",
        "Siru Ouyang",
        "Xianrui Zhong",
        "Jiawei Han",
        "Huimin Zhao"
      ],
      "published": "2025-08-01T20:15:29+00:00",
      "updated": "2025-08-05T05:05:17+00:00",
      "arxiv_id": "2508.01055v2",
      "url": "http://arxiv.org/pdf/2508.01055v2",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM",
        "q-bio.QM"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Autonomous Penetration Testing: Solving Capture-the-Flag Challenges with LLMs",
      "abstract": "This study evaluates the ability of GPT-4o to autonomously solve\nbeginner-level offensive security tasks by connecting the model to\nOverTheWire's Bandit capture-the-flag game. Of the 25 levels that were\ntechnically compatible with a single-command SSH framework, GPT-4o solved 18\nunaided and another two after minimal prompt hints for an overall 80% success\nrate. The model excelled at single-step challenges that involved Linux\nfilesystem navigation, data extraction or decoding, and straightforward\nnetworking. The approach often produced the correct command in one shot and at\na human-surpassing speed. Failures involved multi-command scenarios that\nrequired persistent working directories, complex network reconnaissance, daemon\ncreation, or interaction with non-standard shells. These limitations highlight\ncurrent architectural deficiencies rather than a lack of general exploit\nknowledge. The results demonstrate that large language models (LLMs) can\nautomate a substantial portion of novice penetration-testing workflow,\npotentially lowering the expertise barrier for attackers and offering\nproductivity gains for defenders who use LLMs as rapid reconnaissance aides.\nFurther, the unsolved tasks reveal specific areas where secure-by-design\nenvironments might frustrate simple LLM-driven attacks, informing future\nhardening strategies. Beyond offensive cybersecurity applications, results\nsuggest the potential to integrate LLMs into cybersecurity education as\npractice aids.",
      "authors": [
        "Isabelle Bakker",
        "John Hastings"
      ],
      "published": "2025-08-01T20:11:58+00:00",
      "updated": "2025-08-01T20:11:58+00:00",
      "arxiv_id": "2508.01054v1",
      "url": "http://arxiv.org/pdf/2508.01054v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "D.4.6; I.2.7; K.3.2"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent",
      "abstract": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing\nbut typically requires a high level of expertise from designers. To lower the\nentry barrier and improve design efficiency, we present an agent for CAD\nconceptual design powered by large language models (LLMs). The agent accepts\nboth abstract textual descriptions and freehand sketches as input, engaging in\ninteractive dialogue with users to refine and clarify design requirements\nthrough comprehensive requirement analysis. Built upon a novel\nContext-Independent Imperative Paradigm (CIP), the agent generates high-quality\nCAD modeling code. During the generation process, the agent incorporates\niterative visual feedback to improve model quality. Generated design cases are\nstored in a structured knowledge base, enabling continuous improvement of the\nagent's code generation capabilities. Experimental results demonstrate that our\nmethod achieves state-of-the-art performance in CAD code generation.",
      "authors": [
        "Jingzhe Ni",
        "Xiaolong Yin",
        "Xingyu Lu",
        "Xintong Li",
        "Ji Wei",
        "Ruofeng Tong",
        "Min Tang",
        "Peng Du"
      ],
      "published": "2025-08-01T19:15:56+00:00",
      "updated": "2025-08-05T10:26:43+00:00",
      "arxiv_id": "2508.01031v2",
      "url": "http://arxiv.org/pdf/2508.01031v2",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "AutoEDA: Enabling EDA Flow Automation through Microservice-Based LLM Agents",
      "abstract": "Modern Electronic Design Automation (EDA) workflows, especially the\nRTL-to-GDSII flow, require heavily manual scripting and demonstrate a multitude\nof tool-specific interactions which limits scalability and efficiency. While\nLLMs introduces strides for automation, existing LLM solutions require\nexpensive fine-tuning and do not contain standardized frameworks for\nintegration and evaluation. We introduce AutoEDA, a framework for EDA\nautomation that leverages paralleled learning through the Model Context\nProtocol (MCP) specific for standardized and scalable natural language\nexperience across the entire RTL-to-GDSII flow. AutoEDA limits fine-tuning\nthrough structured prompt engineering, implements intelligent parameter\nextraction and task decomposition, and provides an extended CodeBLEU metric to\nevaluate the quality of TCL scripts. Results from experiments over five\npreviously curated benchmarks show improvements in automation accuracy and\nefficiency, as well as script quality when compared to existing methods.\nAutoEDA is released open-sourced to support reproducibility and the EDA\ncommunity. Available at: https://github.com/AndyLu666/MCP-EDA-Server",
      "authors": [
        "Yiyi Lu",
        "Hoi Ian Au",
        "Junyao Zhang",
        "Jingyu Pan",
        "Yiting Wang",
        "Ang Li",
        "Jianyi Zhang",
        "Yiran Chen"
      ],
      "published": "2025-08-01T18:23:57+00:00",
      "updated": "2025-08-01T18:23:57+00:00",
      "arxiv_id": "2508.01012v1",
      "url": "http://arxiv.org/pdf/2508.01012v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation",
      "abstract": "We present ROVI, a high-quality synthetic dataset for instance-grounded\ntext-to-image generation, created by labeling 1M curated web images. Our key\ninnovation is a strategy called re-captioning, focusing on the pre-detection\nstage, where a VLM (Vision-Language Model) generates comprehensive visual\ndescriptions that are then processed by an LLM (Large Language Model) to\nextract a flat list of potential categories for OVDs (Open-Vocabulary\nDetectors) to detect. This approach yields a global prompt inherently linked to\ninstance annotations while capturing secondary visual elements humans typically\noverlook. Evaluations show that ROVI exceeds existing detection datasets in\nimage quality and resolution while containing two orders of magnitude more\ncategories with an open-vocabulary nature. For demonstrative purposes, a\ntext-to-image model GLIGEN trained on ROVI significantly outperforms\nstate-of-the-art alternatives in instance grounding accuracy, prompt fidelity,\nand aesthetic quality. Our dataset and reproducible pipeline are available at\nhttps://github.com/CihangPeng/ROVI.",
      "authors": [
        "Cihang Peng",
        "Qiming Hou",
        "Zhong Ren",
        "Kun Zhou"
      ],
      "published": "2025-08-01T18:19:51+00:00",
      "updated": "2025-08-01T18:19:51+00:00",
      "arxiv_id": "2508.01008v1",
      "url": "http://arxiv.org/pdf/2508.01008v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu",
      "abstract": "Multilingual Large Language Models (LLMs) have shown remarkable performance\nacross various languages; however, they often include significantly less data\nfor low-resource languages such as Urdu compared to high-resource languages\nlike English. To assess the linguistic knowledge of LLMs in Urdu, we present\nthe Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of\nminimally different sentences that contrast in grammatical acceptability.\nUrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,\ncarefully curated using the Urdu Treebank and diverse Urdu text corpora. A\nhuman evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator\nagreement, confirming the reliability of the dataset. We evaluate twenty\nmultilingual LLMs on UrBLiMP, revealing significant variation in performance\nacross linguistic phenomena. While LLaMA-3-70B achieves the highest average\naccuracy (94.73%), its performance is statistically comparable to other top\nmodels such as Gemma-3-27B-PT. These findings highlight both the potential and\nthe limitations of current multilingual LLMs in capturing fine-grained\nsyntactic knowledge in low-resource languages.",
      "authors": [
        "Farah Adeeba",
        "Brian Dillon",
        "Hassan Sajjad",
        "Rajesh Bhatt"
      ],
      "published": "2025-08-01T18:16:37+00:00",
      "updated": "2025-08-01T18:16:37+00:00",
      "arxiv_id": "2508.01006v1",
      "url": "http://arxiv.org/pdf/2508.01006v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Optimal Scheduling Algorithms for LLM Inference: Theory and Practice",
      "abstract": "With the growing use of Large Language Model (LLM)-based tools like ChatGPT,\nPerplexity, and Gemini across industries, there is a rising need for efficient\nLLM inference systems. These systems handle requests with a unique two-phase\ncomputation structure: a prefill-phase that processes the full input prompt and\na decode-phase that autoregressively generates tokens one at a time. This\nstructure calls for new strategies for routing and scheduling requests.\n  In this paper, we take a comprehensive approach to this challenge by\ndeveloping a theoretical framework that models routing and scheduling in LLM\ninference systems. We identify two key design principles-optimal tiling and\ndynamic resource allocation-that are essential for achieving high throughput.\nGuided by these principles, we propose the Resource-Aware Dynamic (RAD)\nscheduler and prove that it achieves throughput optimality under mild\nconditions. To address practical Service Level Objectives (SLOs) such as\nserving requests with different Time Between Token (TBT) constraints, we design\nthe SLO-Aware LLM Inference (SLAI) scheduler. SLAI uses real-time measurements\nto prioritize decode requests that are close to missing their TBT deadlines and\nreorders prefill requests based on known prompt lengths to further reduce the\nTime To First Token (TTFT) delays.\n  We evaluate SLAI on the Openchat ShareGPT4 dataset using the Mistral-7B model\non an NVIDIA RTX ADA 6000 GPU. Compared to Sarathi-Serve, SLAI reduces the\nmedian TTFT by 53% and increases the maximum serving capacity by 26% such that\nmedian TTFT is below 0.5 seconds, while meeting tail TBT latency constraints.",
      "authors": [
        "Agrim Bari",
        "Parikshit Hegde",
        "Gustavo de Veciana"
      ],
      "published": "2025-08-01T18:12:21+00:00",
      "updated": "2025-08-01T18:12:21+00:00",
      "arxiv_id": "2508.01002v1",
      "url": "http://arxiv.org/pdf/2508.01002v1",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Are LLM-Powered Social Media Bots Realistic?",
      "abstract": "As Large Language Models (LLMs) become more sophisticated, there is a\npossibility to harness LLMs to power social media bots. This work investigates\nthe realism of generating LLM-Powered social media bot networks. Through a\ncombination of manual effort, network science and LLMs, we create synthetic bot\nagent personas, their tweets and their interactions, thereby simulating social\nmedia networks. We compare the generated networks against empirical bot/human\ndata, observing that both network and linguistic properties of LLM-Powered Bots\ndiffer from Wild Bots/Humans. This has implications towards the detection and\neffectiveness of LLM-Powered Bots.",
      "authors": [
        "Lynnette Hui Xian Ng",
        "Kathleen M. Carley"
      ],
      "published": "2025-08-01T18:06:13+00:00",
      "updated": "2025-08-01T18:06:13+00:00",
      "arxiv_id": "2508.00998v1",
      "url": "http://arxiv.org/pdf/2508.00998v1",
      "categories": [
        "cs.SI",
        "cs.AI"
      ],
      "primary_category": "cs.SI"
    },
    {
      "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models",
      "abstract": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.",
      "authors": [
        "Jinsong Li",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Yuhang Cao",
        "Jiaqi Wang",
        "Dahua Lin"
      ],
      "published": "2025-08-01T17:56:07+00:00",
      "updated": "2025-08-01T17:56:07+00:00",
      "arxiv_id": "2508.00819v1",
      "url": "http://arxiv.org/pdf/2508.00819v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management",
      "abstract": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.",
      "authors": [
        "Ping Chen",
        "Zhuohong Deng",
        "Ping Li",
        "Shuibing He",
        "Hongzi Zhu",
        "Yi Zheng",
        "Zhefeng Wang",
        "Baoxing Huai",
        "Minyi Guo"
      ],
      "published": "2025-08-01T17:39:25+00:00",
      "updated": "2025-08-01T17:39:25+00:00",
      "arxiv_id": "2508.00806v1",
      "url": "http://arxiv.org/pdf/2508.00806v1",
      "categories": [
        "cs.LG",
        "cs.DC"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models",
      "abstract": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.",
      "authors": [
        "Xushuo Tang",
        "Yi Ding",
        "Zhengyi Yang",
        "Yin Chen",
        "Yongrui Gu",
        "Wenke Yang",
        "Mingchen Ju",
        "Xin Cao",
        "Yongfei Liu",
        "Wenjie Zhang"
      ],
      "published": "2025-08-01T17:11:42+00:00",
      "updated": "2025-08-01T17:11:42+00:00",
      "arxiv_id": "2508.00788v1",
      "url": "http://arxiv.org/pdf/2508.00788v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation",
      "abstract": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.",
      "authors": [
        "Atakan Site",
        "Emre Hakan Erdemir",
        "GÃ¼lÅen EryiÄit"
      ],
      "published": "2025-08-01T16:38:18+00:00",
      "updated": "2025-08-01T16:38:18+00:00",
      "arxiv_id": "2508.00762v1",
      "url": "http://arxiv.org/pdf/2508.00762v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations",
      "abstract": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.",
      "authors": [
        "Qiyao Xue",
        "Yuchen Dou",
        "Ryan Shi",
        "Xiang Lorraine Li",
        "Wei Gao"
      ],
      "published": "2025-08-01T16:34:57+00:00",
      "updated": "2025-08-01T16:34:57+00:00",
      "arxiv_id": "2508.00760v1",
      "url": "http://arxiv.org/pdf/2508.00760v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "GLiDRE: Generalist Lightweight model for Document-level Relation Extraction",
      "abstract": "Relation Extraction (RE) is a fundamental task in Natural Language\nProcessing, and its document-level variant poses significant challenges, due to\nthe need to model complex interactions between entities across sentences.\nCurrent approaches, largely based on the ATLOP architecture, are commonly\nevaluated on benchmarks like DocRED and Re-DocRED. However, their performance\nin zero-shot or few-shot settings remains largely underexplored due to the\ntask's complexity. Recently, the GLiNER model has shown that a compact NER\nmodel can outperform much larger Large Language Models. With a similar\nmotivation, we introduce GLiDRE, a new model for document-level relation\nextraction that builds on the key ideas of GliNER. We benchmark GLiDRE against\nstate-of-the-art models across various data settings on the Re-DocRED dataset.\nOur results demonstrate that GLiDRE achieves state-of-the-art performance in\nfew-shot scenarios. Our code is publicly available.",
      "authors": [
        "Robin Armingaud",
        "Romaric BesanÃ§on"
      ],
      "published": "2025-08-01T16:33:13+00:00",
      "updated": "2025-08-01T16:33:13+00:00",
      "arxiv_id": "2508.00757v1",
      "url": "http://arxiv.org/pdf/2508.00757v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Agentic large language models improve retrieval-based radiology question answering",
      "abstract": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.",
      "authors": [
        "Sebastian Wind",
        "Jeta Sopa",
        "Daniel Truhn",
        "Mahshad Lotfinia",
        "Tri-Thien Nguyen",
        "Keno Bressem",
        "Lisa Adams",
        "Mirabela Rusu",
        "Harald KÃ¶stler",
        "Gerhard Wellein",
        "Andreas Maier",
        "Soroosh Tayebi Arasteh"
      ],
      "published": "2025-08-01T16:18:52+00:00",
      "updated": "2025-08-01T16:18:52+00:00",
      "arxiv_id": "2508.00743v1",
      "url": "http://arxiv.org/pdf/2508.00743v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents",
      "abstract": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.",
      "authors": [
        "Sarah Mercer",
        "Daniel P. Martin",
        "Phil Swatton"
      ],
      "published": "2025-08-01T16:16:16+00:00",
      "updated": "2025-08-01T16:16:16+00:00",
      "arxiv_id": "2508.00742v1",
      "url": "http://arxiv.org/pdf/2508.00742v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data",
      "abstract": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.",
      "authors": [
        "Sohaib Imran",
        "Rob Lamb",
        "Peter M. Atkinson"
      ],
      "published": "2025-08-01T16:12:23+00:00",
      "updated": "2025-08-01T16:12:23+00:00",
      "arxiv_id": "2508.00741v1",
      "url": "http://arxiv.org/pdf/2508.00741v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "How LLMs are Shaping the Future of Virtual Reality",
      "abstract": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems.",
      "authors": [
        "SÃ¼eda Ãzkaya",
        "Santiago Berrezueta-Guzman",
        "Stefan Wagner"
      ],
      "published": "2025-08-01T16:08:05+00:00",
      "updated": "2025-08-04T09:29:35+00:00",
      "arxiv_id": "2508.00737v2",
      "url": "http://arxiv.org/pdf/2508.00737v2",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models",
      "abstract": "Despite growing interest in hallucination in Multimodal Large Language\nModels, existing studies primarily focus on single-image settings, leaving\nhallucination in multi-image scenarios largely unexplored. To address this gap,\nwe conduct the first systematic study of hallucinations in multi-image MLLMs\nand propose MIHBench, a benchmark specifically tailored for evaluating\nobject-related hallucinations across multiple images. MIHBench comprises three\ncore tasks: Multi-Image Object Existence Hallucination, Multi-Image Object\nCount Hallucination, and Object Identity Consistency Hallucination, targeting\nsemantic understanding across object existence, quantity reasoning, and\ncross-view identity consistency. Through extensive evaluation, we identify key\nfactors associated with the occurrence of multi-image hallucinations,\nincluding: a progressive relationship between the number of image inputs and\nthe likelihood of hallucination occurrences; a strong correlation between\nsingle-image hallucination tendencies and those observed in multi-image\ncontexts; and the influence of same-object image ratios and the positional\nplacement of negative samples within image sequences on the occurrence of\nobject identity consistency hallucination. To address these challenges, we\npropose a Dynamic Attention Balancing mechanism that adjusts inter-image\nattention distributions while preserving the overall visual attention\nproportion. Experiments across multiple state-of-the-art MLLMs demonstrate that\nour method effectively reduces hallucination occurrences and enhances semantic\nintegration and reasoning stability in multi-image scenarios.",
      "authors": [
        "Jiale Li",
        "Mingrui Wu",
        "Zixiang Jin",
        "Hao Chen",
        "Jiayi Ji",
        "Xiaoshuai Sun",
        "Liujuan Cao",
        "Rongrong Ji"
      ],
      "published": "2025-08-01T15:49:29+00:00",
      "updated": "2025-08-01T15:49:29+00:00",
      "arxiv_id": "2508.00726v1",
      "url": "http://arxiv.org/pdf/2508.00726v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA",
      "abstract": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.",
      "authors": [
        "Yingxu Wang",
        "Shiqi Fan",
        "Mengzhu Wang",
        "Siwei Liu"
      ],
      "published": "2025-08-01T15:38:21+00:00",
      "updated": "2025-08-01T15:38:21+00:00",
      "arxiv_id": "2508.00719v1",
      "url": "http://arxiv.org/pdf/2508.00719v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System",
      "abstract": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.",
      "authors": [
        "Shubham Kumar Nigam",
        "Balaramamahanthi Deepak Patnaik",
        "Shivam Mishra",
        "Ajay Varghese Thomas",
        "Noel Shallum",
        "Kripabandhu Ghosh",
        "Arnab Bhattacharya"
      ],
      "published": "2025-08-01T15:23:20+00:00",
      "updated": "2025-08-01T15:23:20+00:00",
      "arxiv_id": "2508.00709v1",
      "url": "http://arxiv.org/pdf/2508.00709v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written Code?",
      "abstract": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation.",
      "authors": [
        "Alfred Santa Molison",
        "Marcia Moraes",
        "Glaucia Melo",
        "Fabio Santos",
        "Wesley K. G. Assuncao"
      ],
      "published": "2025-08-01T15:17:34+00:00",
      "updated": "2025-08-01T15:17:34+00:00",
      "arxiv_id": "2508.00700v1",
      "url": "http://arxiv.org/pdf/2508.00700v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Better Call Claude: Can LLMs Detect Changes of Writing Style?",
      "abstract": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported.",
      "authors": [
        "Johannes RÃ¶misch",
        "Svetlana Gorovaia",
        "Mariia Halchynska",
        "Gleb Schmidt",
        "Ivan P. Yamshchikov"
      ],
      "published": "2025-08-01T14:49:50+00:00",
      "updated": "2025-08-01T14:49:50+00:00",
      "arxiv_id": "2508.00680v1",
      "url": "http://arxiv.org/pdf/2508.00680v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language",
      "abstract": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field.",
      "authors": [
        "Farhan Farsi",
        "Farnaz Aghababaloo",
        "Shahriar Shariati Motlagh",
        "Parsa Ghofrani",
        "MohammadAli SadraeiJavaheri",
        "Shayan Bali",
        "Amirhossein Shabani",
        "Farbod Bijary",
        "Ghazal Zamaninejad",
        "AmirMohammad Salehoof",
        "Saeedeh Momtazi"
      ],
      "published": "2025-08-01T14:46:57+00:00",
      "updated": "2025-08-01T14:46:57+00:00",
      "arxiv_id": "2508.00673v1",
      "url": "http://arxiv.org/pdf/2508.00673v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications",
      "abstract": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.",
      "authors": [
        "Wenxuan Wang",
        "Zizhan Ma",
        "Meidan Ding",
        "Shiyi Zheng",
        "Shengyuan Liu",
        "Jie Liu",
        "Jiaming Ji",
        "Wenting Chen",
        "Xiang Li",
        "Linlin Shen",
        "Yixuan Yuan"
      ],
      "published": "2025-08-01T14:41:31+00:00",
      "updated": "2025-08-01T14:41:31+00:00",
      "arxiv_id": "2508.00669v1",
      "url": "http://arxiv.org/pdf/2508.00669v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Demo: TOSense -- What Did You Just Agree to?",
      "abstract": "Online services often require users to agree to lengthy and obscure Terms of\nService (ToS), leading to information asymmetry and legal risks. This paper\nproposes TOSense-a Chrome extension that allows users to ask questions about\nToS in natural language and get concise answers in real time. The system\ncombines (i) a crawler \"tos-crawl\" that automatically extracts ToS content, and\n(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval\nand BART-encoder for answer relevance verification. To avoid expensive manual\nannotation, we present a novel Question Answering Evaluation Pipeline (QEP)\nthat generates synthetic questions and verifies the correctness of answers\nusing clustered topic matching. Experiments on five major platforms, Apple,\nGoogle, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of\nTOSense (with up to 44.5% accuracy) across varying number of topic clusters.\nDuring the demonstration, we will showcase TOSense in action. Attendees will be\nable to experience seamless extraction, interactive question answering, and\ninstant indexing of new sites.",
      "authors": [
        "Xinzhang Chen",
        "Hassan Ali",
        "Arash Shaghaghi",
        "Salil S. Kanhere",
        "Sanjay Jha"
      ],
      "published": "2025-08-01T14:26:23+00:00",
      "updated": "2025-08-01T14:26:23+00:00",
      "arxiv_id": "2508.00659v1",
      "url": "http://arxiv.org/pdf/2508.00659v1",
      "categories": [
        "cs.CR",
        "cs.CL"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented Generation for NLI",
      "abstract": "We introduce VAULT, a fully automated adversarial RAG pipeline that\nsystematically uncovers and remedies weaknesses in NLI models through three\nstages: retrieval, adversarial generation, and iterative retraining. First, we\nperform balanced few-shot retrieval by embedding premises with both semantic\n(BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM\nprompts to generate adversarial hypotheses, which are then validated by an LLM\nensemble for label fidelity. Finally, the validated adversarial examples are\ninjected back into the training set at increasing mixing ratios, progressively\nfortifying a zero-shot RoBERTa-base model.On standard benchmarks, VAULT\nelevates RoBERTa-base accuracy from 88.48% to 92.60% on SNLI +4.12%, from\n75.04% to 80.95% on ANLI +5.91%, and from 54.67% to 71.99% on MultiNLI +17.32%.\nIt also consistently outperforms prior in-context adversarial methods by up to\n2.0% across datasets. By automating high-quality adversarial data curation at\nscale, VAULT enables rapid, human-independent robustness improvements in NLI\ninference tasks.",
      "authors": [
        "Roie Kazoom",
        "Ofir Cohen",
        "Rami Puzis",
        "Asaf Shabtai",
        "Ofer Hadar"
      ],
      "published": "2025-08-01T14:22:54+00:00",
      "updated": "2025-08-01T14:22:54+00:00",
      "arxiv_id": "2508.00965v1",
      "url": "http://arxiv.org/pdf/2508.00965v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings",
      "abstract": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.",
      "authors": [
        "Alexia Jolicoeur-Martineau"
      ],
      "published": "2025-08-01T13:45:13+00:00",
      "updated": "2025-08-01T13:45:13+00:00",
      "arxiv_id": "2508.00632v1",
      "url": "http://arxiv.org/pdf/2508.00632v1",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.MM"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "MCeT: Behavioral Model Correctness Evaluation using Large Language Models",
      "abstract": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram.",
      "authors": [
        "Khaled Ahmed",
        "Jialing Song",
        "Boqi Chen",
        "Ou Wei",
        "Bingzhou Zheng"
      ],
      "published": "2025-08-01T13:41:58+00:00",
      "updated": "2025-08-01T13:41:58+00:00",
      "arxiv_id": "2508.00630v1",
      "url": "http://arxiv.org/pdf/2508.00630v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models",
      "abstract": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.",
      "authors": [
        "Shantanu Thorat",
        "Andrew Caines"
      ],
      "published": "2025-08-01T13:28:01+00:00",
      "updated": "2025-08-01T13:28:01+00:00",
      "arxiv_id": "2508.00619v1",
      "url": "http://arxiv.org/pdf/2508.00619v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "FinKario: Event-Enhanced Automated Construction of Financial Knowledge Graph",
      "abstract": "Individual investors are significantly outnumbered and disadvantaged in\nfinancial markets, overwhelmed by abundant information and lacking professional\nanalysis. Equity research reports stand out as crucial resources, offering\nvaluable insights. By leveraging these reports, large language models (LLMs)\ncan enhance investors' decision-making capabilities and strengthen financial\nanalysis. However, two key challenges limit their effectiveness: (1) the rapid\nevolution of market events often outpaces the slow update cycles of existing\nknowledge bases, (2) the long-form and unstructured nature of financial reports\nfurther hinders timely and context-aware integration by LLMs. To address these\nchallenges, we tackle both data and methodological aspects. First, we introduce\nthe Event-Enhanced Automated Construction of Financial Knowledge Graph\n(FinKario), a dataset comprising over 305,360 entities, 9,625 relational\ntriples, and 19 distinct relation types. FinKario automatically integrates\nreal-time company fundamentals and market events through prompt-driven\nextraction guided by professional institutional templates, providing structured\nand accessible financial insights for LLMs. Additionally, we propose a\nTwo-Stage, Graph-Based retrieval strategy (FinKario-RAG), optimizing the\nretrieval of evolving, large-scale financial knowledge to ensure efficient and\nprecise data access. Extensive experiments show that FinKario with FinKario-RAG\nachieves superior stock trend prediction accuracy, outperforming financial LLMs\nby 18.81% and institutional strategies by 17.85% on average in backtesting.",
      "authors": [
        "Xiang Li",
        "Penglei Sun",
        "Wanyun Zhou",
        "Zikai Wei",
        "Yongqi Zhang",
        "Xiaowen Chu"
      ],
      "published": "2025-08-01T13:27:35+00:00",
      "updated": "2025-08-01T13:27:35+00:00",
      "arxiv_id": "2508.00961v1",
      "url": "http://arxiv.org/pdf/2508.00961v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?",
      "abstract": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions.",
      "authors": [
        "Lennart Meincke",
        "Ethan Mollick",
        "Lilach Mollick",
        "Dan Shapiro"
      ],
      "published": "2025-08-01T13:23:21+00:00",
      "updated": "2025-08-01T13:23:21+00:00",
      "arxiv_id": "2508.00614v1",
      "url": "http://arxiv.org/pdf/2508.00614v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks",
      "abstract": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard.",
      "authors": [
        "Francesco Panebianco",
        "Stefano Bonfanti",
        "Francesco TrovÃ²",
        "Michele Carminati"
      ],
      "published": "2025-08-01T13:04:28+00:00",
      "updated": "2025-08-01T13:04:28+00:00",
      "arxiv_id": "2508.00602v1",
      "url": "http://arxiv.org/pdf/2508.00602v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models",
      "abstract": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.",
      "authors": [
        "Mingruo Yuan",
        "Shuyi Zhang",
        "Ben Kao"
      ],
      "published": "2025-08-01T12:58:34+00:00",
      "updated": "2025-08-01T12:58:34+00:00",
      "arxiv_id": "2508.00600v1",
      "url": "http://arxiv.org/pdf/2508.00600v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation",
      "abstract": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.",
      "authors": [
        "Ruiqing Ding",
        "Qianfang Sun",
        "Yongkang Leng",
        "Hui Yin",
        "Xiaojian Li"
      ],
      "published": "2025-08-01T12:24:49+00:00",
      "updated": "2025-08-01T12:24:49+00:00",
      "arxiv_id": "2508.00581v1",
      "url": "http://arxiv.org/pdf/2508.00581v1",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval",
      "abstract": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents.",
      "authors": [
        "Ziyu Gong",
        "Yihua Huang",
        "Chengcheng Mai"
      ],
      "published": "2025-08-01T12:22:53+00:00",
      "updated": "2025-08-01T12:22:53+00:00",
      "arxiv_id": "2508.00579v1",
      "url": "http://arxiv.org/pdf/2508.00579v1",
      "categories": [
        "cs.MM",
        "cs.IR"
      ],
      "primary_category": "cs.MM"
    },
    {
      "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought",
      "abstract": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.",
      "authors": [
        "Jianwei Wang",
        "Ziming Wu",
        "Fuming Lai",
        "Shaobing Lian",
        "Ziqian Zeng"
      ],
      "published": "2025-08-01T12:17:35+00:00",
      "updated": "2025-08-01T12:17:35+00:00",
      "arxiv_id": "2508.00574v1",
      "url": "http://arxiv.org/pdf/2508.00574v1",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Session-Based Recommendation with Validated and Enriched LLM Intents",
      "abstract": "Session-based recommendation (SBR) aims to predict the next item for an\nanonymous user in a timely manner. However, SBR suffers from data sparsity due\nto the short and anonymous nature of sessions. Recently, an emerging line of\nwork has explored inferring the underlying user intents of a session using\nlarge language models (LLMs), with the generated intents serving as auxiliary\ntraining signals to enhance SBR models. Despite its promise, this approach\nfaces three key challenges: validating intent quality, incorporating\nsession-level multi-intents, and complementing inevitable LLM failure cases. In\nthis paper, we propose VELI4SBR, a two-stage framework that leverages Validated\nand Enriched LLM-generated Intents for SBR. In the first stage, we generate\nhigh-quality intents using a predict-and-correct loop that validates the\ninformativeness of LLM-generated intents with a global intent pool to constrain\nthe LLM's output space and reduce hallucination. In the second stage, we\nenhance the SBR model using the generated intents through a lightweight\nmulti-intent prediction and fusion mechanism. Furthermore, we introduce a\ntraining strategy that compensates for LLM failures by inferring intents from\ninter-session behavioral similarities. Extensive experiments show that VELI4SBR\noutperforms state-of-the-art baselines while improving explainability.",
      "authors": [
        "Gyuseok Lee",
        "Yaokun Liu",
        "Yifan Liu",
        "Susik Yoon",
        "Dong Wang",
        "SeongKu Kang"
      ],
      "published": "2025-08-01T12:11:10+00:00",
      "updated": "2025-08-01T12:11:10+00:00",
      "arxiv_id": "2508.00570v1",
      "url": "http://arxiv.org/pdf/2508.00570v1",
      "categories": [
        "cs.IR"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism",
      "abstract": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multiagent systems and traditional quantitative investment methods\nacross diverse evaluation metrics.",
      "authors": [
        "Li Zhao",
        "Rui Sun",
        "Zuoyou Jiang",
        "Bo Yang",
        "Yuxiao Bai",
        "Mengting Chen",
        "Xinyang Wang",
        "Jing Li",
        "Zuo Bai"
      ],
      "published": "2025-08-01T11:48:13+00:00",
      "updated": "2025-08-01T11:48:13+00:00",
      "arxiv_id": "2508.00554v1",
      "url": "http://arxiv.org/pdf/2508.00554v1",
      "categories": [
        "q-fin.TR",
        "cs.CL",
        "q-fin.CP"
      ],
      "primary_category": "q-fin.TR"
    },
    {
      "title": "PaPaformer: Language Model from Pre-trained Paraller Paths",
      "abstract": "The training of modern large-language models requires an increasingly amount\nof computation power and time. Even smaller variants, such as small-language\nmodels (SLMs), take several days to train in the best-case scenarios, often\nrequiring multiple GPUs. This paper explores methods to train and evaluate\ndecoder-only transformer-based language models in hours instead of days/weeks.\nWe introduces \\textit{PaPaformer}, a decoder-only transformer architecture\nvariant, whose lower-dimensional parallel paths are combined into larger model.\nThe paper shows that these lower-dimensional paths can be trained individually\nwith different types of training data and then combined into one larger model.\nThis method gives the option to reduce the total number of model parameters and\nthe training time with increasing performance. Moreover, the use of parallel\npath structure opens interesting possibilities to customize paths to\naccommodate specific task requirements.",
      "authors": [
        "Joonas Tapaninaho",
        "Mourad Oussala"
      ],
      "published": "2025-08-01T11:33:45+00:00",
      "updated": "2025-08-01T11:33:45+00:00",
      "arxiv_id": "2508.00544v1",
      "url": "http://arxiv.org/pdf/2508.00544v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Small sample-based adaptive text classification through iterative and contrastive description refinement",
      "abstract": "Zero-shot text classification remains a difficult task in domains with\nevolving knowledge and ambiguous category boundaries, such as ticketing\nsystems. Large language models (LLMs) often struggle to generalize in these\nscenarios due to limited topic separability, while few-shot methods are\nconstrained by insufficient data diversity. We propose a classification\nframework that combines iterative topic refinement, contrastive prompting, and\nactive learning. Starting with a small set of labeled samples, the model\ngenerates initial topic labels. Misclassified or ambiguous samples are then\nused in an iterative contrastive prompting process to refine category\ndistinctions by explicitly teaching the model to differentiate between closely\nrelated classes. The framework features a human-in-the-loop component, allowing\nusers to introduce or revise category definitions in natural language. This\nenables seamless integration of new, unseen categories without retraining,\nmaking the system well-suited for real-world, dynamic environments. The\nevaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy\non AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with\nminimal accuracy shift after introducing unseen classes (82% and 87%,\nrespectively). The results highlight the effectiveness of prompt-based semantic\nreasoning for fine-grained classification with limited supervision.",
      "authors": [
        "Amrit Rajeev",
        "Udayaadithya Avadhanam",
        "Harshula Tulapurkar",
        "SaiBarath Sundar"
      ],
      "published": "2025-08-01T11:12:38+00:00",
      "updated": "2025-08-01T11:12:38+00:00",
      "arxiv_id": "2508.00957v1",
      "url": "http://arxiv.org/pdf/2508.00957v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond",
      "abstract": "Little research explores the correlation between the expressive ability and\ngeneralization ability of the low-rank adaptation (LoRA). Sharpness-Aware\nMinimization (SAM) improves model generalization for both Convolutional Neural\nNetworks (CNNs) and Transformers by encouraging convergence to locally flat\nminima. However, the connection between sharpness and generalization has not\nbeen fully explored for LoRA due to the lack of tools to either empirically\nseek flat minima or develop theoretical methods. In this work, we propose\nFlat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for\nLoRA. Concretely, we theoretically demonstrate that perturbations in the full\nparameter space can be transferred to the low-rank subspace. This approach\neliminates the potential interference introduced by perturbations across\nmultiple matrices in the low-rank subspace. Our extensive experiments on large\nlanguage models and vision-language models demonstrate that EFlat-LoRA achieves\noptimize efficiency comparable to that of LoRA while simultaneously attaining\ncomparable or even better performance. For example, on the GLUE dataset with\nRoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and\n0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat\nshows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,\nrespectively. These empirical results also verify that the generalization of\nLoRA is closely related to sharpness, which is omitted by previous methods.",
      "authors": [
        "Jiaxin Deng",
        "Qingcheng Zhu",
        "Junbiao Pang",
        "Linlin Yang",
        "Zhongqian Fu",
        "Baochang Zhang"
      ],
      "published": "2025-08-01T10:59:49+00:00",
      "updated": "2025-08-01T10:59:49+00:00",
      "arxiv_id": "2508.00522v1",
      "url": "http://arxiv.org/pdf/2508.00522v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection",
      "abstract": "The natural combination of intricate topological structures and rich textual\ninformation in text-attributed graphs (TAGs) opens up a novel perspective for\ngraph anomaly detection (GAD). However, existing GAD methods primarily focus on\ndesigning complex optimization objectives within the graph domain, overlooking\nthe complementary value of the textual modality, whose features are often\nencoded by shallow embedding techniques, such as bag-of-words or skip-gram, so\nthat semantic context related to anomalies may be missed. To unleash the\nenormous potential of textual modality, large language models (LLMs) have\nemerged as promising alternatives due to their strong semantic understanding\nand reasoning capabilities. Nevertheless, their application to TAG anomaly\ndetection remains nascent, and they struggle to encode high-order structural\ninformation inherent in graphs due to input length constraints. For\nhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework that\ncombines LLMs and graph neural networks (GNNs) to leverage their complementary\nstrengths. CoLL employs multi-LLM collaboration for evidence-augmented\ngeneration to capture anomaly-relevant contexts while delivering human-readable\nrationales for detected anomalies. Moreover, CoLL integrates a GNN equipped\nwith a gating mechanism to adaptively fuse textual features with evidence while\npreserving high-order topological information. Extensive experiments\ndemonstrate the superiority of CoLL, achieving an average improvement of 13.37%\nin AP. This study opens a new avenue for incorporating LLMs in advancing GAD.",
      "authors": [
        "Yiming Xu",
        "Jiarun Chen",
        "Zhen Peng",
        "Zihan Chen",
        "Qika Lin",
        "Lan Ma",
        "Bin Shi",
        "Bo Dong"
      ],
      "published": "2025-08-01T10:36:39+00:00",
      "updated": "2025-08-01T10:36:39+00:00",
      "arxiv_id": "2508.00507v1",
      "url": "http://arxiv.org/pdf/2508.00507v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking",
      "abstract": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.",
      "authors": [
        "Haoyu Wang",
        "Chris M. Poskitt",
        "Jun Sun",
        "Jiali Wei"
      ],
      "published": "2025-08-01T10:24:47+00:00",
      "updated": "2025-08-01T10:24:47+00:00",
      "arxiv_id": "2508.00500v1",
      "url": "http://arxiv.org/pdf/2508.00500v1",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization",
      "abstract": "Modern cyber attacks unfold through multiple stages, requiring defenders to\ndynamically prioritize mitigations under uncertainty. While game-theoretic\nmodels capture attacker-defender interactions, existing approaches often rely\non static assumptions and lack integration with real-time threat intelligence,\nlimiting their adaptability. This paper presents CyGATE, a game-theoretic\nframework modeling attacker-defender interactions, using large language models\n(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection\nand patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber\nconflicts as a partially observable stochastic game (POSG) across Cyber Kill\nChain stages. Both agents use belief states to navigate uncertainty, with the\nattacker adapting tactics and the defender re-prioritizing patches based on\nevolving risks and observed adversary behavior. The framework's flexible\narchitecture enables extension to multi-agent scenarios involving coordinated\nattackers, collaborative defenders, or complex enterprise environments with\nmultiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE\neffectively prioritizes high-risk vulnerabilities, enhancing adaptability\nthrough dynamic threat integration, strategic foresight by anticipating\nattacker moves under uncertainty, and efficiency by optimizing resource use.",
      "authors": [
        "Yuning Jiang",
        "Nay Oo",
        "Qiaoran Meng",
        "Lu Lin",
        "Dusit Niyato",
        "Zehui Xiong",
        "Hoon Wei Lim",
        "Biplab Sikdar"
      ],
      "published": "2025-08-01T09:53:06+00:00",
      "updated": "2025-08-01T09:53:06+00:00",
      "arxiv_id": "2508.00478v1",
      "url": "http://arxiv.org/pdf/2508.00478v1",
      "categories": [
        "cs.CR",
        "cs.AI",
        "91A10, 91A43, 68T01, 94A60",
        "C.2.0; I.2.6; K.6.5"
      ],
      "primary_category": "cs.CR"
    },
    {
      "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs",
      "abstract": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended.",
      "authors": [
        "Andrea Asperti",
        "Alberto Naibo",
        "Claudio Sacerdoti Coen"
      ],
      "published": "2025-08-01T09:31:48+00:00",
      "updated": "2025-08-01T09:31:48+00:00",
      "arxiv_id": "2508.00459v1",
      "url": "http://arxiv.org/pdf/2508.00459v1",
      "categories": [
        "cs.AI",
        "68T07, 68T20",
        "I.2.6; I.2.7; I.2.3"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
      "abstract": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
      "authors": [
        "Yuqi Tang",
        "Kehua Feng",
        "Yunfeng Wang",
        "Zhiwen Chen",
        "Chengfei Lv",
        "Gang Yu",
        "Qiang Zhang",
        "Keyan Ding"
      ],
      "published": "2025-08-01T09:26:01+00:00",
      "updated": "2025-08-01T09:26:01+00:00",
      "arxiv_id": "2508.00454v1",
      "url": "http://arxiv.org/pdf/2508.00454v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation",
      "abstract": "Traditional recommendation systems tend to trap users in strong feedback\nloops by excessively pushing content aligned with their historical preferences,\nthereby limiting exploration opportunities and causing content fatigue.\nAlthough large language models (LLMs) demonstrate potential with their diverse\ncontent generation capabilities, existing LLM-enhanced dual-model frameworks\nface two major limitations: first, they overlook long-term preferences driven\nby group identity, leading to biased interest modeling; second, they suffer\nfrom static optimization flaws, as a one-time alignment process fails to\nleverage incremental user data for closed-loop optimization. To address these\nchallenges, we propose the Co-Evolutionary Alignment (CoEA) method. For\ninterest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)\nmodule, jointly modeling long-term group identity and short-term individual\ninterests through parallel processing of behavioral sequences. For static\noptimization limitations, we design a Periodic Collaborative Optimization (PCO)\nmechanism. This mechanism regularly conducts preference verification on\nincremental data using the Relevance LLM, then guides the Novelty LLM to\nperform fine-tuning based on the verification results, and subsequently feeds\nback the output of the incrementally fine-tuned Novelty LLM to the Relevance\nLLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.\nExtensive online and offline experiments verify the effectiveness of the CoEA\nmodel in exploratory recommendation.",
      "authors": [
        "Hongxiang Lin",
        "Hao Guo",
        "Zeshun Li",
        "Erpeng Xue",
        "Yongqian He",
        "Xiangyu Hou",
        "Zhaoyu Hu",
        "Lei Wang",
        "Sheng Chen"
      ],
      "published": "2025-08-01T09:10:56+00:00",
      "updated": "2025-08-01T09:10:56+00:00",
      "arxiv_id": "2508.00450v1",
      "url": "http://arxiv.org/pdf/2508.00450v1",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR"
    },
    {
      "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network",
      "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.",
      "authors": [
        "Minghao Guo",
        "Xi Zhu",
        "Jingyuan Huang",
        "Kai Mei",
        "Yongfeng Zhang"
      ],
      "published": "2025-08-01T08:37:54+00:00",
      "updated": "2025-08-05T09:07:34+00:00",
      "arxiv_id": "2508.00429v2",
      "url": "http://arxiv.org/pdf/2508.00429v2",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Automated Type Annotation in Python Using Large Language Models",
      "abstract": "Type annotations in Python enhance maintainability and error detection.\nHowever, generating these annotations manually is error prone and requires\nextra effort. Traditional automation approaches like static analysis, machine\nlearning, and deep learning struggle with limited type vocabularies, behavioral\nover approximation, and reliance on large labeled datasets. In this work, we\nexplore the use of LLMs for generating type annotations in Python. We develop a\ngenerate check repair pipeline: the LLM proposes annotations guided by a\nConcrete Syntax Tree representation, a static type checker (Mypy) verifies\nthem, and any errors are fed back for iterative refinement. We evaluate four\nLLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini\n(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.\nWe first measure the proportion of code snippets annotated by LLMs for which\nMyPy reported no errors (i.e., consistent results): GPT 4oMini achieved\nconsistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,\nand O4Mini each reached approximately 88.6% consistency (around 11.4%\nfailures). To measure annotation quality, we then compute exact-match and\nbase-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini\nperform the best, achieving up to 70.5% exact match and 79.1% base type\naccuracy, requiring under one repair iteration on average. Our results\ndemonstrate that general-purpose and reasoning optimized LLMs, without any task\nspecific fine tuning or additional training can be effective in generating\nconsistent type annotations.They perform competitively with traditional deep\nlearning techniques which require large labeled dataset for training. While our\nwork focuses on Python, the pipeline can be extended to other optionally typed\nimperative languages like Ruby",
      "authors": [
        "Varun Bharti",
        "Shashwat Jha",
        "Dhruv Kumar",
        "Pankaj Jalote"
      ],
      "published": "2025-08-01T08:24:14+00:00",
      "updated": "2025-08-01T08:24:14+00:00",
      "arxiv_id": "2508.00422v1",
      "url": "http://arxiv.org/pdf/2508.00422v1",
      "categories": [
        "cs.PL",
        "cs.LG"
      ],
      "primary_category": "cs.PL"
    },
    {
      "title": "Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers",
      "abstract": "Loop invariants are essential for proving the correctness of programs with\nloops. Developing loop invariants is challenging, and fully automatic synthesis\ncannot be guaranteed for arbitrary programs. Some approaches have been proposed\nto synthesize loop invariants using symbolic techniques and more recently using\nneural approaches. These approaches are able to correctly synthesize loop\ninvariants only for subsets of standard benchmarks. In this work, we\ninvestigate whether modern, reasoning-optimized large language models can do\nbetter. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled\ngenerate-and-check pipeline with the Z3 SMT solver, using solver\ncounterexamples to iteratively guide invariant refinement. We use Code2Inv\nbenchmark, which provides C programs along with their formal preconditions and\npostconditions. On this benchmark of 133 tasks, our framework achieves 100%\ncoverage (133 out of 133), outperforming the previous best of 107 out of 133,\nwhile requiring only 1-2 model proposals per instance and 14-55 seconds of\nwall-clock time. These results demonstrate that LLMs possess latent logical\nreasoning capabilities which can help automate loop invariant synthesis. While\nour experiments target C-specific programs, this approach should be\ngeneralizable to other imperative languages.",
      "authors": [
        "Varun Bharti",
        "Shashwat Jha",
        "Dhruv Kumar",
        "Pankaj Jalote"
      ],
      "published": "2025-08-01T08:15:15+00:00",
      "updated": "2025-08-01T08:15:15+00:00",
      "arxiv_id": "2508.00419v1",
      "url": "http://arxiv.org/pdf/2508.00419v1",
      "categories": [
        "cs.LO",
        "cs.LG",
        "cs.PL"
      ],
      "primary_category": "cs.LO"
    },
    {
      "title": "Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement",
      "abstract": "Although reinforcement learning with verifiable rewards (RLVR) shows promise\nin improving the reasoning ability of large language models (LLMs), the scaling\nup dilemma remains due to the reliance on human annotated labels especially for\ncomplex tasks. Recent alternatives that explore various self-reward signals\nexhibit the eliciting potential of LLM reasoning, but suffer from the\nnon-negligible collapse issue. Inspired by the success of self-supervised\nlearning, we propose \\textit{Co-Reward}, a novel RL framework that leverages\ncontrastive agreement across semantically analogical questions as a reward\nbasis. Specifically, we construct a similar question for each training sample\n(without labels) and synthesize their individual surrogate labels through a\nsimple rollout voting, and then the reward is constructed by cross-referring\nthe labels of each question pair to enforce the internal reasoning consistency\nacross analogical inputs. Intuitively, such a self-supervised reward-shaping\nmechanism increases the difficulty of learning collapse into a trivial\nsolution, and promotes stable reasoning elicitation and improvement through\nexpanding the input sample variants. Empirically, Co-Reward achieves superior\nperformance compared to other self-reward baselines on multiple reasoning\nbenchmarks and LLM series, and reaches or even surpasses ground-truth (GT)\nlabeled reward, with improvements of up to $+6.8\\%$ on MATH500 over GT reward\non Llama-3.2-3B-Instruct. Our code is publicly available at\nhttps://github.com/tmlr-group/Co-Reward.",
      "authors": [
        "Zizhuo Zhang",
        "Jianing Zhu",
        "Xinmu Ge",
        "Zihua Zhao",
        "Zhanke Zhou",
        "Xuan Li",
        "Xiao Feng",
        "Jiangchao Yao",
        "Bo Han"
      ],
      "published": "2025-08-01T08:09:14+00:00",
      "updated": "2025-08-01T08:09:14+00:00",
      "arxiv_id": "2508.00410v1",
      "url": "http://arxiv.org/pdf/2508.00410v1",
      "categories": [
        "cs.LG"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions",
      "abstract": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%).",
      "authors": [
        "Dong Huang",
        "Jie M. Zhang",
        "Mark Harman",
        "Qianru Zhang",
        "Mingzhe Du",
        "See-Kiong Ng"
      ],
      "published": "2025-08-01T08:08:26+00:00",
      "updated": "2025-08-01T08:08:26+00:00",
      "arxiv_id": "2508.00408v1",
      "url": "http://arxiv.org/pdf/2508.00408v1",
      "categories": [
        "cs.SE",
        "cs.CL"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition",
      "abstract": "Cued Speech (CS) is a visual communication system that combines lip-reading\nwith hand coding to facilitate communication for individuals with hearing\nimpairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures\nand lip movements into text via AI-driven methods. Traditionally, the temporal\nasynchrony between hand and lip movements requires the design of complex\nmodules to facilitate effective multimodal fusion. However, constrained by\nlimited data availability, current methods demonstrate insufficient capacity\nfor adequately training these fusion mechanisms, resulting in suboptimal\nperformance. Recently, multi-agent systems have shown promising capabilities in\nhandling complex tasks with limited data availability. To this end, we propose\nthe first collaborative multi-agent system for ACSR, named Cued-Agent. It\nintegrates four specialized sub-agents: a Multimodal Large Language Model-based\nHand Recognition agent that employs keyframe screening and CS expert prompt\nstrategies to decode hand movements, a pretrained Transformer-based Lip\nRecognition agent that extracts lip features from the input video, a Hand\nPrompt Decoding agent that dynamically integrates hand prompts with lip\nfeatures during inference in a training-free manner, and a Self-Correction\nPhoneme-to-Word agent that enables post-process and end-to-end conversion from\nphoneme sequences to natural language sentences for the first time through\nsemantic refinement. To support this study, we expand the existing Mandarin CS\ndataset by collecting data from eight hearing-impaired cuers, establishing a\nmixed dataset of fourteen subjects. Extensive experiments demonstrate that our\nCued-Agent performs superbly in both normal and hearing-impaired scenarios\ncompared with state-of-the-art methods. The implementation is available at\nhttps://github.com/DennisHgj/Cued-Agent.",
      "authors": [
        "Guanjie Huang",
        "Danny H. K. Tsang",
        "Shan Yang",
        "Guangzhi Lei",
        "Li Liu"
      ],
      "published": "2025-08-01T07:40:39+00:00",
      "updated": "2025-08-01T07:40:39+00:00",
      "arxiv_id": "2508.00391v1",
      "url": "http://arxiv.org/pdf/2508.00391v1",
      "categories": [
        "cs.CV",
        "eess.AS"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model",
      "abstract": "Multimodal Large Language Models (MLLMs) have emerged as a promising solution\nfor universal embedding tasks, yet adapting their generative nature for\ndiscriminative representation learning remains a significant challenge. The\ndominant paradigm of large-scale contrastive pre-training suffers from critical\ninefficiencies, including prohibitive computational costs and a failure to\nleverage the intrinsic, instruction-following capabilities of MLLMs. To\novercome these limitations, we propose an efficient framework for universal\nmultimodal embeddings, which bridges this gap by centering on two synergistic\ncomponents. First, our hierarchical embedding prompt template employs a\ntwo-level instruction architecture that forces the model to produce\ndiscriminative representations. Building on this strong foundation, our second\ncomponent, self-aware hard negative sampling, redefines the fine-tuning process\nby leveraging the model's own understanding to efficiently mine challenging\nnegatives while actively filtering out potential false negatives. Our\ncomprehensive experiments show that our hierarchical prompt achieves zero-shot\nperformance competitive with contrastively trained baselines and enhances the\nfine-tuning process by lifting a simple in-batch negative baseline by 4.8\npoints on the MMEB benchmark. We further boost the performance via our\nself-aware hard negative sampling, achieving the state-of-the-art performance\nwithout the contrative pre-training. Our work presents an effective and\nefficient pathway to adapt MLLMs for universal embedding tasks, significantly\nreducing training time.",
      "authors": [
        "Yeong-Joon Ju",
        "Seong-Whan Lee"
      ],
      "published": "2025-08-01T07:31:24+00:00",
      "updated": "2025-08-01T07:31:24+00:00",
      "arxiv_id": "2508.00955v1",
      "url": "http://arxiv.org/pdf/2508.00955v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness",
      "abstract": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.",
      "authors": [
        "Dingzirui Wang",
        "Xuangliang Zhang",
        "Keyan Xu",
        "Qingfu Zhu",
        "Wanxiang Che",
        "Yang Deng"
      ],
      "published": "2025-08-01T07:26:39+00:00",
      "updated": "2025-08-01T07:26:39+00:00",
      "arxiv_id": "2508.00385v1",
      "url": "http://arxiv.org/pdf/2508.00385v1",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models",
      "abstract": "Video-based long-term action anticipation is crucial for early risk detection\nin areas such as automated driving and robotics. Conventional approaches\nextract features from past actions using encoders and predict future events\nwith decoders, which limits performance due to their unidirectional nature.\nThese methods struggle to capture semantically distinct sub-actions within a\nscene. The proposed method, BiAnt, addresses this limitation by combining\nforward prediction with backward prediction using a large language model.\nExperimental results on Ego4D demonstrate that BiAnt improves performance in\nterms of edit distance compared to baseline methods.",
      "authors": [
        "Yuji Sato",
        "Yasunori Ishii",
        "Takayoshi Yamashita"
      ],
      "published": "2025-08-01T07:07:24+00:00",
      "updated": "2025-08-01T07:07:24+00:00",
      "arxiv_id": "2508.00374v1",
      "url": "http://arxiv.org/pdf/2508.00374v1",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices",
      "abstract": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
      "authors": [
        "Jiyu Chen",
        "Poh Seng Lim",
        "Shuang Peng",
        "Daxiong Luo",
        "JungHau Foo",
        "Yap Deep",
        "Timothy Lee Jun Jie",
        "Kelvin Teh Kae Wen",
        "Fan Yang",
        "Danyu Feng",
        "Hao-Yun Chen",
        "Peng-Wen Chen",
        "Fangyuan Li",
        "Xiaoxin Chen",
        "Wong Wai Mun"
      ],
      "published": "2025-08-01T07:03:16+00:00",
      "updated": "2025-08-06T08:32:53+00:00",
      "arxiv_id": "2508.00370v2",
      "url": "http://arxiv.org/pdf/2508.00370v2",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning",
      "abstract": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale.",
      "authors": [
        "Keer Lu",
        "Chong Chen",
        "Bin Cui",
        "Huang Leng",
        "Wentao Zhang"
      ],
      "published": "2025-08-01T06:17:11+00:00",
      "updated": "2025-08-01T06:17:11+00:00",
      "arxiv_id": "2508.00344v1",
      "url": "http://arxiv.org/pdf/2508.00344v1",
      "categories": [
        "cs.CL"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "From Patient Burdens to User Agency: Designing for Real-Time Protection Support in Online Health Consultations",
      "abstract": "Online medical consultation platforms, while convenient, are undermined by\nsignificant privacy risks that erode user trust. We first conducted in-depth\nsemi-structured interviews with 12 users to understand their perceptions of\nsecurity and privacy landscapes on online medical consultation platforms, as\nwell as their practices, challenges and expectation. Our analysis reveals a\ncritical disconnect between users' desires for anonymity and control, and\nplatform realities that offload the responsibility of ``privacy labor''. To\nbridge this gap, we present SafeShare, an interaction technique that leverages\nlocalized LLM to redact consultations in real-time. SafeShare balances utility\nand privacy through selectively anonymize private information. A technical\nevaluation of SafeShare's core PII detection module on 3 dataset demonstrates\nhigh efficacy, achieving 89.64\\% accuracy with Qwen3-4B on IMCS21 dataset.",
      "authors": [
        "Shuning Zhang",
        "Ying Ma",
        "Yongquan `Owen' Hu",
        "Ting Dang",
        "Hong Jia",
        "Xin Yi",
        "Hewu Li"
      ],
      "published": "2025-08-01T05:21:42+00:00",
      "updated": "2025-08-01T05:21:42+00:00",
      "arxiv_id": "2508.00328v1",
      "url": "http://arxiv.org/pdf/2508.00328v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Evaluating the Efficacy of Large Language Models for Generating Fine-Grained Visual Privacy Policies in Homes",
      "abstract": "The proliferation of visual sensors in smart home environments, particularly\nthrough wearable devices like smart glasses, introduces profound privacy\nchallenges. Existing privacy controls are often static and coarse-grained,\nfailing to accommodate the dynamic and socially nuanced nature of home\nenvironments. This paper investigates the viability of using Large Language\nModels (LLMs) as the core of a dynamic and adaptive privacy policy engine. We\npropose a conceptual framework where visual data is classified using a\nmulti-dimensional schema that considers data sensitivity, spatial context, and\nsocial presence. An LLM then reasons over this contextual information to\nenforce fine-grained privacy rules, such as selective object obfuscation, in\nreal-time. Through a comparative evaluation of state-of-the-art Vision Language\nModels (including GPT-4o and the Qwen-VL series) in simulated home settings ,\nour findings show the feasibility of this approach. The LLM-based engine\nachieved a top machine-evaluated appropriateness score of 3.99 out of 5, and\nthe policies generated by the models received a top human-evaluated score of\n4.00 out of 5.",
      "authors": [
        "Shuning Zhang",
        "Ying Ma",
        "Xin Yi",
        "Hewu Li"
      ],
      "published": "2025-08-01T05:11:29+00:00",
      "updated": "2025-08-01T05:11:29+00:00",
      "arxiv_id": "2508.00321v1",
      "url": "http://arxiv.org/pdf/2508.00321v1",
      "categories": [
        "cs.HC"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models",
      "abstract": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.",
      "authors": [
        "Ammar Ahmed",
        "Sheng Di",
        "Franck Cappello",
        "Zirui Liu",
        "Jingoo Han",
        "Ali Anwar"
      ],
      "published": "2025-08-01T04:17:24+00:00",
      "updated": "2025-08-01T04:17:24+00:00",
      "arxiv_id": "2508.00305v1",
      "url": "http://arxiv.org/pdf/2508.00305v1",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.PF"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems",
      "abstract": "Explanations are crucial for building trustworthy AI systems, but a gap often\nexists between the explanations provided by models and those needed by users.\nTo address this gap, we introduce MetaExplainer, a neuro-symbolic framework\ndesigned to generate user-centered explanations. Our approach employs a\nthree-stage process: first, we decompose user questions into machine-readable\nformats using state-of-the-art large language models (LLM); second, we delegate\nthe task of generating system recommendations to model explainer methods; and\nfinally, we synthesize natural language explanations that summarize the\nexplainer outputs. Throughout this process, we utilize an Explanation Ontology\nto guide the language models and explainer methods. By leveraging LLMs and a\nstructured approach to explanation generation, MetaExplainer aims to enhance\nthe interpretability and trustworthiness of AI systems across various\napplications, providing users with tailored, question-driven explanations that\nbetter meet their needs. Comprehensive evaluations of MetaExplainer demonstrate\na step towards evaluating and utilizing current state-of-the-art explanation\nframeworks. Our results show high performance across all stages, with a 59.06%\nF1-score in question reframing, 70% faithfulness in model explanations, and 67%\ncontext-utilization in natural language synthesis. User studies corroborate\nthese findings, highlighting the creativity and comprehensiveness of generated\nexplanations. Tested on the Diabetes (PIMA Indian) tabular dataset,\nMetaExplainer supports diverse explanation types, including Contrastive,\nCounterfactual, Rationale, Case-Based, and Data explanations. The framework's\nversatility and traceability from using ontology to guide LLMs suggest broad\napplicability beyond the tested scenarios, positioning MetaExplainer as a\npromising tool for enhancing AI explainability across various domains.",
      "authors": [
        "Shruthi Chari",
        "Oshani Seneviratne",
        "Prithwish Chakraborty",
        "Pablo Meyer",
        "Deborah L. McGuinness"
      ],
      "published": "2025-08-01T04:01:40+00:00",
      "updated": "2025-08-01T04:01:40+00:00",
      "arxiv_id": "2508.00300v1",
      "url": "http://arxiv.org/pdf/2508.00300v1",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.HC"
    },
    {
      "title": "Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models",
      "abstract": "Large Language Models (LLMs) often retain inaccurate or outdated information\nfrom pre-training, leading to incorrect predictions or biased outputs during\ninference. While existing model editing methods can address this challenge,\nthey struggle with editing large amounts of factual information simultaneously\nand may compromise the general capabilities of the models. In this paper, our\nempirical study demonstrates that it is feasible to edit the internal\nrepresentations of LLMs and replace the entities in a manner similar to editing\nnatural language inputs. Based on this insight, we introduce the Latent\nKnowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of\nspecific entities via a lightweight hypernetwork to enable precise and\nlarge-scale editing. Experiments conducted on Llama-2 and Mistral show even\nwith the number of simultaneous edits reaching 10,000, LKS effectively performs\nknowledge editing while preserving the general abilities of the edited LLMs.\nCode is available at: https://github.com/Linuxin-xxx/LKS.",
      "authors": [
        "Xin Liu",
        "Qiyang Song",
        "Shaowen Xu",
        "Kerou Zhou",
        "Wenbo Jiang",
        "Xiaoqi Jia",
        "Weijuan Zhang",
        "Heqing Huang",
        "Yakai Li"
      ],
      "published": "2025-08-01T03:51:43+00:00",
      "updated": "2025-08-01T03:51:43+00:00",
      "arxiv_id": "2508.03741v1",
      "url": "http://arxiv.org/pdf/2508.03741v1",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Blueprint First, Model Second: A Framework for Deterministic LLM Workflow",
      "abstract": "While powerful, the inherent non-determinism of large language model (LLM)\nagents limits their application in structured operational environments where\nprocedural fidelity and predictable execution are strict requirements. This\nlimitation stems from current architectures that conflate probabilistic,\nhigh-level planning with low-level action execution within a single generative\nprocess. To address this, we introduce the Source Code Agent framework, a new\nparadigm built on the \"Blueprint First, Model Second\" philosophy. Our framework\ndecouples the workflow logic from the generative model. An expert-defined\noperational procedure is first codified into a source code-based Execution\nBlueprint, which is then executed by a deterministic engine. The LLM is\nstrategically invoked as a specialized tool to handle bounded, complex\nsub-tasks within the workflow, but never to decide the workflow's path. We\nconduct a comprehensive evaluation on the challenging tau-bench benchmark,\ndesigned for complex user-tool-rule scenarios. Our results demonstrate that the\nSource Code Agent establishes a new state-of-the-art, outperforming the\nstrongest baseline by 10.1 percentage points on the average Pass^1 score while\ndramatically improving execution efficiency. Our work enables the verifiable\nand reliable deployment of autonomous agents in applications governed by strict\nprocedural logic.",
      "authors": [
        "Libin Qiu",
        "Yuhang Ye",
        "Zhirong Gao",
        "Xide Zou",
        "Junfu Chen",
        "Ziming Gui",
        "Weizhi Huang",
        "Xiaobo Xue",
        "Wenkai Qiu",
        "Kun Zhao"
      ],
      "published": "2025-08-01T03:10:00+00:00",
      "updated": "2025-08-01T03:10:00+00:00",
      "arxiv_id": "2508.02721v1",
      "url": "http://arxiv.org/pdf/2508.02721v1",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering",
      "abstract": "Objective: Large Language Models (LLMs) demonstrate significant capabilities\nin medical text understanding and generation. However, their diagnostic\nreliability in complex clinical scenarios remains limited. This study aims to\nenhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We\npropose an Etiology-Aware Attention Steering Framework to integrate structured\nclinical reasoning into LLM-based diagnosis. Specifically, we first construct\nClinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines\nfor three representative acute abdominal emergencies: acute appendicitis, acute\npancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head\nIdentification algorithm to pinpoint attention heads crucial for the model's\netiology reasoning. To ensure reliable clinical reasoning alignment, we\nintroduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds\netiological reasoning cues into input representations and steers the selected\nEtiology-Aware Heads toward critical information through a Reasoning-Guided\nLoss function. Result: On the Consistent Diagnosis Cohort, our framework\nimproves average diagnostic accuracy by 15.65% and boosts the average Reasoning\nFocus Score by 31.6% over baselines. External validation on the Discrepant\nDiagnosis Cohort further confirms its effectiveness in enhancing diagnostic\naccuracy. Further assessments via Reasoning Attention Frequency indicate that\nour models exhibit enhanced reliability when faced with real-world complex\nscenarios. Conclusion: This study presents a practical and effective approach\nto enhance clinical reasoning in LLM-based diagnosis. By aligning model\nattention with structured CRS, the proposed framework offers a promising\nparadigm for building more interpretable and reliable AI diagnostic systems in\ncomplex clinical settings.",
      "authors": [
        "Peixian Li",
        "Yu Tian",
        "Ruiqi Tu",
        "Chengkai Wu",
        "Jingjing Ren",
        "Jingsong Li"
      ],
      "published": "2025-08-01T03:05:43+00:00",
      "updated": "2025-08-01T03:05:43+00:00",
      "arxiv_id": "2508.00285v1",
      "url": "http://arxiv.org/pdf/2508.00285v1",
      "categories": [
        "cs.CL",
        "I.2.7; J.3"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks",
      "abstract": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals. We\nconclude that there is a core gap between the value-driven, embodied nature of\nhuman cognition and the statistical patterns of LLMs, highlighting the\nnecessity of incorporating intrinsic motivation and physical grounding into the\ndesign of more human-aligned agents.",
      "authors": [
        "Yi-Long Lu",
        "Jiajun Song",
        "Chunhui Zhang",
        "Wei Wang"
      ],
      "published": "2025-08-01T03:00:41+00:00",
      "updated": "2025-08-05T09:10:21+00:00",
      "arxiv_id": "2508.00282v2",
      "url": "http://arxiv.org/pdf/2508.00282v2",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI"
    },
    {
      "title": "Multimodal Referring Segmentation: A Survey",
      "abstract": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
      "authors": [
        "Henghui Ding",
        "Song Tang",
        "Shuting He",
        "Chang Liu",
        "Zuxuan Wu",
        "Yu-Gang Jiang"
      ],
      "published": "2025-08-01T02:14:00+00:00",
      "updated": "2025-08-05T11:42:44+00:00",
      "arxiv_id": "2508.00265v2",
      "url": "http://arxiv.org/pdf/2508.00265v2",
      "categories": [
        "cs.CV"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Calibrated Language Models and How to Find Them with Label Smoothing",
      "abstract": "Recent advances in natural language processing (NLP) have opened up greater\nopportunities to enable fine-tuned large language models (LLMs) to behave as\nmore powerful interactive agents through improved instruction-following\nability. However, understanding how this impacts confidence calibration for\nreliable model output has not been researched in full. In this work, we examine\nvarious open-sourced LLMs, identifying significant calibration degradation\nafter instruction tuning in each. Seeking a practical solution, we look towards\nlabel smoothing, which has been shown as an effective method to regularize for\noverconfident predictions but has yet to be widely adopted in the supervised\nfine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing\nis sufficient to maintain calibration throughout the SFT process. However,\nsettings remain where the effectiveness of smoothing is severely diminished, in\nparticular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to\nstem from the ability to become over-confident, which has a direct relationship\nwith the hidden size and vocabulary size, and justify this theoretically and\nexperimentally. Finally, we address an outstanding issue regarding the memory\nfootprint of the cross-entropy loss computation in the label smoothed loss\nsetting, designing a customized kernel to dramatically reduce memory\nconsumption without sacrificing speed or performance in comparison to existing\nsolutions for non-smoothed losses.",
      "authors": [
        "Jerry Huang",
        "Peng Lu",
        "Qiuhao Zeng"
      ],
      "published": "2025-08-01T02:12:20+00:00",
      "updated": "2025-08-01T02:12:20+00:00",
      "arxiv_id": "2508.00264v1",
      "url": "http://arxiv.org/pdf/2508.00264v1",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG"
    },
    {
      "title": "Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models",
      "abstract": "Continual learning enables pre-trained generative vision-language models\n(VLMs) to incorporate knowledge from new tasks without retraining data from\nprevious ones. Recent methods update a visual projector to translate visual\ninformation for new tasks, connecting pre-trained vision encoders with large\nlanguage models. However, such adjustments may cause the models to prioritize\nvisual inputs over language instructions, particularly learning tasks with\nrepetitive types of textual instructions. To address the neglect of language\ninstructions, we propose a novel framework that grounds the translation of\nvisual information on instructions for language models. We introduce a mixture\nof visual projectors, each serving as a specialized visual-to-language\ntranslation expert based on the given instruction context to adapt to new\ntasks. To avoid using experts for irrelevant instruction contexts, we propose\nan expert recommendation strategy that reuses experts for tasks similar to\nthose previously learned. Additionally, we introduce expert pruning to\nalleviate interference from the use of experts that cumulatively activated in\nprevious tasks. Extensive experiments on diverse vision-language tasks\ndemonstrate that our method outperforms existing continual learning approaches\nby generating instruction-following responses.",
      "authors": [
        "Hyundong Jin",
        "Hyung Jin Chang",
        "Eunwoo Kim"
      ],
      "published": "2025-08-01T02:08:09+00:00",
      "updated": "2025-08-01T02:08:09+00:00",
      "arxiv_id": "2508.00260v1",
      "url": "http://arxiv.org/pdf/2508.00260v1",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.CV"
    },
    {
      "title": "Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study",
      "abstract": "Low-altitude wireless networks (LAWNs) have the potential to revolutionize\ncommunications by supporting a range of applications, including urban parcel\ndelivery, aerial inspections and air taxis. However, compared with traditional\nwireless networks, LAWNs face unique security challenges due to low-altitude\noperations, frequent mobility and reliance on unlicensed spectrum, making it\nmore vulnerable to some malicious attacks. In this paper, we investigate some\nlarge artificial intelligence model (LAM)-enabled solutions for secure\ncommunications in LAWNs. Specifically, we first explore the amplified security\nrisks and important limitations of traditional AI methods in LAWNs. Then, we\nintroduce the basic concepts of LAMs and delve into the role of LAMs in\naddressing these challenges. To demonstrate the practical benefits of LAMs for\nsecure communications in LAWNs, we propose a novel LAM-based optimization\nframework that leverages large language models (LLMs) to generate enhanced\nstate features on top of handcrafted representations, and to design intrinsic\nrewards accordingly, thereby improving reinforcement learning performance for\nsecure communication tasks. Through a typical case study, simulation results\nvalidate the effectiveness of the proposed framework. Finally, we outline\nfuture directions for integrating LAMs into secure LAWN applications.",
      "authors": [
        "Chuang Zhang",
        "Geng Sun",
        "Jiacheng Wang",
        "Yijing Lin",
        "Weijie Yuan",
        "Sinem Coleri",
        "Dusit Niyato",
        "Tony Q. S. Quek"
      ],
      "published": "2025-08-01T01:53:58+00:00",
      "updated": "2025-08-01T01:53:58+00:00",
      "arxiv_id": "2508.00256v1",
      "url": "http://arxiv.org/pdf/2508.00256v1",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI"
    },
    {
      "title": "Accurate and Consistent Graph Model Generation from Text with Large Language Models",
      "abstract": "Graph model generation from natural language description is an important task\nwith many applications in software engineering. With the rise of large language\nmodels (LLMs), there is a growing interest in using LLMs for graph model\ngeneration. Nevertheless, LLM-based graph model generation typically produces\npartially correct models that suffer from three main issues: (1) syntax\nviolations: the generated model may not adhere to the syntax defined by its\nmetamodel, (2) constraint inconsistencies: the structure of the model might not\nconform to some domain-specific constraints, and (3) inaccuracy: due to the\ninherent uncertainty in LLMs, the models can include inaccurate, hallucinated\nelements. While the first issue is often addressed through techniques such as\nconstraint decoding or filtering, the latter two remain largely unaddressed.\nMotivated by recent self-consistency approaches in LLMs, we propose a novel\nabstraction-concretization framework that enhances the consistency and quality\nof generated graph models by considering multiple outputs from an LLM. Our\napproach first constructs a probabilistic partial model that aggregates all\ncandidate outputs and then refines this partial model into the most appropriate\nconcrete model that satisfies all constraints. We evaluate our framework on\nseveral popular open-source and closed-source LLMs using diverse datasets for\nmodel generation tasks. The results demonstrate that our approach significantly\nimproves both the consistency and quality of the generated graph models.",
      "authors": [
        "Boqi Chen",
        "Ou Wei",
        "Bingzhou Zheng",
        "Gunter Mussbacher"
      ],
      "published": "2025-08-01T01:52:25+00:00",
      "updated": "2025-08-01T01:52:25+00:00",
      "arxiv_id": "2508.00255v1",
      "url": "http://arxiv.org/pdf/2508.00255v1",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Leveraging Large Language Model for Information Retrieval-based Bug Localization",
      "abstract": "Information Retrieval-based Bug Localization aims to identify buggy source\nfiles for a given bug report. While existing approaches -- ranging from vector\nspace models to deep learning models -- have shown potential in this domain,\ntheir effectiveness is often limited by the vocabulary mismatch between bug\nreports and source code. To address this issue, we propose a novel Large\nLanguage Model (LLM) based bug localization approach, called GenLoc. Given a\nbug report, GenLoc leverages an LLM equipped with code-exploration functions to\niteratively analyze the code base and identify potential buggy files. To gather\nbetter context, GenLoc may optionally retrieve semantically relevant files\nusing vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug\nreports from six large-scale Java projects. Experimental results show that\nGenLoc outperforms five state-of-the-art bug localization techniques across\nmultiple metrics, achieving an average improvement of more than 60\\% in\nAccuracy@1.",
      "authors": [
        "Moumita Asad",
        "Rafed Muhammad Yasir",
        "Armin Geramirad",
        "Sam Malek"
      ],
      "published": "2025-08-01T01:48:10+00:00",
      "updated": "2025-08-01T01:48:10+00:00",
      "arxiv_id": "2508.00253v1",
      "url": "http://arxiv.org/pdf/2508.00253v1",
      "categories": [
        "cs.SE"
      ],
      "primary_category": "cs.SE"
    },
    {
      "title": "Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English",
      "abstract": "In recent years, written language, particularly in science and education, has\nundergone remarkable shifts in word usage. These changes are widely attributed\nto the growing influence of Large Language Models (LLMs), which frequently rely\non a distinct lexical style. Divergences between model output and target\naudience norms can be viewed as a form of misalignment. While these shifts are\noften linked to using Artificial Intelligence (AI) directly as a tool to\ngenerate text, it remains unclear whether the changes reflect broader changes\nin the human language system itself. To explore this question, we constructed a\ndataset of 22.1 million words from unscripted spoken language drawn from\nconversational science and technology podcasts. We analyzed lexical trends\nbefore and after ChatGPT's release in 2022, focusing on commonly LLM-associated\nwords. Our results show a moderate yet significant increase in the usage of\nthese words post-2022, suggesting a convergence between human word choices and\nLLM-associated patterns. In contrast, baseline synonym words exhibit no\nsignificant directional shift. Given the short time frame and the number of\nwords affected, this may indicate the onset of a remarkable shift in language\nuse. Whether this represents natural language change or a novel shift driven by\nAI exposure remains an open question. Similarly, although the shifts may stem\nfrom broader adoption patterns, it may also be that upstream training\nmisalignments ultimately contribute to changes in human language use. These\nfindings parallel ethical concerns that misaligned models may shape social and\nmoral beliefs.",
      "authors": [
        "Bryce Anderson",
        "Riley Galpin",
        "Tom S. Juzek"
      ],
      "published": "2025-08-01T00:47:33+00:00",
      "updated": "2025-08-01T00:47:33+00:00",
      "arxiv_id": "2508.00238v1",
      "url": "http://arxiv.org/pdf/2508.00238v1",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2; I.2.7"
      ],
      "primary_category": "cs.CL"
    },
    {
      "title": "Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nleading to a significant increase in user demand for LLM services. However,\ncloud-based LLM services often suffer from high latency, unstable\nresponsiveness, and privacy concerns. Therefore, multiple LLMs are usually\ndeployed at the network edge to boost real-time responsiveness and protect data\nprivacy, particularly for many emerging smart mobile and IoT applications.\nGiven the varying response quality and latency of LLM services, a critical\nissue is how to route user requests from mobile and IoT devices to an\nappropriate LLM service (i.e., edge LLM expert) to ensure acceptable\nquality-of-service (QoS). Existing routing algorithms fail to simultaneously\naddress the heterogeneity of LLM services, the interference among requests, and\nthe dynamic workloads necessary for maintaining long-term stable QoS. To meet\nthese challenges, in this paper we propose a novel deep reinforcement learning\n(DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM\nservices. Due to the dynamic nature of the global state, we propose a dynamic\nstate abstraction technique to compactly represent global state features with a\nheterogeneous graph attention network (HAN). Additionally, we introduce an\naction impact estimator and a tailored reward function to guide the DRL agent\nin maximizing QoS and preventing latency violations. Extensive experiments on\nboth Poisson and real-world workloads demonstrate that our proposed algorithm\nsignificantly improves average QoS and computing resource efficiency compared\nto existing baselines.",
      "authors": [
        "Jin Yang",
        "Qiong Wu",
        "Zhiying Feng",
        "Zhi Zhou",
        "Deke Guo",
        "Xu Chen"
      ],
      "published": "2025-08-01T00:45:15+00:00",
      "updated": "2025-08-01T00:45:15+00:00",
      "arxiv_id": "2508.00234v1",
      "url": "http://arxiv.org/pdf/2508.00234v1",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC",
        "cs.MA"
      ],
      "primary_category": "cs.NI"
    },
    {
      "title": "Academic Vibe Coding: Opportunities for Accelerating Research in an Era of Resource Constraint",
      "abstract": "Academic laboratories face mounting resource constraints: budgets are\ntightening, grant overheads are potentially being capped, and the market rate\nfor data-science talent significantly outstrips university compensation. Vibe\ncoding, which is structured, prompt-driven code generation with large language\nmodels (LLMs) embedded in reproducible workflows, offers one pragmatic\nresponse. It aims to compress the idea-to-analysis timeline, reduce staffing\npressure on specialized data roles, and maintain rigorous, version-controlled\noutputs. This article defines the vibe coding concept, situates it against the\ncurrent academic resourcing crisis, details a beginner-friendly toolchain for\nits implementation, and analyzes inherent limitations that necessitate\ngovernance and mindful application.",
      "authors": [
        "Matthew G Crowson",
        "Leo Celi A. Celi"
      ],
      "published": "2025-08-01T00:42:44+00:00",
      "updated": "2025-08-01T00:42:44+00:00",
      "arxiv_id": "2508.00952v1",
      "url": "http://arxiv.org/pdf/2508.00952v1",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.PL",
        "cs.SE"
      ],
      "primary_category": "cs.CY"
    },
    {
      "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product",
      "abstract": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for\nadapting large pre-trained models. Amongst PEFT methods, low-rank adaptation\n(LoRA) has achieved notable success. However, recent studies have highlighted\nits limitations compared against full-rank alternatives, particularly when\napplied to multimodal and large language models. In this work, we present a\nquantitative comparison amongst full-rank and low-rank PEFT methods using a\nsynthetic matrix approximation benchmark with controlled spectral properties.\nOur results confirm that LoRA struggles to approximate matrices with relatively\nflat spectrums or high frequency components -- signs of high effective ranks.\nTo this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the\nKhatri-Rao product to produce weight updates, which, by construction, tends to\nproduce matrix product with a high effective rank. We demonstrate performance\ngains with KRAdapter on vision-language models up to 1B parameters and on large\nlanguage models up to 8B parameters, particularly on unseen common-sense\nreasoning tasks. In addition, KRAdapter maintains the memory and compute\nefficiency of LoRA, making it a practical and robust alternative to fine-tune\nbillion-scale parameter models.",
      "authors": [
        "Paul Albert",
        "Frederic Z. Zhang",
        "Hemanth Saratchandran",
        "Anton van den Hengel",
        "Ehsan Abbasnejad"
      ],
      "published": "2025-08-01T00:29:13+00:00",
      "updated": "2025-08-01T00:29:13+00:00",
      "arxiv_id": "2508.00230v1",
      "url": "http://arxiv.org/pdf/2508.00230v1",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG"
    }
  ]
}